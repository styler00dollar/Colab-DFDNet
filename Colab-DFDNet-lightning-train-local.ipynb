{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-DFDNet-lightning-train-Git.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ozp0WxFnfrh"
      },
      "source": [
        "# Colab-DFDNet-lightning\n",
        "\n",
        "Official repo: [csxmli2016/DFDNet](https://github.com/csxmli2016/DFDNet)\n",
        "\n",
        "Original repo: [max-vasyuk/DFDNet](https://github.com/max-vasyuk/DFDNet)\n",
        "\n",
        "My fork: [styler00dollar/Colab-DFDNet](https://github.com/styler00dollar/Colab-DFDNet)\n",
        "\n",
        "Porting everything to pytorch lightning. And storing code within Github. For more instructions, view my [other Colab](https://github.com/styler00dollar/Colab-DFDNet/blob/master/Colab-DFDNet-lightning-train.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUTjmSQawAg4"
      },
      "source": [
        "Preconfigured paths:\n",
        "```\n",
        "/content/DFDNet/DictionaryCenter512 (numpy dictionary files)\n",
        "/content/DFDNet/ffhq (dataset path)\n",
        "/content/DFDNet/landmarks (landmarks for that dataset)\n",
        "/content/DFDNet/weights/vgg19.pth (vgg19 path)\n",
        "\n",
        "/content/validation (validation path)\n",
        "/content/landmarks (landmark path for validation)\n",
        "```\n",
        "By default it will assume, that 1024px is used for training and 1024px is also used for validation. ``self.size`` is 512px and shouldn't be changed. If you don't want to apply rescaling to validation, change \n",
        "```\n",
        "part_locations = self.get_part_location(self.partpath, ImgName, 2)\n",
        "``` \n",
        "inside the validation dataloader. 2 is the scaling factor for landmarks.\n",
        "\n",
        "Edit ``vgg path`` and ``dictionary_path`` inside ``model.py``. Inside ``CustomTrainClass.py`` it is possible to configure logging path, loss functions and weights.\n",
        "\n",
        "Warning: Don't use AMP with StyleLoss. Perceptual loss will have problems with multi-GPU.\n",
        "\n",
        "Restart with ``runtime > restart runtime`` once if you see ``AttributeError: module 'PIL.TiffTags' has no attribute 'IFD'``.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1pZpG2W_oS6"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKlLiOhnWRR",
        "cellView": "form"
      },
      "source": [
        "#@title GPU\n",
        "!pip install pytorch-lightning -U\n",
        "!git clone -b local --single-branch https://github.com/styler00dollar/Colab-DFDNet DFDNet\n",
        "%cd /content/DFDNet\n",
        "!pip install -r requirements.txt\n",
        "!pip install pytorch-msssim\n",
        "!pip install trains\n",
        "!pip install PyJWT==1.7.1\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heMQnTKznpxa",
        "cellView": "form"
      },
      "source": [
        "#@title TPU  (restart runtime afterwards)\n",
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n",
        "#!pip install pytorch-lightning\n",
        "!pip install lightning-flash\n",
        "\n",
        "import collections\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\n",
        "VERSION = \"xrt==1.15.0\"  #@param [\"xrt==1.15.0\", \"torch_xla==nightly\"]\n",
        "CONFIG = {\n",
        "    'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),\n",
        "    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n",
        "        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n",
        "}[VERSION]\n",
        "DIST_BUCKET = 'gs://tpu-pytorch/wheels'\n",
        "TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "\n",
        "# Update TPU XRT version\n",
        "def update_server_xrt():\n",
        "  print('Updating server-side XRT to {} ...'.format(CONFIG.server))\n",
        "  url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(\n",
        "      TPU_ADDRESS=os.environ['COLAB_TPU_ADDR'].split(':')[0],\n",
        "      XRT_VERSION=CONFIG.server,\n",
        "  )\n",
        "  print('Done updating server-side XRT: {}'.format(requests.post(url)))\n",
        "\n",
        "update = threading.Thread(target=update_server_xrt)\n",
        "update.start()\n",
        "\n",
        "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
        "!pip uninstall -y torch torchvision\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
        "!pip install \"$TORCH_WHEEL\"\n",
        "!pip install \"$TORCH_XLA_WHEEL\"\n",
        "!pip install \"$TORCHVISION_WHEEL\"\n",
        "!sudo apt-get install libomp5\n",
        "update.join()\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "\n",
        "\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev > /dev/null\n",
        "!pip install pytorch-lightning > /dev/null\n",
        "\n",
        "\n",
        "!git clone https://github.com/styler00dollar/Colab-DFDNet DFDNet\n",
        "%cd /content/DFDNet\n",
        "!pip install -r requirements.txt\n",
        "!pip install pytorch-msssim\n",
        "!pip install trains\n",
        "!pip install PyJWT==1.7.1\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ZR-M5NbrnrsM"
      },
      "source": [
        "#@title download data\n",
        "!mkdir /content/DFDNet/DictionaryCenter512\n",
        "%cd /content/DFDNet/DictionaryCenter512\n",
        "!gdown --id 1sEB9j3s7Wj9aqPai1NF-MR7B-c0zfTin\n",
        "!gdown --id 1H4kByBiVmZuS9TbrWUR5uSNY770Goid6\n",
        "!gdown --id 10ctK3d9znZ9nGN3d1Z77xW3GGshbeKBb\n",
        "!gdown --id 1gcwmrIZjPFVu-cHjdQD6P4luohkPsil-\n",
        "!gdown --id 1rJ8cORPxbJsIVAiNrjBag0ihaY_Mvurn\n",
        "!gdown --id 1LkfJv2a3ud-mefAc1eZMJuINuNdSYgYO\n",
        "!gdown --id 1LH-nxD__icSJvTiAbXAXDch03oDtbpkZ\n",
        "!gdown --id 1JRTStLFsQ8dwaQjQ8qG5fNyrOvo6Tcvd\n",
        "!gdown --id 1Z4AkU1pOYTYpdbfljCgNMmPilhdEd0Kl\n",
        "!gdown --id 1Z4e1ltB3ACbYKzkoMBuVtzZ7a310G4xc\n",
        "!gdown --id 1fqWmi6-8ZQzUtZTp9UH4hyom7n4nl8aZ\n",
        "!gdown --id 1wfHtsExLvSgfH_EWtCPjTF5xsw3YyvjC\n",
        "!gdown --id 1Jr3Luf6tmcdKANcSLzvt0sjXr0QUIQ2g\n",
        "!gdown --id 1sPd4_IMYgqGLol0gqhHjBedKKxFAxswR\n",
        "!gdown --id 1eVFjXJRnBH4mx7ZbAmZRwVXZNUbgCQec\n",
        "!gdown --id 1w0GfO_KY775ZVF3KMk74ya6QL_bNU4cJ\n",
        "%cd /content/DFDNet\n",
        "!gdown --id 1VE5tnOKcfL6MoV839IVCCw5FhJxIgml5\n",
        "!7z x data.zip\n",
        "!mkdir /content/DFDNet/weights/\n",
        "%cd /content/DFDNet/weights/\n",
        "!gdown --id 1SfKKZJduOGhDD27Xl01yDx0-YSEkL2Aa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bxed8Z2pkpn"
      },
      "source": [
        "# (Optional) EfficientNet\n",
        "!pip install efficientnet_pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok2zT8sGvrzF"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gux_aPW8rz1y"
      },
      "source": [
        "%cd /content/DFDNet/lightning\n",
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ozJnvWyjAd"
      },
      "source": [
        "# Check data within dataset\n",
        "\n",
        "Training can crash if the extracted feature has dimension 0 somewhere inside the shape. To avoid this problem, it is recommended to test all images with vgg extraction, to make sure that won't crash normal training. Test for one epoch. If a checkpoint gets generated, don't use it for real training. Broken filenames will get printed. If epoch is finished, the test is complete. Delete the printed files (and their landmarks). Data inside validation folders does not matter and will be skipped, but you probably should put at least one file there to avoid crashing. Ignore loss.\n",
        "\n",
        "Tip: Avoid pictures with multiple faces.\n",
        "\n",
        "Warning: The provided dataset has one bad image and will crash training because there are 2 faces. You will find one file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_CYv04OsCOq"
      },
      "source": [
        "# File checking\n",
        "%cd /content/DFDNet/lightning\n",
        "!python check.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
