{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-DFDNet-lightning-train-Git.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ozp0WxFnfrh"
      },
      "source": [
        "# Colab-DFDNet-lightning\n",
        "\n",
        "Official repo: [csxmli2016/DFDNet](https://github.com/csxmli2016/DFDNet)\n",
        "\n",
        "Original repo: [max-vasyuk/DFDNet](https://github.com/max-vasyuk/DFDNet)\n",
        "\n",
        "My fork: [styler00dollar/Colab-DFDNet](https://github.com/styler00dollar/Colab-DFDNet)\n",
        "\n",
        "Porting everything to pytorch lightning. And storing code within Github. For more instructions, view my [other Colab](https://github.com/styler00dollar/Colab-DFDNet/blob/master/Colab-DFDNet-lightning-train.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUTjmSQawAg4"
      },
      "source": [
        "Preconfigured paths:\n",
        "```\n",
        "/content/DFDNet/DictionaryCenter512 (numpy dictionary files)\n",
        "/content/DFDNet/ffhq (dataset path)\n",
        "/content/DFDNet/landmarks (landmarks for that dataset)\n",
        "/content/DFDNet/weights/vgg19.pth (vgg19 path)\n",
        "\n",
        "/content/validation (validation path)\n",
        "/content/landmarks (landmark path for validation)\n",
        "```\n",
        "\n",
        "Edit ``vgg path`` and ``dictionary_path`` inside ``model.py``. Inside ``CustomTrainClass.py`` it is possible to configure logging path, loss functions and weights.\n",
        "\n",
        "Warning: Don't use AMP with StyleLoss. Perceptual loss will have problems with multi-GPU.\n",
        "\n",
        "Restart with ``runtime > restart runtime`` once if you see ``AttributeError: module 'PIL.TiffTags' has no attribute 'IFD'``.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1pZpG2W_oS6"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKlLiOhnWRR",
        "cellView": "form"
      },
      "source": [
        "#@title GPU\n",
        "!pip install pytorch-lightning -U\n",
        "!git clone -b local --single-branch https://github.com/styler00dollar/Colab-DFDNet DFDNet\n",
        "%cd /content/DFDNet\n",
        "!pip install -r requirements.txt\n",
        "!pip install pytorch-msssim\n",
        "!pip install trains\n",
        "!pip install PyJWT==1.7.1\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heMQnTKznpxa",
        "cellView": "form"
      },
      "source": [
        "#@title TPU  (restart runtime afterwards)\n",
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n",
        "#!pip install pytorch-lightning\n",
        "!pip install lightning-flash\n",
        "\n",
        "import collections\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\n",
        "VERSION = \"xrt==1.15.0\"  #@param [\"xrt==1.15.0\", \"torch_xla==nightly\"]\n",
        "CONFIG = {\n",
        "    'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),\n",
        "    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n",
        "        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n",
        "}[VERSION]\n",
        "DIST_BUCKET = 'gs://tpu-pytorch/wheels'\n",
        "TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "\n",
        "# Update TPU XRT version\n",
        "def update_server_xrt():\n",
        "  print('Updating server-side XRT to {} ...'.format(CONFIG.server))\n",
        "  url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(\n",
        "      TPU_ADDRESS=os.environ['COLAB_TPU_ADDR'].split(':')[0],\n",
        "      XRT_VERSION=CONFIG.server,\n",
        "  )\n",
        "  print('Done updating server-side XRT: {}'.format(requests.post(url)))\n",
        "\n",
        "update = threading.Thread(target=update_server_xrt)\n",
        "update.start()\n",
        "\n",
        "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
        "!pip uninstall -y torch torchvision\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
        "!pip install \"$TORCH_WHEEL\"\n",
        "!pip install \"$TORCH_XLA_WHEEL\"\n",
        "!pip install \"$TORCHVISION_WHEEL\"\n",
        "!sudo apt-get install libomp5\n",
        "update.join()\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "\n",
        "\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev > /dev/null\n",
        "!pip install pytorch-lightning > /dev/null\n",
        "\n",
        "\n",
        "!git clone https://github.com/styler00dollar/Colab-DFDNet DFDNet\n",
        "%cd /content/DFDNet\n",
        "!pip install -r requirements.txt\n",
        "!pip install pytorch-msssim\n",
        "!pip install trains\n",
        "!pip install PyJWT==1.7.1\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ZR-M5NbrnrsM"
      },
      "source": [
        "#@title download data\n",
        "!mkdir /content/DFDNet/DictionaryCenter512\n",
        "%cd /content/DFDNet/DictionaryCenter512\n",
        "!gdown --id 1sEB9j3s7Wj9aqPai1NF-MR7B-c0zfTin\n",
        "!gdown --id 1H4kByBiVmZuS9TbrWUR5uSNY770Goid6\n",
        "!gdown --id 10ctK3d9znZ9nGN3d1Z77xW3GGshbeKBb\n",
        "!gdown --id 1gcwmrIZjPFVu-cHjdQD6P4luohkPsil-\n",
        "!gdown --id 1rJ8cORPxbJsIVAiNrjBag0ihaY_Mvurn\n",
        "!gdown --id 1LkfJv2a3ud-mefAc1eZMJuINuNdSYgYO\n",
        "!gdown --id 1LH-nxD__icSJvTiAbXAXDch03oDtbpkZ\n",
        "!gdown --id 1JRTStLFsQ8dwaQjQ8qG5fNyrOvo6Tcvd\n",
        "!gdown --id 1Z4AkU1pOYTYpdbfljCgNMmPilhdEd0Kl\n",
        "!gdown --id 1Z4e1ltB3ACbYKzkoMBuVtzZ7a310G4xc\n",
        "!gdown --id 1fqWmi6-8ZQzUtZTp9UH4hyom7n4nl8aZ\n",
        "!gdown --id 1wfHtsExLvSgfH_EWtCPjTF5xsw3YyvjC\n",
        "!gdown --id 1Jr3Luf6tmcdKANcSLzvt0sjXr0QUIQ2g\n",
        "!gdown --id 1sPd4_IMYgqGLol0gqhHjBedKKxFAxswR\n",
        "!gdown --id 1eVFjXJRnBH4mx7ZbAmZRwVXZNUbgCQec\n",
        "!gdown --id 1w0GfO_KY775ZVF3KMk74ya6QL_bNU4cJ\n",
        "%cd /content/DFDNet\n",
        "!gdown --id 1VE5tnOKcfL6MoV839IVCCw5FhJxIgml5\n",
        "!7z x data.zip\n",
        "!mkdir /content/DFDNet/weights/\n",
        "%cd /content/DFDNet/weights/\n",
        "!gdown --id 1SfKKZJduOGhDD27Xl01yDx0-YSEkL2Aa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bxed8Z2pkpn"
      },
      "source": [
        "# (Optional) EfficientNet\n",
        "!pip install efficientnet_pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok2zT8sGvrzF"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gux_aPW8rz1y"
      },
      "source": [
        "%cd /content/DFDNet/lightning\n",
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ozJnvWyjAd"
      },
      "source": [
        "# Check data within dataset\n",
        "\n",
        "Training can crash if the extracted feature has dimension 0 somewhere inside the shape. To avoid this problem, it is recommended to test all images with vgg extraction, to make sure that won't crash normal training. Test for one epoch. If a checkpoint gets generated, don't use it for real training. Broken filenames will get printed. If epoch is finished, the test is complete. Delete the printed files (and their landmarks). Data inside validation folders does not matter and will be skipped, but you probably should put at least one file there to avoid crashing. Ignore loss.\n",
        "\n",
        "Tip: Avoid pictures with multiple faces.\n",
        "\n",
        "Warning: The provided dataset has one bad image and will crash training because there are 2 faces. You will find one file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cDHT1ohSBIYw"
      },
      "source": [
        "#@title model_check.py\n",
        "%%writefile /content/DFDNet/lightning/model_check.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import functools\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter as P\n",
        "#from util import util\n",
        "from torchvision import models\n",
        "#import scipy.io as sio\n",
        "import numpy as np\n",
        "#import scipy.ndimage\n",
        "import torch.nn.utils.spectral_norm as SpectralNorm\n",
        "#from spectral_norm import SpectralNorm\n",
        "\n",
        "from torch.autograd import Function\n",
        "from math import sqrt\n",
        "import random\n",
        "import os\n",
        "import math\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from functions import *\n",
        "\n",
        "\n",
        "class VGGFeat(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Input: (B, C, H, W), RGB, [-1, 1]\n",
        "    \"\"\"\n",
        "    def __init__(self, weight_path='/content/DFDNet/weights/vgg19.pth'):\n",
        "        super().__init__()\n",
        "        self.model = models.vgg19(pretrained=False)\n",
        "        self.build_vgg_layers()\n",
        "\n",
        "        self.model.load_state_dict(torch.load(weight_path))\n",
        "\n",
        "        self.register_parameter(\"RGB_mean\", nn.Parameter(torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)))\n",
        "        self.register_parameter(\"RGB_std\", nn.Parameter(torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)))\n",
        "\n",
        "        # self.model.eval()\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def build_vgg_layers(self):\n",
        "        vgg_pretrained_features = self.model.features\n",
        "        self.features = []\n",
        "        # feature_layers = [0, 3, 8, 17, 26, 35]\n",
        "        feature_layers = [0, 8, 17, 26, 35]\n",
        "        for i in range(len(feature_layers)-1):\n",
        "            module_layers = torch.nn.Sequential()\n",
        "            for j in range(feature_layers[i], feature_layers[i+1]):\n",
        "                module_layers.add_module(str(j), vgg_pretrained_features[j])\n",
        "            self.features.append(module_layers)\n",
        "        self.features = torch.nn.ModuleList(self.features)\n",
        "\n",
        "    def preprocess(self, x):\n",
        "        x = (x + 1) / 2\n",
        "        x = (x - self.RGB_mean) / self.RGB_std\n",
        "        if x.shape[3] < 224:\n",
        "            x = torch.nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.preprocess(x)\n",
        "        features = []\n",
        "        for m in self.features:\n",
        "            # print(m)\n",
        "            x = m(x)\n",
        "            features.append(x)\n",
        "        return features\n",
        "\n",
        "\n",
        "class StyledUpBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1,upsample=False):\n",
        "        super().__init__()\n",
        "        if upsample:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                Blur(out_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                Blur(in_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding)\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        self.convup = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                # EqualConv2d(out_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(out_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                # Blur(out_channel),\n",
        "            )\n",
        "        # self.noise1 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain1 = AdaptiveInstanceNorm(out_channel)\n",
        "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        # self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\n",
        "        # self.noise2 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain2 = AdaptiveInstanceNorm(out_channel)\n",
        "        # self.lrelu2 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.ScaleModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1))\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "        self.ShiftModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1)),\n",
        "            nn.Sigmoid(),\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input, style):\n",
        "        out = self.conv1(input)\n",
        "#         out = self.noise1(out, noise)\n",
        "        out = self.lrelu1(out)\n",
        "\n",
        "        Shift1 = self.ShiftModel1(style)\n",
        "        Scale1 = self.ScaleModel1(style)\n",
        "        out = out * Scale1 + Shift1\n",
        "        # out = self.adain1(out, style)\n",
        "        outup = self.convup(out)\n",
        "\n",
        "        return outup\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class StyledUpBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1,upsample=False):\n",
        "        super().__init__()\n",
        "        if upsample:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                Blur(out_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                Blur(in_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding)\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        self.convup = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                # EqualConv2d(out_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(out_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                # Blur(out_channel),\n",
        "            )\n",
        "        # self.noise1 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain1 = AdaptiveInstanceNorm(out_channel)\n",
        "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        # self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\n",
        "        # self.noise2 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain2 = AdaptiveInstanceNorm(out_channel)\n",
        "        # self.lrelu2 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.ScaleModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1))\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "        self.ShiftModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1)),\n",
        "            nn.Sigmoid(),\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input, style):\n",
        "        out = self.conv1(input)\n",
        "#         out = self.noise1(out, noise)\n",
        "        out = self.lrelu1(out)\n",
        "\n",
        "        Shift1 = self.ShiftModel1(style)\n",
        "        Scale1 = self.ScaleModel1(style)\n",
        "        out = out * Scale1 + Shift1\n",
        "        # out = self.adain1(out, style)\n",
        "        outup = self.convup(out)\n",
        "\n",
        "        return outup\n",
        "\n",
        "class UNetDictFace(pl.LightningModule):\n",
        "    def __init__(self, ngf=64, dictionary_path='/content/DFDNet/DictionaryCenter512'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.part_sizes = np.array([80,80,50,110]) # size for 512\n",
        "        self.feature_sizes = np.array([256,128,64,32])\n",
        "        self.channel_sizes = np.array([128,256,512,512])\n",
        "        Parts = ['left_eye','right_eye','nose','mouth']\n",
        "        self.Dict_256 = {}\n",
        "        self.Dict_128 = {}\n",
        "        self.Dict_64 = {}\n",
        "        self.Dict_32 = {}\n",
        "        for j,i in enumerate(Parts):\n",
        "            f_256 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_256_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_256_reshape = f_256.reshape(f_256.size(0),self.channel_sizes[0],self.part_sizes[j]//2,self.part_sizes[j]//2)\n",
        "            max_256 = torch.max(torch.sqrt(compute_sum(torch.pow(f_256_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_256[i] = f_256_reshape #/ max_256\n",
        "\n",
        "            f_128 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_128_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_128_reshape = f_128.reshape(f_128.size(0),self.channel_sizes[1],self.part_sizes[j]//4,self.part_sizes[j]//4)\n",
        "            max_128 = torch.max(torch.sqrt(compute_sum(torch.pow(f_128_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_128[i] = f_128_reshape #/ max_128\n",
        "\n",
        "            f_64 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_64_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_64_reshape = f_64.reshape(f_64.size(0),self.channel_sizes[2],self.part_sizes[j]//8,self.part_sizes[j]//8)\n",
        "            max_64 = torch.max(torch.sqrt(compute_sum(torch.pow(f_64_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_64[i] = f_64_reshape #/ max_64\n",
        "\n",
        "            f_32 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_32_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_32_reshape = f_32.reshape(f_32.size(0),self.channel_sizes[3],self.part_sizes[j]//16,self.part_sizes[j]//16)\n",
        "            max_32 = torch.max(torch.sqrt(compute_sum(torch.pow(f_32_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_32[i] = f_32_reshape #/ max_32\n",
        "\n",
        "        self.le_256 = AttentionBlock(128)\n",
        "        self.le_128 = AttentionBlock(256)\n",
        "        self.le_64 = AttentionBlock(512)\n",
        "        self.le_32 = AttentionBlock(512)\n",
        "\n",
        "        self.re_256 = AttentionBlock(128)\n",
        "        self.re_128 = AttentionBlock(256)\n",
        "        self.re_64 = AttentionBlock(512)\n",
        "        self.re_32 = AttentionBlock(512)\n",
        "\n",
        "        self.no_256 = AttentionBlock(128)\n",
        "        self.no_128 = AttentionBlock(256)\n",
        "        self.no_64 = AttentionBlock(512)\n",
        "        self.no_32 = AttentionBlock(512)\n",
        "\n",
        "        self.mo_256 = AttentionBlock(128)\n",
        "        self.mo_128 = AttentionBlock(256)\n",
        "        self.mo_64 = AttentionBlock(512)\n",
        "        self.mo_32 = AttentionBlock(512)\n",
        "\n",
        "        #norm\n",
        "        self.VggExtract = VGGFeat()\n",
        "\n",
        "        ######################\n",
        "        self.MSDilate = MSDilateBlock(ngf*8, dilation = [4,3,2,1])  #\n",
        "\n",
        "        self.up0 = StyledUpBlock(ngf*8,ngf*8)\n",
        "        self.up1 = StyledUpBlock(ngf*8, ngf*4) #\n",
        "        self.up2 = StyledUpBlock(ngf*4, ngf*2) #\n",
        "        self.up3 = StyledUpBlock(ngf*2, ngf) #\n",
        "        self.up4 = nn.Sequential( # 128\n",
        "            # nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            SpectralNorm(nn.Conv2d(ngf, ngf, 3, 1, 1)),\n",
        "            # nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            UpResBlock(ngf),\n",
        "            UpResBlock(ngf),\n",
        "            # SpectralNorm(nn.Conv2d(ngf, 3, kernel_size=3, stride=1, padding=1)),\n",
        "            nn.Conv2d(ngf, 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.to_rgb0 = ToRGB(ngf*8)\n",
        "        self.to_rgb1 = ToRGB(ngf*4)\n",
        "        self.to_rgb2 = ToRGB(ngf*2)\n",
        "        self.to_rgb3 = ToRGB(ngf*1)\n",
        "\n",
        "        # for param in self.BlurInputConv.parameters():\n",
        "        #     param.requires_grad = False\n",
        "\n",
        "    def forward(self, input, part_locations):\n",
        "        VggFeatures = self.VggExtract(input)\n",
        "        # for b in range(input.size(0)):\n",
        "        b = 0\n",
        "        delete_flag = 0\n",
        "        UpdateVggFeatures = []\n",
        "        for i, f_size in enumerate(self.feature_sizes):\n",
        "            cur_feature = VggFeatures[i]\n",
        "\n",
        "            update_feature = cur_feature.clone() #* 0\n",
        "            cur_part_sizes = self.part_sizes // (512/f_size)\n",
        "            dicts_feature = getattr(self, 'Dict_'+str(f_size))\n",
        "\n",
        "            LE_Dict_feature = dicts_feature['left_eye'].to(input)\n",
        "            RE_Dict_feature = dicts_feature['right_eye'].to(input)\n",
        "            NO_Dict_feature = dicts_feature['nose'].to(input)\n",
        "            MO_Dict_feature = dicts_feature['mouth'].to(input)\n",
        "\n",
        "            le_location = (part_locations[0][b] // (512/f_size)).int()\n",
        "            re_location = (part_locations[1][b] // (512/f_size)).int()\n",
        "            no_location = (part_locations[2][b] // (512/f_size)).int()\n",
        "            mo_location = (part_locations[3][b] // (512/f_size)).int()\n",
        "\n",
        "            LE_feature = cur_feature[:,:,le_location[1]:le_location[3],le_location[0]:le_location[2]].clone()\n",
        "            RE_feature = cur_feature[:,:,re_location[1]:re_location[3],re_location[0]:re_location[2]].clone()\n",
        "            NO_feature = cur_feature[:,:,no_location[1]:no_location[3],no_location[0]:no_location[2]].clone()\n",
        "            MO_feature = cur_feature[:,:,mo_location[1]:mo_location[3],mo_location[0]:mo_location[2]].clone()\n",
        "\n",
        "            #resize\n",
        "            # check\n",
        "            # good: torch.Size([1, 128, 45, 46])\n",
        "            # bad:  torch.Size([1, 128, 36, 0])\n",
        "\n",
        "            if LE_feature.shape[2] == 0 or LE_feature.shape[3] == 0:\n",
        "              delete_flag = 1\n",
        "            if RE_feature.shape[2] == 0 or RE_feature.shape[3] == 0:\n",
        "              delete_flag = 1\n",
        "            if NO_feature.shape[2] == 0 or NO_feature.shape[3] == 0:\n",
        "              delete_flag = 1\n",
        "            if MO_feature.shape[2] == 0 or MO_feature.shape[3] == 0:\n",
        "              delete_flag = 1\n",
        "\n",
        "        return delete_flag\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# smaller blocks\n",
        "def AttentionBlock(in_channel):\n",
        "    return nn.Sequential(\n",
        "        SpectralNorm(nn.Conv2d(in_channel, in_channel, 3, 1, 1)),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        SpectralNorm(nn.Conv2d(in_channel, in_channel, 3, 1, 1))\n",
        "    )\n",
        "\n",
        "\n",
        "class MSDilateBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channels,conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, kernel_size=3, dilation=[1,1,1,1], bias=True):\n",
        "        super(MSDilateBlock, self).__init__()\n",
        "        self.conv1 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[0], bias=bias)\n",
        "        self.conv2 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[1], bias=bias)\n",
        "        self.conv3 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[2], bias=bias)\n",
        "        self.conv4 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[3], bias=bias)\n",
        "        self.convi =  SpectralNorm(conv_layer(in_channels*4, in_channels, kernel_size=kernel_size, stride=1, padding=(kernel_size-1)//2, bias=bias))\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv1(x)\n",
        "        conv2 = self.conv2(x)\n",
        "        conv3 = self.conv3(x)\n",
        "        conv4 = self.conv4(x)\n",
        "        cat  = torch.cat([conv1, conv2, conv3, conv4], 1)\n",
        "        out = self.convi(cat) + x\n",
        "        return out\n",
        "\n",
        "\n",
        "class VggClassNet(pl.LightningModule):\n",
        "    def __init__(self, select_layer = ['0','5','10','19']):\n",
        "        super(VggClassNet, self).__init__()\n",
        "        self.select = select_layer\n",
        "        self.vgg = models.vgg19(pretrained=True).features\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for name, layer in self.vgg._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in self.select:\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class UpResBlock(pl.LightningModule):\n",
        "    def __init__(self, dim, conv_layer = nn.Conv2d, norm_layer = nn.BatchNorm2d):\n",
        "        super(UpResBlock, self).__init__()\n",
        "        self.Model = nn.Sequential(\n",
        "            # SpectralNorm(conv_layer(dim, dim, 3, 1, 1)),\n",
        "            conv_layer(dim, dim, 3, 1, 1),\n",
        "            # norm_layer(dim),\n",
        "            nn.LeakyReLU(0.2,True),\n",
        "            # SpectralNorm(conv_layer(dim, dim, 3, 1, 1)),\n",
        "            conv_layer(dim, dim, 3, 1, 1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        out = x + self.Model(x)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_CYv04OsCOq"
      },
      "source": [
        "# File checking\n",
        "%cd /content/DFDNet/lightning\n",
        "!python check.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}