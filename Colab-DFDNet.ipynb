{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-DFDNet-train.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnEeMO8E66PK"
      },
      "source": [
        "# Colab-DFDNet\r\n",
        "\r\n",
        "Official repo: [csxmli2016/DFDNet](https://github.com/csxmli2016/DFDNet)\r\n",
        "\r\n",
        "Original repo: [max-vasyuk/DFDNet](https://github.com/max-vasyuk/DFDNet)\r\n",
        "\r\n",
        "My fork: [styler00dollar/Colab-DFDNet](https://github.com/styler00dollar/Colab-DFDNet)\r\n",
        "\r\n",
        "This Colab does focus on training DFDNet models and using them afterwards. If you just want to try DFDNet, then this isn't the Colab you are looking for. There are multiple different Colabs that already do that and can be found [here](https://github.com/styler00dollar/dl-colab-notebooks). Simply using the original pre-train with ``inference.py`` will result in weight errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP9vfUVc2d60"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N27WaAPb0FgU",
        "cellView": "form"
      },
      "source": [
        "#@title install\r\n",
        "!git clone https://github.com/max-vasyuk/DFDNet\r\n",
        "%cd /content/DFDNet\r\n",
        "!pip install -r requirements.txt\r\n",
        "!pip install pytorch-msssim\r\n",
        "!pip install trains\r\n",
        "!pip install PyJWT==1.7.1\r\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGNU4sic60mJ",
        "cellView": "form"
      },
      "source": [
        "#@title download data\r\n",
        "!mkdir /content/DFDNet/DictionaryCenter512\r\n",
        "%cd /content/DFDNet/DictionaryCenter512\r\n",
        "!gdown --id 1sEB9j3s7Wj9aqPai1NF-MR7B-c0zfTin\r\n",
        "!gdown --id 1H4kByBiVmZuS9TbrWUR5uSNY770Goid6\r\n",
        "!gdown --id 10ctK3d9znZ9nGN3d1Z77xW3GGshbeKBb\r\n",
        "!gdown --id 1gcwmrIZjPFVu-cHjdQD6P4luohkPsil-\r\n",
        "!gdown --id 1rJ8cORPxbJsIVAiNrjBag0ihaY_Mvurn\r\n",
        "!gdown --id 1LkfJv2a3ud-mefAc1eZMJuINuNdSYgYO\r\n",
        "!gdown --id 1LH-nxD__icSJvTiAbXAXDch03oDtbpkZ\r\n",
        "!gdown --id 1JRTStLFsQ8dwaQjQ8qG5fNyrOvo6Tcvd\r\n",
        "!gdown --id 1Z4AkU1pOYTYpdbfljCgNMmPilhdEd0Kl\r\n",
        "!gdown --id 1Z4e1ltB3ACbYKzkoMBuVtzZ7a310G4xc\r\n",
        "!gdown --id 1fqWmi6-8ZQzUtZTp9UH4hyom7n4nl8aZ\r\n",
        "!gdown --id 1wfHtsExLvSgfH_EWtCPjTF5xsw3YyvjC\r\n",
        "!gdown --id 1Jr3Luf6tmcdKANcSLzvt0sjXr0QUIQ2g\r\n",
        "!gdown --id 1sPd4_IMYgqGLol0gqhHjBedKKxFAxswR\r\n",
        "!gdown --id 1eVFjXJRnBH4mx7ZbAmZRwVXZNUbgCQec\r\n",
        "!gdown --id 1w0GfO_KY775ZVF3KMk74ya6QL_bNU4cJ\r\n",
        "%cd /content/DFDNet\r\n",
        "!gdown --id 1VE5tnOKcfL6MoV839IVCCw5FhJxIgml5\r\n",
        "!7z x data.zip\r\n",
        "!mkdir /content/DFDNet/weights/\r\n",
        "%cd /content/DFDNet/weights/\r\n",
        "!gdown --id 1SfKKZJduOGhDD27Xl01yDx0-YSEkL2Aa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZByAvJCxGOA"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AamsWxJTawzj"
      },
      "source": [
        "The saving frequency can be adjusted by changing the value insde ``if i % 10 == 0 and i != 0:`` in ``run.py``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSWLqHlT-ypr",
        "cellView": "form"
      },
      "source": [
        "#@title run.py (removing trains, printing instead, saving training images locally, tensorboard)\r\n",
        "%%writefile /content/DFDNet/run.py\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "\r\n",
        "import math\r\n",
        "from PIL import Image\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch\r\n",
        "import random\r\n",
        "from skimage import transform as trans\r\n",
        "from skimage import io\r\n",
        "import sys\r\n",
        "sys.path.append('FaceLandmarkDetection')\r\n",
        "import face_alignment\r\n",
        "import dlib\r\n",
        "#import logging\r\n",
        "\r\n",
        "from models import *\r\n",
        "from models.model_resnet import MultiScaleDiscriminator\r\n",
        "from data.custom_dataset import AlignedDataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import torchvision\r\n",
        "\r\n",
        "from util.Loss import hinge_loss, hinge_loss_G\r\n",
        "from pytorch_msssim import MS_SSIM\r\n",
        "\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "#from trains import Task\r\n",
        "from torchvision.utils import make_grid\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "from util.losses import LossNetworkVgg19\r\n",
        "from torchvision.utils import save_image\r\n",
        "\r\n",
        "from tensorboardX import SummaryWriter\r\n",
        "logdir='/content/DFDNet/logging'\r\n",
        "writer = SummaryWriter(logdir=logdir)\r\n",
        "\r\n",
        "#logger = logging.getLogger('logging')\r\n",
        "\r\n",
        "def tensor2im(input_image, norm=1, imtype=np.uint8):\r\n",
        "    image_numpy = input_image.data.cpu().float().clamp_(-1,1).numpy()\r\n",
        "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\r\n",
        "    return image_numpy.astype(imtype)\r\n",
        "\r\n",
        "\r\n",
        "def train(device, dataset, trainloader, netG, netD, writer):\r\n",
        "    optimizerG = torch.optim.Adam(netG.parameters(), lr=2e-4, betas=(0.5, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\r\n",
        "    optimizerD = torch.optim.Adam(netD.parameters(), lr=2e-4, betas=(0.5, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\r\n",
        "    \r\n",
        "    # Establish convention for real and fake labels during training\r\n",
        "    real_label = 1.\r\n",
        "    fake_label = 0.\r\n",
        "\r\n",
        "    criterionG = torch.nn.MSELoss()\r\n",
        "    criterionD = torch.nn.BCELoss()\r\n",
        "#     criterionD = torch.nn.MSELoss()\r\n",
        "#     criterionD = hinge_loss()\r\n",
        "    hinge_G = hinge_loss_G()\r\n",
        "    ms_ssim_module = MS_SSIM(data_range=255, size_average=True, channel=3)\r\n",
        "    \r\n",
        "    weights_layer_perceptual = [0.5, 1., 2., 4.]\r\n",
        "    vgg19_model = torchvision.models.vgg19(pretrained=True)\r\n",
        "    vgg19_model.to(device)\r\n",
        "    \r\n",
        "    perceptual_loss_vgg19 = LossNetworkVgg19(vgg19_model)\r\n",
        "    perceptual_loss_vgg19.to(device)\r\n",
        "    perceptual_loss_vgg19.eval()\r\n",
        "    del vgg19_model\r\n",
        "\r\n",
        "    num_epochs = 15\r\n",
        "\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        mean_loss_G = 0.0\r\n",
        "        mean_loss_D = 0.0\r\n",
        "        for i, data in enumerate((trainloader), 0):\r\n",
        "\r\n",
        "            data_a, data_c = data['A'], data['C']\r\n",
        "            data_a = data_a.to(device)\r\n",
        "            data_c = data_c.to(device)\r\n",
        "            data_part_locations = data['part_locations']\r\n",
        "\r\n",
        "            ############################\r\n",
        "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\r\n",
        "            ###########################\r\n",
        "            ## Train with all-real batch\r\n",
        "            netD.zero_grad()\r\n",
        "\r\n",
        "            # Format batch\r\n",
        "            real_batch  = {}\r\n",
        "            real_batch['1'] = data_c\r\n",
        "            real_batch['2'] = F.interpolate(data['C'], (256, 256)).to(device)\r\n",
        "            real_batch['4'] = F.interpolate(data['C'], (128, 128)).to(device)\r\n",
        "            real_batch['8'] = F.interpolate(data['C'], (64, 64)).to(device)\r\n",
        "            label = torch.full((1,), real_label, dtype=torch.float, device=device)\r\n",
        "\r\n",
        "            # Forward pass real batch through D\r\n",
        "            output = netD(real_batch)\r\n",
        "\r\n",
        "            # Calculate loss on all-real batch\r\n",
        "            errD_real_1 = criterionD(output['prediction_1'].view(-1), label)\r\n",
        "            errD_real_2 = criterionD(output['prediction_2'].view(-1), label)\r\n",
        "            errD_real_4 = criterionD(output['prediction_4'].view(-1), label)\r\n",
        "            errD_real_8 = criterionD(output['prediction_8'].view(-1), label)\r\n",
        "            \r\n",
        "\r\n",
        "            errD_real = errD_real_1 + errD_real_2 + errD_real_4 + errD_real_8\r\n",
        "\r\n",
        "\r\n",
        "            # Calculate gradients for D in backward pass\r\n",
        "            errD_real.backward()\r\n",
        "\r\n",
        "\r\n",
        "            ## Train with all-fake batch\r\n",
        "            fake = netG(data_a, part_locations=data_part_locations)\r\n",
        "\r\n",
        "            # Format batch\r\n",
        "            fake_batch  = {}\r\n",
        "            fake_batch['1'] = fake.detach()\r\n",
        "            fake_batch['2'] = F.interpolate(fake, (256, 256)).to(device)\r\n",
        "            fake_batch['4'] = F.interpolate(fake, (128, 128)).to(device)\r\n",
        "            fake_batch['8'] = F.interpolate(fake, (64, 64)).to(device)\r\n",
        "            label.fill_(fake_label)\r\n",
        "\r\n",
        "            # Classify all fake batch with D\r\n",
        "            output_fake = netD(fake_batch)\r\n",
        "\r\n",
        "            # Calculate D's loss on the all-fake batch\r\n",
        "            errD_fake_1 = criterionD(output_fake['prediction_1'].view(-1), label)\r\n",
        "            errD_fake_2 = criterionD(output_fake['prediction_2'].view(-1), label)\r\n",
        "            errD_fake_4 = criterionD(output_fake['prediction_4'].view(-1), label)\r\n",
        "            errD_fake_8 = criterionD(output_fake['prediction_8'].view(-1), label)\r\n",
        "\r\n",
        "            errD_fake = errD_fake_1 + errD_fake_2 + errD_fake_4 + errD_fake_8\r\n",
        "\r\n",
        "            # Calculate the gradients for this batch\r\n",
        "            errD_fake.backward()\r\n",
        "            \r\n",
        "            errD = errD_real + errD_fake\r\n",
        "            \r\n",
        "#             errD.backward()\r\n",
        "            \r\n",
        "            # Update D\r\n",
        "            optimizerD.step()\r\n",
        "\r\n",
        "\r\n",
        "            ###########################\r\n",
        "            # (2) Update G network: maximize log(D(G(z)))\r\n",
        "            ###########################\r\n",
        "            netG.zero_grad()\r\n",
        "            fake = netG(data_a, part_locations=data_part_locations)\r\n",
        "            mse_loss = criterionG(fake, data_c)\r\n",
        "            \r\n",
        "            #             VGG_LOSS\r\n",
        "            L_p_vgg19 = 0\r\n",
        "            perceptual_dst_pred = perceptual_loss_vgg19(fake)\r\n",
        "            with torch.no_grad():\r\n",
        "                perceptual_dst_img = perceptual_loss_vgg19(data_c)                    \r\n",
        "            for k in range(4):\r\n",
        "                L_p_vgg19 += weights_layer_perceptual[k] * torch.nn.MSELoss()(perceptual_dst_pred[k],perceptual_dst_img[k])\r\n",
        "\r\n",
        "            L_p = L_p_vgg19\r\n",
        "            \r\n",
        "            ms_ssim_loss = 1 - ms_ssim_module(fake, data_c)\r\n",
        "\r\n",
        "            # Format batch\r\n",
        "            fake_batch  = {}\r\n",
        "            fake_batch['1'] = fake\r\n",
        "            fake_batch['2'] = F.interpolate(fake, (256, 256)).to(device)\r\n",
        "            fake_batch['4'] = F.interpolate(fake, (128, 128)).to(device)\r\n",
        "            fake_batch['8'] = F.interpolate(fake, (64, 64)).to(device)\r\n",
        "            label.fill_(fake_label)\r\n",
        "\r\n",
        "            label.fill_(real_label)  # fake labels are real for generator cost\r\n",
        "\r\n",
        "            # Since we just updated D, perform another forward pass of all-fake batch through D\r\n",
        "            output = netD(fake_batch)\r\n",
        "\r\n",
        "            # Calculate G's loss based on this output\r\n",
        "#             err_hinge_G = hinge_G(output['prediction_1']) + hinge_G(output['prediction_2']) + hinge_G(output['prediction_4']) + hinge_G(output['prediction_8'])\r\n",
        "#             adversarial = 4 * output_fake['prediction_1'] + 2 * output_fake['prediction_2'] + output_fake['prediction_4'] + output_fake['prediction_8']\r\n",
        "            # Adversarial loss\r\n",
        "            adversarial = 4. - (output['prediction_1'].view(-1) + output['prediction_2'].view(-1) + output['prediction_4'].view(-1) + output['prediction_8'].view(-1))\r\n",
        "            errG = mse_loss + L_p + adversarial + 100*ms_ssim_loss\r\n",
        "\r\n",
        "            # Calculate gradients for G\r\n",
        "            errG.backward()\r\n",
        "\r\n",
        "            # Update G\r\n",
        "            optimizerG.step()\r\n",
        "            \r\n",
        "            writer.add_scalar('perceptual_loss', L_p.item(), epoch * len(trainloader) + i)\r\n",
        "            writer.add_scalar('mse_loss', mse_loss.item(), epoch * len(trainloader) + i)\r\n",
        "            writer.add_scalar('adversarial', adversarial.item(), epoch * len(trainloader) + i)\r\n",
        "            writer.add_scalar('ms-ssim', 100*ms_ssim_loss.item(), epoch * len(trainloader) + i)\r\n",
        "            writer.add_scalar('loss_G', errG, epoch * len(trainloader) + i)\r\n",
        "            writer.add_scalar('loss_D', errD, epoch * len(trainloader) + i)\r\n",
        "            \r\n",
        "            \r\n",
        "            # Output training stats\r\n",
        "            if i % 10 == 0 and i != 0:                \r\n",
        "                torch.save(netG.state_dict(), f'weights/netG_epoch_{epoch}_i_{i}.pth')\r\n",
        "                torch.save(netD.state_dict(), f'weights/netD_epoch_{epoch}_i_{i}.pth')\r\n",
        "                \r\n",
        "                # lr, fake, hr\r\n",
        "                images = [data_a[0].cpu(), fake[0].cpu(), data_c[0].cpu()]\r\n",
        "                report_img = make_grid(images)\r\n",
        "\r\n",
        "\r\n",
        "                \r\n",
        "                #logger.report_image('image', f'epoch_{epoch}, iter_{i}', iteration=epoch * len(trainloader) + i, image=tensor2im(report_img))\r\n",
        "                #logger.flush()\r\n",
        "                #print(f'epoch: {epoch}, iter: {i}, perceptual_loss: {perceptual_loss}, mse_loss: {mse_loss}, adversarial: {adversarial}, ms-ssim: {ms-ssim}, loss_G: {loss_G}, loss_D: {loss_D}')\r\n",
        "\r\n",
        "                print(f'epoch: {epoch}, iter: {i}, perceptual_loss: {L_p.item():.3f}, mse_loss: {mse_loss.item():.3f}, adversarial: {adversarial.item():.3f}, ms-ssim: {100*ms_ssim_loss.item():.3f}, loss_G: {errG.cpu().detach().numpy()[0]:.3f}, loss_D: {errD.cpu().detach().numpy():.3f}')\r\n",
        "                save_image(report_img, f'epoch_{epoch}_i_{i}.png')\r\n",
        "                \r\n",
        "                # tensorboard logging\r\n",
        "                writer.add_scalar('perceptual_loss', L_p.item(), epoch * len(trainloader) + i)\r\n",
        "                writer.add_scalar('mse_loss', mse_loss.item(), epoch * len(trainloader) + i)\r\n",
        "                writer.add_scalar('adversarial', adversarial.item(), epoch * len(trainloader) + i)\r\n",
        "                writer.add_scalar('ms-ssim', 100*ms_ssim_loss.item(), epoch * len(trainloader) + i)\r\n",
        "                writer.add_scalar('loss_G', errG.cpu().detach().numpy()[0], epoch * len(trainloader) + i)\r\n",
        "                writer.add_scalar('loss_D', errD.cpu().detach().numpy(), epoch * len(trainloader) + i)\r\n",
        "\r\n",
        "def main():\r\n",
        "    \r\n",
        "    # tensorboard\r\n",
        "    writer = SummaryWriter()\r\n",
        "\r\n",
        "    # trains parameters dict\r\n",
        "    parameters_dict = {\r\n",
        "        'optimizerG': 'Adam (0.5, 0.99)',\r\n",
        "        'optimizerD': 'Adam (0.5, 0.99)',\r\n",
        "        'learning_rate': '2e-4',\r\n",
        "        'dataset': 'celeba (30k)',\r\n",
        "        'resolution': '512',\r\n",
        "    }\r\n",
        "    \r\n",
        "    # init trains\r\n",
        "    #task = Task.init(project_name='face_enhancement', task_name='Exp. 2.5, res 512, celeba 30k, bce + ms_ssim')\r\n",
        "    #logger = task.get_logger()\r\n",
        "    \r\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \r\n",
        "\r\n",
        "    dataset = AlignedDataset('dataset_celeba/images', fine_size=512)\r\n",
        "    trainloader = DataLoader(dataset,\r\n",
        "                            batch_size=1,\r\n",
        "                            shuffle=True,\r\n",
        "                            num_workers=8)\r\n",
        "    netG = networks.define_G('UNetDictFace', ['cuda:0'])\r\n",
        "    netD = MultiScaleDiscriminator(scales=(1, 2, 4, 8))\r\n",
        "    netD.to('cuda:0')         \r\n",
        "    \r\n",
        "#     netG.load_state_dict(torch.load('weights/netG_30k_epoch0_exp2_1.pth'))\r\n",
        "#     netD.load_state_dict(torch.load('weights/netD_30k_epoch0_exp2_1.pth'))\r\n",
        "    \r\n",
        "    cfg_str = str(netG) + str('\\n\\n Discriminator:\\n\\n') + str(netD)\r\n",
        "    #Task.current_task().set_model_config(cfg_str)\r\n",
        "    \r\n",
        "    # connect the dictionary to TRAINS Task\r\n",
        "    #parameters_dict = Task.current_task().connect(parameters_dict)\r\n",
        "    \r\n",
        "    train(device, dataset, trainloader, netG, netD, writer)\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "if __name__ == '__main__':\r\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwpnHjS1B5QK",
        "cellView": "form"
      },
      "source": [
        "#@title custom_dataset.py (paths)\r\n",
        "%%writefile /content/DFDNet/data/custom_dataset.py\r\n",
        "# -- coding: utf-8 --\r\n",
        "import os.path\r\n",
        "import os\r\n",
        "import random\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torch\r\n",
        "from PIL import Image, ImageFilter\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import math\r\n",
        "from scipy.io import loadmat\r\n",
        "from PIL import Image\r\n",
        "import PIL\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "import glob\r\n",
        "\r\n",
        "class AlignedDataset(Dataset):\r\n",
        "    \r\n",
        "    def __init__(self, root_dir, fine_size=512, transform=None):\r\n",
        "        self.root_dir = '/content/DFDNet/ffhq'\r\n",
        "        #self.pathes = [os.path.join(self.root_dir, x) for x in os.listdir(self.root_dir) if x[-3:] == 'jpg']\r\n",
        "        \r\n",
        "        self.pathes = glob.glob(self.root_dir + '/**/*.png', recursive=True)\r\n",
        "\r\n",
        "        #print(\"self.pathes\")\r\n",
        "        #print(self.pathes)\r\n",
        "        self.transform = transform\r\n",
        "        self.fine_size = fine_size\r\n",
        "        self.partpath = '/content/DFDNet/landmarks'\r\n",
        "        \r\n",
        "    def AddNoise(self,img): # noise\r\n",
        "        if random.random() > 0.9: #\r\n",
        "            return img\r\n",
        "        self.sigma = np.random.randint(1, 11)\r\n",
        "        img_tensor = torch.from_numpy(np.array(img)).float()\r\n",
        "        noise = torch.randn(img_tensor.size()).mul_(self.sigma/1.0)\r\n",
        "\r\n",
        "        noiseimg = torch.clamp(noise+img_tensor,0,255)\r\n",
        "        return Image.fromarray(np.uint8(noiseimg.numpy()))\r\n",
        "\r\n",
        "    def AddBlur(self,img): # gaussian blur or motion blur\r\n",
        "        if random.random() > 0.9: #\r\n",
        "            return img\r\n",
        "        img = np.array(img)\r\n",
        "        if random.random() > 0.35: ##gaussian blur\r\n",
        "            blursize = random.randint(1,17) * 2 + 1 ##3,5,7,9,11,13,15\r\n",
        "            blursigma = random.randint(3, 20)\r\n",
        "            img = cv2.GaussianBlur(img, (blursize,blursize), blursigma/10)\r\n",
        "        else: #motion blur\r\n",
        "            M = random.randint(1,32)\r\n",
        "            KName = './data/MotionBlurKernel/m_%02d.mat' % M\r\n",
        "            k = loadmat(KName)['kernel']\r\n",
        "            k = k.astype(np.float32)\r\n",
        "            k /= np.sum(k)\r\n",
        "            img = cv2.filter2D(img,-1,k)\r\n",
        "        return Image.fromarray(img)\r\n",
        "\r\n",
        "    def AddDownSample(self,img): # downsampling\r\n",
        "        if random.random() > 0.95: #\r\n",
        "            return img\r\n",
        "        sampler = random.randint(20, 100)*1.0\r\n",
        "        img = img.resize((int(self.fine_size/sampler*10.0), int(self.fine_size/sampler*10.0)), Image.BICUBIC)\r\n",
        "        return img\r\n",
        "\r\n",
        "    def AddJPEG(self,img): # JPEG compression\r\n",
        "        if random.random() > 0.6:\r\n",
        "            return img\r\n",
        "        imQ = random.randint(40, 80)\r\n",
        "        img = np.array(img)\r\n",
        "        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY),imQ] # (0,100),higher is better,default is 95\r\n",
        "        _, encA = cv2.imencode('.jpg', img, encode_param)\r\n",
        "        img = cv2.imdecode(encA,1)\r\n",
        "        return Image.fromarray(img)\r\n",
        "\r\n",
        "    def AddUpSample(self,img):\r\n",
        "        return img.resize((self.fine_size, self.fine_size), Image.BICUBIC)\r\n",
        "\r\n",
        "    def __getitem__(self, index): # indexation\r\n",
        "\r\n",
        "        path = self.pathes[index]\r\n",
        "        Imgs = Image.open(path).convert('RGB')\r\n",
        "        \r\n",
        "        A = Imgs.resize((self.fine_size, self.fine_size))\r\n",
        "        A = transforms.ColorJitter(0.3, 0.3, 0.3, 0)(A)\r\n",
        "        C = A\r\n",
        "        A = self.AddBlur(A)\r\n",
        "        \r\n",
        "        tmps = path.split('/')\r\n",
        "        ImgName = tmps[-1]\r\n",
        "        part_locations = self.get_part_location(self.partpath, ImgName, 2)\r\n",
        "        \r\n",
        "        A = transforms.ToTensor()(A)\r\n",
        "        C = transforms.ToTensor()(C)\r\n",
        "        \r\n",
        "        A = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(A) \r\n",
        "        C = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(C)\r\n",
        "        \r\n",
        "        return {'A': A, 'C': C, 'path': path, 'part_locations': part_locations}\r\n",
        "\r\n",
        "    def get_part_location(self, landmarkpath, imgname, downscale=1):\r\n",
        "        Landmarks = []\r\n",
        "        with open(os.path.join(landmarkpath, imgname + '.txt'),'r') as f:\r\n",
        "            for line in f:\r\n",
        "                tmp = [np.float(i) for i in line.split(' ') if i != '\\n']\r\n",
        "                Landmarks.append(tmp)\r\n",
        "        Landmarks = np.array(Landmarks)/downscale # 512 * 512\r\n",
        "        \r\n",
        "        Map_LE = list(np.hstack((range(17,22), range(36,42))))\r\n",
        "        Map_RE = list(np.hstack((range(22,27), range(42,48))))\r\n",
        "        Map_NO = list(range(29,36))\r\n",
        "        Map_MO = list(range(48,68))\r\n",
        "        #left eye\r\n",
        "        Mean_LE = np.mean(Landmarks[Map_LE],0)\r\n",
        "        L_LE = np.max((np.max(np.max(Landmarks[Map_LE],0) - np.min(Landmarks[Map_LE],0))/2,16))\r\n",
        "        Location_LE = np.hstack((Mean_LE - L_LE + 1, Mean_LE + L_LE)).astype(int)\r\n",
        "        #right eye\r\n",
        "        Mean_RE = np.mean(Landmarks[Map_RE],0)\r\n",
        "        L_RE = np.max((np.max(np.max(Landmarks[Map_RE],0) - np.min(Landmarks[Map_RE],0))/2,16))\r\n",
        "        Location_RE = np.hstack((Mean_RE - L_RE + 1, Mean_RE + L_RE)).astype(int)\r\n",
        "        #nose\r\n",
        "        Mean_NO = np.mean(Landmarks[Map_NO],0)\r\n",
        "        L_NO = np.max((np.max(np.max(Landmarks[Map_NO],0) - np.min(Landmarks[Map_NO],0))/2,16))\r\n",
        "        Location_NO = np.hstack((Mean_NO - L_NO + 1, Mean_NO + L_NO)).astype(int)\r\n",
        "        #mouth\r\n",
        "        Mean_MO = np.mean(Landmarks[Map_MO],0)\r\n",
        "        L_MO = np.max((np.max(np.max(Landmarks[Map_MO],0) - np.min(Landmarks[Map_MO],0))/2,16))\r\n",
        "\r\n",
        "        Location_MO = np.hstack((Mean_MO - L_MO + 1, Mean_MO + L_MO)).astype(int)\r\n",
        "        return Location_LE, Location_RE, Location_NO, Location_MO\r\n",
        "\r\n",
        "    def __len__(self): #\r\n",
        "        return len(self.pathes)\r\n",
        "\r\n",
        "    def name(self):\r\n",
        "        return 'AlignedDataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE174rSPoms9",
        "cellView": "form"
      },
      "source": [
        "#@title networks.py (debugging prints (disabled))\r\n",
        "%%writefile /content/DFDNet/models/networks.py\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.nn import init\r\n",
        "import functools\r\n",
        "from torch.optim import lr_scheduler\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.nn import Parameter as P\r\n",
        "from util import util\r\n",
        "from torchvision import models\r\n",
        "import scipy.io as sio\r\n",
        "import numpy as np\r\n",
        "import scipy.ndimage\r\n",
        "import torch.nn.utils.spectral_norm as SpectralNorm\r\n",
        "\r\n",
        "from torch.autograd import Function\r\n",
        "from math import sqrt\r\n",
        "import random\r\n",
        "import os\r\n",
        "import math\r\n",
        "\r\n",
        "from sync_batchnorm import convert_model\r\n",
        "####\r\n",
        "\r\n",
        "###############################################################################\r\n",
        "# Helper Functions\r\n",
        "###############################################################################\r\n",
        "def get_norm_layer(norm_type='instance'):\r\n",
        "    if norm_type == 'batch':\r\n",
        "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\r\n",
        "    elif norm_type == 'instance':\r\n",
        "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\r\n",
        "    elif norm_type == 'none':\r\n",
        "        norm_layer = None\r\n",
        "    else:\r\n",
        "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\r\n",
        "\r\n",
        "    return norm_layer\r\n",
        "\r\n",
        "\r\n",
        "def get_scheduler(optimizer, opt):\r\n",
        "    if opt.lr_policy == 'lambda':\r\n",
        "        def lambda_rule(epoch):\r\n",
        "            lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\r\n",
        "            return lr_l\r\n",
        "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\r\n",
        "    elif opt.lr_policy == 'step':\r\n",
        "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\r\n",
        "    elif opt.lr_policy == 'plateau':\r\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\r\n",
        "    else:\r\n",
        "        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\r\n",
        "\r\n",
        "    return scheduler\r\n",
        "\r\n",
        "\r\n",
        "def init_weights(net, init_type='normal', gain=0.02):\r\n",
        "    def init_func(m):\r\n",
        "        classname = m.__class__.__name__\r\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\r\n",
        "            if init_type == 'normal':\r\n",
        "                init.normal_(m.weight.data, 0.0, gain)\r\n",
        "            elif init_type == 'xavier':\r\n",
        "                init.xavier_normal_(m.weight.data, gain=gain)\r\n",
        "            elif init_type == 'kaiming':\r\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\r\n",
        "            elif init_type == 'orthogonal':\r\n",
        "                init.orthogonal_(m.weight.data, gain=gain)\r\n",
        "            else:\r\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\r\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\r\n",
        "                init.constant_(m.bias.data, 0.0)\r\n",
        "        elif classname.find('BatchNorm2d') != -1:\r\n",
        "            init.normal_(m.weight.data, 1.0, gain)\r\n",
        "            init.constant_(m.bias.data, 0.0)\r\n",
        "\r\n",
        "    print('initialize network with %s' % init_type)\r\n",
        "    net.apply(init_func)\r\n",
        "\r\n",
        "\r\n",
        "def init_net(net, init_type='normal', init_gain=0.02, gpu_ids=[], init_flag=True):\r\n",
        "    if len(gpu_ids) > 0:\r\n",
        "        assert(torch.cuda.is_available())\r\n",
        "        net = convert_model(net)\r\n",
        "        net.to(gpu_ids[0])\r\n",
        "        net = torch.nn.DataParallel(net, gpu_ids)\r\n",
        "\r\n",
        "    if init_flag:\r\n",
        "\r\n",
        "        init_weights(net, init_type, gain=init_gain)\r\n",
        "\r\n",
        "    return net\r\n",
        "\r\n",
        "\r\n",
        "# compute adaptive instance norm\r\n",
        "def calc_mean_std(feat, eps=1e-5):\r\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\r\n",
        "    size = feat.size()\r\n",
        "    assert (len(size) == 3)\r\n",
        "    C, _ = size[:2]\r\n",
        "    feat_var = feat.contiguous().view(C, -1).var(dim=1) + eps\r\n",
        "    feat_std = feat_var.sqrt().view(C, 1, 1)\r\n",
        "    feat_mean = feat.contiguous().view(C, -1).mean(dim=1).view(C, 1, 1)\r\n",
        "\r\n",
        "    return feat_mean, feat_std\r\n",
        "\r\n",
        "\r\n",
        "def adaptive_instance_normalization(content_feat, style_feat):  # content_feat is degraded feature, style is ref feature\r\n",
        "    assert (content_feat.size()[:1] == style_feat.size()[:1])\r\n",
        "    size = content_feat.size()\r\n",
        "    style_mean, style_std = calc_mean_std(style_feat)\r\n",
        "    content_mean, content_std = calc_mean_std(content_feat)\r\n",
        "\r\n",
        "    normalized_feat = (content_feat - content_mean.expand(\r\n",
        "        size)) / content_std.expand(size)\r\n",
        "\r\n",
        "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\r\n",
        "\r\n",
        "def calc_mean_std_4D(feat, eps=1e-5):\r\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\r\n",
        "    size = feat.size()\r\n",
        "    assert (len(size) == 4)\r\n",
        "    N, C = size[:2]\r\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\r\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\r\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\r\n",
        "    return feat_mean, feat_std\r\n",
        "\r\n",
        "def adaptive_instance_normalization_4D(content_feat, style_feat): # content_feat is ref feature, style is degradate feature\r\n",
        "    # assert (content_feat.size()[:2] == style_feat.size()[:2])\r\n",
        "    size = content_feat.size()\r\n",
        "    style_mean, style_std = calc_mean_std_4D(style_feat)\r\n",
        "    content_mean, content_std = calc_mean_std_4D(content_feat)\r\n",
        "    normalized_feat = (content_feat - content_mean.expand(\r\n",
        "        size)) / content_std.expand(size)\r\n",
        "    return normalized_feat * style_std + style_mean\r\n",
        "\r\n",
        "def define_G(which_model_netG, gpu_ids=[]):\r\n",
        "    if which_model_netG == 'UNetDictFace':\r\n",
        "        netG = UNetDictFace(64)\r\n",
        "        init_flag = False\r\n",
        "    else:\r\n",
        "        raise NotImplementedError('Generator model name [%s] is not recognized' % which_model_netG)\r\n",
        "    return init_net(netG, 'normal', 0.02, gpu_ids, init_flag)\r\n",
        "\r\n",
        "\r\n",
        "##############################################################################\r\n",
        "# Classes\r\n",
        "############################################################################################################################################\r\n",
        "\r\n",
        "\r\n",
        "def convU(in_channels, out_channels,conv_layer, norm_layer, kernel_size=3, stride=1,dilation=1, bias=True):\r\n",
        "    return nn.Sequential(\r\n",
        "        SpectralNorm(conv_layer(in_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias)),\r\n",
        "#         conv_layer(in_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias),\r\n",
        "#         nn.BatchNorm2d(out_channels),\r\n",
        "        nn.LeakyReLU(0.2),\r\n",
        "        SpectralNorm(conv_layer(out_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias)),\r\n",
        "    )\r\n",
        "class MSDilateBlock(nn.Module):\r\n",
        "    def __init__(self, in_channels,conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, kernel_size=3, dilation=[1,1,1,1], bias=True):\r\n",
        "        super(MSDilateBlock, self).__init__()\r\n",
        "        self.conv1 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[0], bias=bias)\r\n",
        "        self.conv2 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[1], bias=bias)\r\n",
        "        self.conv3 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[2], bias=bias)\r\n",
        "        self.conv4 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[3], bias=bias)\r\n",
        "        self.convi =  SpectralNorm(conv_layer(in_channels*4, in_channels, kernel_size=kernel_size, stride=1, padding=(kernel_size-1)//2, bias=bias))\r\n",
        "    def forward(self, x):\r\n",
        "        conv1 = self.conv1(x)\r\n",
        "        conv2 = self.conv2(x)\r\n",
        "        conv3 = self.conv3(x)\r\n",
        "        conv4 = self.conv4(x)\r\n",
        "        cat  = torch.cat([conv1, conv2, conv3, conv4], 1)\r\n",
        "        out = self.convi(cat) + x\r\n",
        "        return out\r\n",
        "\r\n",
        "##############################UNetFace#########################\r\n",
        "class AdaptiveInstanceNorm(nn.Module):\r\n",
        "    def __init__(self, in_channel):\r\n",
        "        super().__init__()\r\n",
        "        self.norm = nn.InstanceNorm2d(in_channel)\r\n",
        "\r\n",
        "    def forward(self, input, style):\r\n",
        "        style_mean, style_std = calc_mean_std_4D(style)\r\n",
        "        out = self.norm(input)\r\n",
        "        size = input.size()\r\n",
        "        out = style_std.expand(size) * out + style_mean.expand(size)\r\n",
        "        return out\r\n",
        "\r\n",
        "class BlurFunctionBackward(Function):\r\n",
        "    @staticmethod\r\n",
        "    def forward(ctx, grad_output, kernel, kernel_flip):\r\n",
        "        ctx.save_for_backward(kernel, kernel_flip)\r\n",
        "\r\n",
        "        grad_input = F.conv2d(\r\n",
        "            grad_output, kernel_flip, padding=1, groups=grad_output.shape[1]\r\n",
        "        )\r\n",
        "        return grad_input\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def backward(ctx, gradgrad_output):\r\n",
        "        kernel, kernel_flip = ctx.saved_tensors\r\n",
        "\r\n",
        "        grad_input = F.conv2d(\r\n",
        "            gradgrad_output, kernel, padding=1, groups=gradgrad_output.shape[1]\r\n",
        "        )\r\n",
        "        return grad_input, None, None\r\n",
        "\r\n",
        "\r\n",
        "class BlurFunction(Function):\r\n",
        "    @staticmethod\r\n",
        "    def forward(ctx, input, kernel, kernel_flip):\r\n",
        "        ctx.save_for_backward(kernel, kernel_flip)\r\n",
        "\r\n",
        "        output = F.conv2d(input, kernel, padding=1, groups=input.shape[1])\r\n",
        "\r\n",
        "        return output\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def backward(ctx, grad_output):\r\n",
        "        kernel, kernel_flip = ctx.saved_tensors\r\n",
        "\r\n",
        "        grad_input = BlurFunctionBackward.apply(grad_output, kernel, kernel_flip)\r\n",
        "\r\n",
        "        return grad_input, None, None\r\n",
        "\r\n",
        "blur = BlurFunction.apply\r\n",
        "\r\n",
        "\r\n",
        "class Blur(nn.Module):\r\n",
        "    def __init__(self, channel):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        weight = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=torch.float32)\r\n",
        "        weight = weight.view(1, 1, 3, 3)\r\n",
        "        weight = weight / weight.sum()\r\n",
        "        weight_flip = torch.flip(weight, [2, 3])\r\n",
        "\r\n",
        "        self.register_buffer('weight', weight.repeat(channel, 1, 1, 1))\r\n",
        "        self.register_buffer('weight_flip', weight_flip.repeat(channel, 1, 1, 1))\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        return blur(input, self.weight, self.weight_flip)\r\n",
        "\r\n",
        "class EqualLR:\r\n",
        "    def __init__(self, name):\r\n",
        "        self.name = name\r\n",
        "\r\n",
        "    def compute_weight(self, module):\r\n",
        "        weight = getattr(module, self.name + '_orig')\r\n",
        "        fan_in = weight.data.size(1) * weight.data[0][0].numel()\r\n",
        "        return weight * sqrt(2 / fan_in)\r\n",
        "    @staticmethod\r\n",
        "    def apply(module, name):\r\n",
        "        fn = EqualLR(name)\r\n",
        "\r\n",
        "        weight = getattr(module, name)\r\n",
        "        del module._parameters[name]\r\n",
        "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\r\n",
        "        module.register_forward_pre_hook(fn)\r\n",
        "\r\n",
        "        return fn\r\n",
        "\r\n",
        "    def __call__(self, module, input):\r\n",
        "        weight = self.compute_weight(module)\r\n",
        "        setattr(module, self.name, weight)\r\n",
        "\r\n",
        "def equal_lr(module, name='weight'):\r\n",
        "    EqualLR.apply(module, name)\r\n",
        "    return module\r\n",
        "\r\n",
        "class EqualConv2d(nn.Module):\r\n",
        "    def __init__(self, *args, **kwargs):\r\n",
        "        super().__init__()\r\n",
        "        conv = nn.Conv2d(*args, **kwargs)\r\n",
        "        conv.weight.data.normal_()\r\n",
        "        conv.bias.data.zero_()\r\n",
        "        self.conv = equal_lr(conv)\r\n",
        "    def forward(self, input):\r\n",
        "        return self.conv(input)\r\n",
        "\r\n",
        "class NoiseInjection(nn.Module):\r\n",
        "    def __init__(self, channel):\r\n",
        "        super().__init__()\r\n",
        "        self.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))\r\n",
        "    def forward(self, image, noise):\r\n",
        "        return image + self.weight * noise\r\n",
        "\r\n",
        "class StyledUpBlock(nn.Module):\r\n",
        "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1,upsample=False):\r\n",
        "        super().__init__()\r\n",
        "        if upsample:\r\n",
        "            self.conv1 = nn.Sequential(\r\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\r\n",
        "                Blur(out_channel),\r\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding),\r\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\r\n",
        "                nn.LeakyReLU(0.2),\r\n",
        "            )\r\n",
        "        else:\r\n",
        "            self.conv1 = nn.Sequential(\r\n",
        "                Blur(in_channel),\r\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding)\r\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\r\n",
        "                nn.LeakyReLU(0.2),\r\n",
        "            )\r\n",
        "        self.convup = nn.Sequential(\r\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\r\n",
        "                # EqualConv2d(out_channel, out_channel, kernel_size, padding=padding),\r\n",
        "                SpectralNorm(nn.Conv2d(out_channel, out_channel, kernel_size, padding=padding)),\r\n",
        "                nn.LeakyReLU(0.2),\r\n",
        "                # Blur(out_channel),\r\n",
        "            )\r\n",
        "        # self.noise1 = equal_lr(NoiseInjection(out_channel))\r\n",
        "        # self.adain1 = AdaptiveInstanceNorm(out_channel)\r\n",
        "        self.lrelu1 = nn.LeakyReLU(0.2)\r\n",
        "\r\n",
        "        # self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\r\n",
        "        # self.noise2 = equal_lr(NoiseInjection(out_channel))\r\n",
        "        # self.adain2 = AdaptiveInstanceNorm(out_channel)\r\n",
        "        # self.lrelu2 = nn.LeakyReLU(0.2)\r\n",
        "\r\n",
        "        self.ScaleModel1 = nn.Sequential(\r\n",
        "            # Blur(in_channel),\r\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\r\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\r\n",
        "            nn.LeakyReLU(0.2, True),\r\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1))\r\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\r\n",
        "        )\r\n",
        "        self.ShiftModel1 = nn.Sequential(\r\n",
        "            # Blur(in_channel),\r\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\r\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\r\n",
        "            nn.LeakyReLU(0.2, True),\r\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1)),\r\n",
        "            nn.Sigmoid(),\r\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\r\n",
        "        )\r\n",
        "       \r\n",
        "    def forward(self, input, style):\r\n",
        "        out = self.conv1(input)\r\n",
        "#         out = self.noise1(out, noise)\r\n",
        "        out = self.lrelu1(out)\r\n",
        "\r\n",
        "        Shift1 = self.ShiftModel1(style)\r\n",
        "        Scale1 = self.ScaleModel1(style)\r\n",
        "        out = out * Scale1 + Shift1\r\n",
        "        # out = self.adain1(out, style)\r\n",
        "        outup = self.convup(out)\r\n",
        "\r\n",
        "        return outup\r\n",
        "\r\n",
        "##############################################################################\r\n",
        "##Face Dictionary\r\n",
        "##############################################################################\r\n",
        "class VGGFeat(torch.nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Input: (B, C, H, W), RGB, [-1, 1]\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, weight_path='./weights/vgg19.pth'):\r\n",
        "        super().__init__()\r\n",
        "        self.model = models.vgg19(pretrained=False)\r\n",
        "        self.build_vgg_layers()\r\n",
        "        \r\n",
        "        self.model.load_state_dict(torch.load(weight_path))\r\n",
        "\r\n",
        "        self.register_parameter(\"RGB_mean\", nn.Parameter(torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)))\r\n",
        "        self.register_parameter(\"RGB_std\", nn.Parameter(torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)))\r\n",
        "        \r\n",
        "        # self.model.eval()\r\n",
        "        for param in self.model.parameters():\r\n",
        "            param.requires_grad = False\r\n",
        "    \r\n",
        "    def build_vgg_layers(self):\r\n",
        "        vgg_pretrained_features = self.model.features\r\n",
        "        self.features = []\r\n",
        "        # feature_layers = [0, 3, 8, 17, 26, 35]\r\n",
        "        feature_layers = [0, 8, 17, 26, 35]\r\n",
        "        for i in range(len(feature_layers)-1): \r\n",
        "            module_layers = torch.nn.Sequential() \r\n",
        "            for j in range(feature_layers[i], feature_layers[i+1]):\r\n",
        "                module_layers.add_module(str(j), vgg_pretrained_features[j])\r\n",
        "            self.features.append(module_layers)\r\n",
        "        self.features = torch.nn.ModuleList(self.features)\r\n",
        "\r\n",
        "    def preprocess(self, x):\r\n",
        "        x = (x + 1) / 2\r\n",
        "        x = (x - self.RGB_mean) / self.RGB_std\r\n",
        "        if x.shape[3] < 224:\r\n",
        "            x = torch.nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.preprocess(x)\r\n",
        "        features = []\r\n",
        "        for m in self.features:\r\n",
        "            # print(m)\r\n",
        "            x = m(x)\r\n",
        "            features.append(x)\r\n",
        "        return features \r\n",
        "\r\n",
        "def compute_sum(x, axis=None, keepdim=False):\r\n",
        "    if not axis:\r\n",
        "        axis = range(len(x.shape))\r\n",
        "    for i in sorted(axis, reverse=True):\r\n",
        "        x = torch.sum(x, dim=i, keepdim=keepdim)\r\n",
        "    return x\r\n",
        "def ToRGB(in_channel):\r\n",
        "    return nn.Sequential(\r\n",
        "        SpectralNorm(nn.Conv2d(in_channel,in_channel,3, 1, 1)),\r\n",
        "        nn.LeakyReLU(0.2),\r\n",
        "        SpectralNorm(nn.Conv2d(in_channel,3,3, 1, 1))\r\n",
        "    )\r\n",
        "\r\n",
        "def AttentionBlock(in_channel):\r\n",
        "    return nn.Sequential(\r\n",
        "        SpectralNorm(nn.Conv2d(in_channel, in_channel, 3, 1, 1)),\r\n",
        "        nn.LeakyReLU(0.2),\r\n",
        "        SpectralNorm(nn.Conv2d(in_channel, in_channel, 3, 1, 1))\r\n",
        "    )\r\n",
        "\r\n",
        "class UNetDictFace(nn.Module):\r\n",
        "    def __init__(self, ngf=64, dictionary_path='./DictionaryCenter512'):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.part_sizes = np.array([80,80,50,110]) # size for 512\r\n",
        "        self.feature_sizes = np.array([256,128,64,32])\r\n",
        "        self.channel_sizes = np.array([128,256,512,512])\r\n",
        "        Parts = ['left_eye','right_eye','nose','mouth']\r\n",
        "        self.Dict_256 = {}\r\n",
        "        self.Dict_128 = {}\r\n",
        "        self.Dict_64 = {}\r\n",
        "        self.Dict_32 = {}\r\n",
        "        for j,i in enumerate(Parts):\r\n",
        "            f_256 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_256_center.npy'.format(i)), allow_pickle=True))\r\n",
        "\r\n",
        "            f_256_reshape = f_256.reshape(f_256.size(0),self.channel_sizes[0],self.part_sizes[j]//2,self.part_sizes[j]//2)\r\n",
        "            max_256 = torch.max(torch.sqrt(compute_sum(torch.pow(f_256_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\r\n",
        "            self.Dict_256[i] = f_256_reshape #/ max_256\r\n",
        "\r\n",
        "            f_128 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_128_center.npy'.format(i)), allow_pickle=True))\r\n",
        "\r\n",
        "            f_128_reshape = f_128.reshape(f_128.size(0),self.channel_sizes[1],self.part_sizes[j]//4,self.part_sizes[j]//4)\r\n",
        "            max_128 = torch.max(torch.sqrt(compute_sum(torch.pow(f_128_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\r\n",
        "            self.Dict_128[i] = f_128_reshape #/ max_128\r\n",
        "\r\n",
        "            f_64 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_64_center.npy'.format(i)), allow_pickle=True))\r\n",
        "\r\n",
        "            f_64_reshape = f_64.reshape(f_64.size(0),self.channel_sizes[2],self.part_sizes[j]//8,self.part_sizes[j]//8)\r\n",
        "            max_64 = torch.max(torch.sqrt(compute_sum(torch.pow(f_64_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\r\n",
        "            self.Dict_64[i] = f_64_reshape #/ max_64\r\n",
        "\r\n",
        "            f_32 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_32_center.npy'.format(i)), allow_pickle=True))\r\n",
        "\r\n",
        "            f_32_reshape = f_32.reshape(f_32.size(0),self.channel_sizes[3],self.part_sizes[j]//16,self.part_sizes[j]//16)\r\n",
        "            max_32 = torch.max(torch.sqrt(compute_sum(torch.pow(f_32_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\r\n",
        "            self.Dict_32[i] = f_32_reshape #/ max_32\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        print(\"self.Dict_256\")\r\n",
        "        print(len(self.Dict_256))\r\n",
        "        print(\"self.Dict_256['left_eye'].shape\")\r\n",
        "        print(self.Dict_256['left_eye'].shape)\r\n",
        "        print(\"self.Dict_256['right_eye'].shape\")\r\n",
        "        print(self.Dict_256['right_eye'].shape)\r\n",
        "        print(\"self.Dict_256['nose'].shape\")\r\n",
        "        print(self.Dict_256['nose'].shape)\r\n",
        "        print(\"self.Dict_256['mouth'].shape\")\r\n",
        "        print(self.Dict_256['mouth'].shape)\r\n",
        "        print(\"-------------------\")\r\n",
        "        print(\"self.Dict_128\")\r\n",
        "        print(len(self.Dict_128))\r\n",
        "        print(\"self.Dict_128['left_eye'].shape\")\r\n",
        "        print(self.Dict_128['left_eye'].shape)\r\n",
        "        print(\"self.Dict_128['right_eye'].shape\")\r\n",
        "        print(self.Dict_128['right_eye'].shape)\r\n",
        "        print(\"self.Dict_128['nose'].shape\")\r\n",
        "        print(self.Dict_128['nose'].shape)\r\n",
        "        print(\"self.Dict_128['mouth'].shape\")\r\n",
        "        print(self.Dict_128['mouth'].shape)\r\n",
        "        print(\"-------------------\")\r\n",
        "        print(\"self.Dict_64\")\r\n",
        "        print(len(self.Dict_64))\r\n",
        "        print(\"self.Dict_64['left_eye'].shape\")\r\n",
        "        print(self.Dict_64['left_eye'].shape)\r\n",
        "        print(\"self.Dict_64['right_eye'].shape\")\r\n",
        "        print(self.Dict_64['right_eye'].shape)\r\n",
        "        print(\"self.Dict_64['nose'].shape\")\r\n",
        "        print(self.Dict_64['nose'].shape)\r\n",
        "        print(\"self.Dict_64['mouth'].shape\")\r\n",
        "        print(self.Dict_64['mouth'].shape)\r\n",
        "        print(\"-------------------\")\r\n",
        "        print(\"self.Dict_32\")\r\n",
        "        print(len(self.Dict_32))\r\n",
        "        print(\"self.Dict_32['left_eye'].shape\")\r\n",
        "        print(self.Dict_32['left_eye'].shape)\r\n",
        "        print(\"self.Dict_32['right_eye'].shape\")\r\n",
        "        print(self.Dict_32['right_eye'].shape)\r\n",
        "        print(\"self.Dict_32['nose'].shape\")\r\n",
        "        print(self.Dict_32['nose'].shape)\r\n",
        "        print(\"self.Dict_32['mouth'].shape\")\r\n",
        "        print(self.Dict_32['mouth'].shape)\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        self.le_256 = AttentionBlock(128)\r\n",
        "        self.le_128 = AttentionBlock(256)\r\n",
        "        self.le_64 = AttentionBlock(512)\r\n",
        "        self.le_32 = AttentionBlock(512)\r\n",
        "\r\n",
        "        self.re_256 = AttentionBlock(128)\r\n",
        "        self.re_128 = AttentionBlock(256)\r\n",
        "        self.re_64 = AttentionBlock(512)\r\n",
        "        self.re_32 = AttentionBlock(512)\r\n",
        "\r\n",
        "        self.no_256 = AttentionBlock(128)\r\n",
        "        self.no_128 = AttentionBlock(256)\r\n",
        "        self.no_64 = AttentionBlock(512)\r\n",
        "        self.no_32 = AttentionBlock(512)\r\n",
        "\r\n",
        "        self.mo_256 = AttentionBlock(128)\r\n",
        "        self.mo_128 = AttentionBlock(256)\r\n",
        "        self.mo_64 = AttentionBlock(512)\r\n",
        "        self.mo_32 = AttentionBlock(512)\r\n",
        "\r\n",
        "        #norm\r\n",
        "        self.VggExtract = VGGFeat()\r\n",
        "        \r\n",
        "        ######################\r\n",
        "        self.MSDilate = MSDilateBlock(ngf*8, dilation = [4,3,2,1])  #\r\n",
        "\r\n",
        "        self.up0 = StyledUpBlock(ngf*8,ngf*8)\r\n",
        "        self.up1 = StyledUpBlock(ngf*8, ngf*4) #\r\n",
        "        self.up2 = StyledUpBlock(ngf*4, ngf*2) #\r\n",
        "        self.up3 = StyledUpBlock(ngf*2, ngf) #\r\n",
        "        self.up4 = nn.Sequential( # 128\r\n",
        "            # nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\r\n",
        "            SpectralNorm(nn.Conv2d(ngf, ngf, 3, 1, 1)),\r\n",
        "            # nn.BatchNorm2d(32),\r\n",
        "            nn.LeakyReLU(0.2),\r\n",
        "            UpResBlock(ngf),\r\n",
        "            UpResBlock(ngf),\r\n",
        "            # SpectralNorm(nn.Conv2d(ngf, 3, kernel_size=3, stride=1, padding=1)),\r\n",
        "            nn.Conv2d(ngf, 3, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.Tanh()\r\n",
        "        )\r\n",
        "        self.to_rgb0 = ToRGB(ngf*8)\r\n",
        "        self.to_rgb1 = ToRGB(ngf*4)\r\n",
        "        self.to_rgb2 = ToRGB(ngf*2)\r\n",
        "        self.to_rgb3 = ToRGB(ngf*1)\r\n",
        "\r\n",
        "        # for param in self.BlurInputConv.parameters():\r\n",
        "        #     param.requires_grad = False\r\n",
        "    \r\n",
        "    def forward(self, input, part_locations):\r\n",
        "        #print(\"input.shape\")\r\n",
        "        #print(input.shape)\r\n",
        "        VggFeatures = self.VggExtract(input) #VggFeatures = list object\r\n",
        "        # for b in range(input.size(0)):\r\n",
        "        b = 0\r\n",
        "        UpdateVggFeatures = []\r\n",
        "        for i, f_size in enumerate(self.feature_sizes):\r\n",
        "            cur_feature = VggFeatures[i]\r\n",
        "            #print(\"cur_feature.shape\")\r\n",
        "            #print(cur_feature.shape)\r\n",
        "\r\n",
        "            update_feature = cur_feature.clone() #* 0\r\n",
        "            cur_part_sizes = self.part_sizes // (512/f_size)\r\n",
        "            dicts_feature = getattr(self, 'Dict_'+str(f_size))\r\n",
        "            \r\n",
        "            LE_Dict_feature = dicts_feature['left_eye'].to(input)\r\n",
        "            RE_Dict_feature = dicts_feature['right_eye'].to(input)\r\n",
        "            NO_Dict_feature = dicts_feature['nose'].to(input)\r\n",
        "            MO_Dict_feature = dicts_feature['mouth'].to(input)\r\n",
        "\r\n",
        "            le_location = (part_locations[0][b] // (512/f_size)).int()\r\n",
        "            re_location = (part_locations[1][b] // (512/f_size)).int()\r\n",
        "            no_location = (part_locations[2][b] // (512/f_size)).int()\r\n",
        "            mo_location = (part_locations[3][b] // (512/f_size)).int()\r\n",
        "\r\n",
        "            LE_feature = cur_feature[:,:,le_location[1]:le_location[3],le_location[0]:le_location[2]].clone()\r\n",
        "            RE_feature = cur_feature[:,:,re_location[1]:re_location[3],re_location[0]:re_location[2]].clone()\r\n",
        "            NO_feature = cur_feature[:,:,no_location[1]:no_location[3],no_location[0]:no_location[2]].clone()\r\n",
        "            MO_feature = cur_feature[:,:,mo_location[1]:mo_location[3],mo_location[0]:mo_location[2]].clone()\r\n",
        "            \r\n",
        "            #resize\r\n",
        "            LE_feature_resize = F.interpolate(LE_feature,(LE_Dict_feature.size(2),LE_Dict_feature.size(3)),mode='bilinear',align_corners=False)\r\n",
        "            RE_feature_resize = F.interpolate(RE_feature,(RE_Dict_feature.size(2),RE_Dict_feature.size(3)),mode='bilinear',align_corners=False)\r\n",
        "            NO_feature_resize = F.interpolate(NO_feature,(NO_Dict_feature.size(2),NO_Dict_feature.size(3)),mode='bilinear',align_corners=False)\r\n",
        "            MO_feature_resize = F.interpolate(MO_feature,(MO_Dict_feature.size(2),MO_Dict_feature.size(3)),mode='bilinear',align_corners=False)\r\n",
        "            \r\n",
        "            #print(\"LE_feature_resize.shape\")\r\n",
        "            #print(LE_feature_resize.shape)\r\n",
        "\r\n",
        "            LE_Dict_feature_norm = adaptive_instance_normalization_4D(LE_Dict_feature, LE_feature_resize)\r\n",
        "            RE_Dict_feature_norm = adaptive_instance_normalization_4D(RE_Dict_feature, RE_feature_resize)\r\n",
        "            NO_Dict_feature_norm = adaptive_instance_normalization_4D(NO_Dict_feature, NO_feature_resize)\r\n",
        "            MO_Dict_feature_norm = adaptive_instance_normalization_4D(MO_Dict_feature, MO_feature_resize)\r\n",
        "            \r\n",
        "            LE_score = F.conv2d(LE_feature_resize, LE_Dict_feature_norm)\r\n",
        "\r\n",
        "            LE_score = F.softmax(LE_score.view(-1),dim=0)\r\n",
        "            LE_index = torch.argmax(LE_score)\r\n",
        "            LE_Swap_feature = F.interpolate(LE_Dict_feature_norm[LE_index:LE_index+1], (LE_feature.size(2), LE_feature.size(3)))\r\n",
        "\r\n",
        "            LE_Attention = getattr(self, 'le_'+str(f_size))(LE_Swap_feature-LE_feature)\r\n",
        "            LE_Att_feature = LE_Attention * LE_Swap_feature\r\n",
        "            \r\n",
        "\r\n",
        "            RE_score = F.conv2d(RE_feature_resize, RE_Dict_feature_norm)\r\n",
        "            RE_score = F.softmax(RE_score.view(-1),dim=0)\r\n",
        "            RE_index = torch.argmax(RE_score)\r\n",
        "            RE_Swap_feature = F.interpolate(RE_Dict_feature_norm[RE_index:RE_index+1], (RE_feature.size(2), RE_feature.size(3)))\r\n",
        "            \r\n",
        "            RE_Attention = getattr(self, 're_'+str(f_size))(RE_Swap_feature-RE_feature)\r\n",
        "            RE_Att_feature = RE_Attention * RE_Swap_feature\r\n",
        "\r\n",
        "            NO_score = F.conv2d(NO_feature_resize, NO_Dict_feature_norm)\r\n",
        "            NO_score = F.softmax(NO_score.view(-1),dim=0)\r\n",
        "            NO_index = torch.argmax(NO_score)\r\n",
        "            NO_Swap_feature = F.interpolate(NO_Dict_feature_norm[NO_index:NO_index+1], (NO_feature.size(2), NO_feature.size(3)))\r\n",
        "            \r\n",
        "            NO_Attention = getattr(self, 'no_'+str(f_size))(NO_Swap_feature-NO_feature)\r\n",
        "            NO_Att_feature = NO_Attention * NO_Swap_feature\r\n",
        "\r\n",
        "            \r\n",
        "            MO_score = F.conv2d(MO_feature_resize, MO_Dict_feature_norm)\r\n",
        "            MO_score = F.softmax(MO_score.view(-1),dim=0)\r\n",
        "            MO_index = torch.argmax(MO_score)\r\n",
        "            MO_Swap_feature = F.interpolate(MO_Dict_feature_norm[MO_index:MO_index+1], (MO_feature.size(2), MO_feature.size(3)))\r\n",
        "            \r\n",
        "            MO_Attention = getattr(self, 'mo_'+str(f_size))(MO_Swap_feature-MO_feature)\r\n",
        "            MO_Att_feature = MO_Attention * MO_Swap_feature\r\n",
        "\r\n",
        "            update_feature[:,:,le_location[1]:le_location[3],le_location[0]:le_location[2]] = LE_Att_feature + LE_feature\r\n",
        "            update_feature[:,:,re_location[1]:re_location[3],re_location[0]:re_location[2]] = RE_Att_feature + RE_feature\r\n",
        "            update_feature[:,:,no_location[1]:no_location[3],no_location[0]:no_location[2]] = NO_Att_feature + NO_feature\r\n",
        "            update_feature[:,:,mo_location[1]:mo_location[3],mo_location[0]:mo_location[2]] = MO_Att_feature + MO_feature\r\n",
        "\r\n",
        "            UpdateVggFeatures.append(update_feature) \r\n",
        "        \r\n",
        "        fea_vgg = self.MSDilate(VggFeatures[3])\r\n",
        "        #new version\r\n",
        "        fea_up0 = self.up0(fea_vgg, UpdateVggFeatures[3])\r\n",
        "        # out1 = F.interpolate(fea_up0,(512,512))\r\n",
        "        # out1 = self.to_rgb0(out1)\r\n",
        "\r\n",
        "        fea_up1 = self.up1( fea_up0, UpdateVggFeatures[2]) #\r\n",
        "        # out2 = F.interpolate(fea_up1,(512,512))\r\n",
        "        # out2 = self.to_rgb1(out2)\r\n",
        "\r\n",
        "        fea_up2 = self.up2(fea_up1, UpdateVggFeatures[1]) #\r\n",
        "        # out3 = F.interpolate(fea_up2,(512,512))\r\n",
        "        # out3 = self.to_rgb2(out3)\r\n",
        "\r\n",
        "        fea_up3 = self.up3(fea_up2, UpdateVggFeatures[0]) #\r\n",
        "        # out4 = F.interpolate(fea_up3,(512,512))\r\n",
        "        # out4 = self.to_rgb3(out4)\r\n",
        "\r\n",
        "        output = self.up4(fea_up3) #\r\n",
        "        \r\n",
        "    \r\n",
        "        return output  #+ out4 + out3 + out2 + out1\r\n",
        "        #0 128 * 256 * 256\r\n",
        "        #1 256 * 128 * 128\r\n",
        "        #2 512 * 64 * 64\r\n",
        "        #3 512 * 32 * 32\r\n",
        "\r\n",
        "\r\n",
        "class UpResBlock(nn.Module):\r\n",
        "    def __init__(self, dim, conv_layer = nn.Conv2d, norm_layer = nn.BatchNorm2d):\r\n",
        "        super(UpResBlock, self).__init__()\r\n",
        "        self.Model = nn.Sequential(\r\n",
        "            # SpectralNorm(conv_layer(dim, dim, 3, 1, 1)),\r\n",
        "            conv_layer(dim, dim, 3, 1, 1),\r\n",
        "            # norm_layer(dim),\r\n",
        "            nn.LeakyReLU(0.2,True),\r\n",
        "            # SpectralNorm(conv_layer(dim, dim, 3, 1, 1)),\r\n",
        "            conv_layer(dim, dim, 3, 1, 1),\r\n",
        "        )\r\n",
        "    def forward(self, x):\r\n",
        "        out = x + self.Model(x)\r\n",
        "        return out\r\n",
        "\r\n",
        "class VggClassNet(nn.Module):\r\n",
        "    def __init__(self, select_layer = ['0','5','10','19']):\r\n",
        "        super(VggClassNet, self).__init__()\r\n",
        "        self.select = select_layer\r\n",
        "        self.vgg = models.vgg19(pretrained=True).features\r\n",
        "        for param in self.parameters():\r\n",
        "            param.requires_grad = False\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        features = []\r\n",
        "        for name, layer in self.vgg._modules.items():\r\n",
        "            x = layer(x)\r\n",
        "            if name in self.select:\r\n",
        "                features.append(x)\r\n",
        "        return features\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    print('this is network')\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmBl9Iin3fH8"
      },
      "source": [
        "%cd /content/DFDNet\r\n",
        "!python run.py --batchSize 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY5GzmkG6RTP"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoMpPsDi7EJ1"
      },
      "source": [
        "Add data to ``/content/input/<image.png>``. Currently just searching for ``.png``. Change the extention inside ``custom_dataset.py`` if you want. Change path to your own model. Output in ``/content/``. Also, don't forget to create landmarks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgXn9AYE1whc",
        "cellView": "form"
      },
      "source": [
        "#@title inference.py (fixing import, changing paths)\r\n",
        "%%writefile /content/DFDNet/inference.py\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "\r\n",
        "import math\r\n",
        "from PIL import Image\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch\r\n",
        "import random\r\n",
        "from skimage import transform as trans\r\n",
        "from skimage import io\r\n",
        "import sys\r\n",
        "sys.path.append('FaceLandmarkDetection')\r\n",
        "import face_alignment\r\n",
        "import dlib\r\n",
        "\r\n",
        "from models import *\r\n",
        "from data.custom_dataset import AlignedDataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import torchvision\r\n",
        "\r\n",
        "from tqdm import tqdm\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "from trains import Task\r\n",
        "from torchvision.utils import make_grid\r\n",
        "\r\n",
        "from options.test_options import TestOptions\r\n",
        "from data.image_folder import make_dataset\r\n",
        "\r\n",
        "from torchvision.utils import save_image\r\n",
        "\r\n",
        "\r\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \r\n",
        "\r\n",
        "# values don't matter, configure custom_dataset.py\r\n",
        "dataset = AlignedDataset('/content/checked/', fine_size=256)\r\n",
        "trainloader = DataLoader(dataset,\r\n",
        "                        batch_size=1,\r\n",
        "                        shuffle=False,\r\n",
        "                        num_workers=1)\r\n",
        "netG = networks.define_G('UNetDictFace', ['cuda:0'])\r\n",
        "\r\n",
        "# # tensorboard\r\n",
        "# writer = SummaryWriter()\r\n",
        "\r\n",
        "# # trains parameters dict\r\n",
        "# parameters_dict = {\r\n",
        "#     'test': 'test',\r\n",
        "#     }\r\n",
        "\r\n",
        "# # init trains\r\n",
        "# task = Task.init(project_name='face_enhancement', task_name='test')\r\n",
        "# logger = task.get_logger()\r\n",
        "\r\n",
        "# cfg_str = str(netG) + str('\\n\\n Discriminator:\\n\\n') + str(netD)\r\n",
        "# Task.current_task().set_model_config(cfg_str)\r\n",
        "\r\n",
        "# # connect the dictionary to TRAINS Task\r\n",
        "# parameters_dict = Task.current_task().connect(parameters_dict)\r\n",
        "\r\n",
        "#netG.load_state_dict(torch.load('weights/netG_30k_epoch4_exp2_3.pth'))\r\n",
        "netG.load_state_dict(torch.load('/content/DFDNet/weights/netG_epoch_0_i_10.pth'))\r\n",
        "\r\n",
        "for i, data in enumerate(tqdm(trainloader), 0):\r\n",
        "    data_a, data_c = data['A'], data['C']\r\n",
        "    data_a = data_a.to(device)\r\n",
        "    data_c = data_c.to(device)\r\n",
        "    data_part_locations = data['part_locations']\r\n",
        "\r\n",
        "    out = netG(data_a, part_locations=data_part_locations)\r\n",
        "    images = [data_a[0].cpu(), out[0].cpu()]\r\n",
        "    report_img = make_grid(images)\r\n",
        "#     report_img = report_img.mul_(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\r\n",
        "    result = 255 * (report_img.permute(1, 2, 0).cpu().detach().numpy() + 1) / 2\r\n",
        "    cv2.imwrite(f'/content/{i}.jpg', cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBkN_DGTjqRf",
        "cellView": "form"
      },
      "source": [
        "#@title custom_dataset.py (paths, disabling augmentations, disabling resize)\r\n",
        "%%writefile /content/DFDNet/data/custom_dataset.py\r\n",
        "# -- coding: utf-8 --\r\n",
        "import os.path\r\n",
        "import os\r\n",
        "import random\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torch\r\n",
        "from PIL import Image, ImageFilter\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import math\r\n",
        "from scipy.io import loadmat\r\n",
        "from PIL import Image\r\n",
        "import PIL\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "import glob\r\n",
        "\r\n",
        "class AlignedDataset(Dataset):\r\n",
        "    \r\n",
        "    def __init__(self, root_dir, fine_size=512, transform=None):\r\n",
        "        self.root_dir = '/content/input'\r\n",
        "        #self.pathes = [os.path.join(self.root_dir, x) for x in os.listdir(self.root_dir) if x[-3:] == 'jpg']\r\n",
        "        \r\n",
        "        self.pathes = glob.glob(self.root_dir + '/**/*.png', recursive=True)\r\n",
        "        #files = glob.glob(self.root_dir + '/**/*.png', recursive=True)\r\n",
        "        #files_jpg = glob.glob(self.root_dir + '/**/*.jpg', recursive=True)\r\n",
        "        #self.pathes = files.extend(files_jpg)\r\n",
        "\r\n",
        "\r\n",
        "        #print(\"self.pathes\")\r\n",
        "        #print(self.pathes)\r\n",
        "        self.transform = transform\r\n",
        "        self.fine_size = fine_size\r\n",
        "        self.partpath = '/content/landmark_output'\r\n",
        "        \r\n",
        "    def AddNoise(self,img): # noise\r\n",
        "        if random.random() > 0.9: #\r\n",
        "            return img\r\n",
        "        self.sigma = np.random.randint(1, 11)\r\n",
        "        img_tensor = torch.from_numpy(np.array(img)).float()\r\n",
        "        noise = torch.randn(img_tensor.size()).mul_(self.sigma/1.0)\r\n",
        "\r\n",
        "        noiseimg = torch.clamp(noise+img_tensor,0,255)\r\n",
        "        return Image.fromarray(np.uint8(noiseimg.numpy()))\r\n",
        "\r\n",
        "    def AddBlur(self,img): # gaussian blur or motion blur\r\n",
        "        if random.random() > 0.9: #\r\n",
        "            return img\r\n",
        "        img = np.array(img)\r\n",
        "        if random.random() > 0.35: ##gaussian blur\r\n",
        "            blursize = random.randint(1,17) * 2 + 1 ##3,5,7,9,11,13,15\r\n",
        "            blursigma = random.randint(3, 20)\r\n",
        "            img = cv2.GaussianBlur(img, (blursize,blursize), blursigma/10)\r\n",
        "        else: #motion blur\r\n",
        "            M = random.randint(1,32)\r\n",
        "            KName = './data/MotionBlurKernel/m_%02d.mat' % M\r\n",
        "            k = loadmat(KName)['kernel']\r\n",
        "            k = k.astype(np.float32)\r\n",
        "            k /= np.sum(k)\r\n",
        "            img = cv2.filter2D(img,-1,k)\r\n",
        "        return Image.fromarray(img)\r\n",
        "\r\n",
        "    def AddDownSample(self,img): # downsampling\r\n",
        "        if random.random() > 0.95: #\r\n",
        "            return img\r\n",
        "        sampler = random.randint(20, 100)*1.0\r\n",
        "        img = img.resize((int(self.fine_size/sampler*10.0), int(self.fine_size/sampler*10.0)), Image.BICUBIC)\r\n",
        "        return img\r\n",
        "\r\n",
        "    def AddJPEG(self,img): # JPEG compression\r\n",
        "        if random.random() > 0.6:\r\n",
        "            return img\r\n",
        "        imQ = random.randint(40, 80)\r\n",
        "        img = np.array(img)\r\n",
        "        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY),imQ] # (0,100),higher is better,default is 95\r\n",
        "        _, encA = cv2.imencode('.jpg', img, encode_param)\r\n",
        "        img = cv2.imdecode(encA,1)\r\n",
        "        return Image.fromarray(img)\r\n",
        "\r\n",
        "    def AddUpSample(self,img):\r\n",
        "        return img.resize((self.fine_size, self.fine_size), Image.BICUBIC)\r\n",
        "\r\n",
        "    def __getitem__(self, index): # indexation\r\n",
        "\r\n",
        "        path = self.pathes[index]\r\n",
        "        Imgs = Image.open(path).convert('RGB')\r\n",
        "        \r\n",
        "        #A = Imgs.resize((self.fine_size, self.fine_size))\r\n",
        "        A = Imgs\r\n",
        "        #A = transforms.ColorJitter(0.3, 0.3, 0.3, 0)(A)\r\n",
        "        C = A\r\n",
        "        #A = self.AddBlur(A)\r\n",
        "        \r\n",
        "        tmps = path.split('/')\r\n",
        "        ImgName = tmps[-1]\r\n",
        "        part_locations = self.get_part_location(self.partpath, ImgName, 2)\r\n",
        "        \r\n",
        "        A = transforms.ToTensor()(A)\r\n",
        "        C = transforms.ToTensor()(C)\r\n",
        "        \r\n",
        "        A = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(A) \r\n",
        "        C = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(C)\r\n",
        "        \r\n",
        "        return {'A': A, 'C': C, 'path': path, 'part_locations': part_locations}\r\n",
        "\r\n",
        "    def get_part_location(self, landmarkpath, imgname, downscale=1):\r\n",
        "        Landmarks = []\r\n",
        "        with open(os.path.join(landmarkpath, imgname + '.txt'),'r') as f:\r\n",
        "            for line in f:\r\n",
        "                tmp = [np.float(i) for i in line.split(' ') if i != '\\n']\r\n",
        "                Landmarks.append(tmp)\r\n",
        "        Landmarks = np.array(Landmarks)/downscale # 512 * 512\r\n",
        "        \r\n",
        "        Map_LE = list(np.hstack((range(17,22), range(36,42))))\r\n",
        "        Map_RE = list(np.hstack((range(22,27), range(42,48))))\r\n",
        "        Map_NO = list(range(29,36))\r\n",
        "        Map_MO = list(range(48,68))\r\n",
        "        #left eye\r\n",
        "        Mean_LE = np.mean(Landmarks[Map_LE],0)\r\n",
        "        L_LE = np.max((np.max(np.max(Landmarks[Map_LE],0) - np.min(Landmarks[Map_LE],0))/2,16))\r\n",
        "        Location_LE = np.hstack((Mean_LE - L_LE + 1, Mean_LE + L_LE)).astype(int)\r\n",
        "        #right eye\r\n",
        "        Mean_RE = np.mean(Landmarks[Map_RE],0)\r\n",
        "        L_RE = np.max((np.max(np.max(Landmarks[Map_RE],0) - np.min(Landmarks[Map_RE],0))/2,16))\r\n",
        "        Location_RE = np.hstack((Mean_RE - L_RE + 1, Mean_RE + L_RE)).astype(int)\r\n",
        "        #nose\r\n",
        "        Mean_NO = np.mean(Landmarks[Map_NO],0)\r\n",
        "        L_NO = np.max((np.max(np.max(Landmarks[Map_NO],0) - np.min(Landmarks[Map_NO],0))/2,16))\r\n",
        "        Location_NO = np.hstack((Mean_NO - L_NO + 1, Mean_NO + L_NO)).astype(int)\r\n",
        "        #mouth\r\n",
        "        Mean_MO = np.mean(Landmarks[Map_MO],0)\r\n",
        "        L_MO = np.max((np.max(np.max(Landmarks[Map_MO],0) - np.min(Landmarks[Map_MO],0))/2,16))\r\n",
        "\r\n",
        "        Location_MO = np.hstack((Mean_MO - L_MO + 1, Mean_MO + L_MO)).astype(int)\r\n",
        "        return Location_LE, Location_RE, Location_NO, Location_MO\r\n",
        "\r\n",
        "    def __len__(self): #\r\n",
        "        return len(self.pathes)\r\n",
        "\r\n",
        "    def name(self):\r\n",
        "        return 'AlignedDataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HelwF4Rc8hkp"
      },
      "source": [
        "%cd /content/DFDNet\r\n",
        "!python inference.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRPNaAsfFZIX"
      },
      "source": [
        "# Landmark generation\n",
        "\n",
        "Install that and restart runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrphVUigFbVT"
      },
      "source": [
        "!pip install face-alignment\r\n",
        "!pip install matplotlib --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lOy1Pf2IJ0T",
        "cellView": "form"
      },
      "source": [
        "%cd /content/\r\n",
        "import face_alignment\r\n",
        "from skimage import io\r\n",
        "import numpy as np\r\n",
        "import glob\r\n",
        "from tqdm import tqdm\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "\r\n",
        "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False)\r\n",
        "\r\n",
        "unchecked_input_path = '/content/input' #@param {type:\"string\"}\r\n",
        "checked_output_path = '/content/checked' #@param {type:\"string\"}\r\n",
        "failed_output_path = '/content/failed' #@param {type:\"string\"}\r\n",
        "landmark_output_path = '/content/landmark_output' #@param {type:\"string\"}\r\n",
        "\r\n",
        "if not os.path.exists(unchecked_input_path):\r\n",
        "    os.makedirs(unchecked_input_path)\r\n",
        "if not os.path.exists(checked_output_path):\r\n",
        "    os.makedirs(checked_output_path)\r\n",
        "if not os.path.exists(failed_output_path):\r\n",
        "    os.makedirs(failed_output_path)\r\n",
        "if not os.path.exists(landmark_output_path):\r\n",
        "    os.makedirs(landmark_output_path)\r\n",
        "\r\n",
        "files = glob.glob(unchecked_input_path + '/**/*.png', recursive=True)\r\n",
        "files_jpg = glob.glob(unchecked_input_path + '/**/*.jpg', recursive=True)\r\n",
        "files.extend(files_jpg)\r\n",
        "err_files=[]\r\n",
        "\r\n",
        "for f in tqdm(files):\r\n",
        "  input = io.imread(f)\r\n",
        "  preds = fa.get_landmarks(input)\r\n",
        "  #print(preds)\r\n",
        "  if preds is not None:\r\n",
        "    np.savetxt(os.path.join(landmark_output_path, os.path.basename(f)+\".txt\"), preds[0], delimiter=' ', fmt='%1.3f')   # X is an array\r\n",
        "    shutil.move(f, os.path.join(checked_output_path,os.path.basename(f)))\r\n",
        "  else:\r\n",
        "    shutil.move(f, os.path.join(failed_output_path,os.path.basename(f)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcxYC5-3TOxX"
      },
      "source": [
        "# [Experimental] Training with own features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "let-5evAt9Uy"
      },
      "source": [
        "The goal is to avoid using the already provided ```.npy``` files and creating the needed feature files manually instead. Feature saving was added into ```networks.py```, so you can use good looking images to extract features from. It will train as normal, but during training features will be concentrated into one array and then saved with torch as a file after a certain amount of iterations. Change ```self.amount_features``` if you want to configure the amount of features. The default is 20. Just wait until it says that all the features were saved (one print for each size (32,64,128,256))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93N4egT6TiF6",
        "cellView": "form"
      },
      "source": [
        "#@title networks.py (added feature generation)\r\n",
        "%%writefile /content/DFDNet/models/networks.py\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.nn import init\r\n",
        "import functools\r\n",
        "from torch.optim import lr_scheduler\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.nn import Parameter as P\r\n",
        "from util import util\r\n",
        "from torchvision import models\r\n",
        "import scipy.io as sio\r\n",
        "import numpy as np\r\n",
        "import scipy.ndimage\r\n",
        "import torch.nn.utils.spectral_norm as SpectralNorm\r\n",
        "\r\n",
        "from torch.autograd import Function\r\n",
        "from math import sqrt\r\n",
        "import random\r\n",
        "import os\r\n",
        "import math\r\n",
        "\r\n",
        "from sync_batchnorm import convert_model\r\n",
        "####\r\n",
        "\r\n",
        "###############################################################################\r\n",
        "# Helper Functions\r\n",
        "###############################################################################\r\n",
        "def get_norm_layer(norm_type='instance'):\r\n",
        "    if norm_type == 'batch':\r\n",
        "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\r\n",
        "    elif norm_type == 'instance':\r\n",
        "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\r\n",
        "    elif norm_type == 'none':\r\n",
        "        norm_layer = None\r\n",
        "    else:\r\n",
        "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\r\n",
        "\r\n",
        "    return norm_layer\r\n",
        "\r\n",
        "\r\n",
        "def get_scheduler(optimizer, opt):\r\n",
        "    if opt.lr_policy == 'lambda':\r\n",
        "        def lambda_rule(epoch):\r\n",
        "            lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\r\n",
        "            return lr_l\r\n",
        "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\r\n",
        "    elif opt.lr_policy == 'step':\r\n",
        "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\r\n",
        "    elif opt.lr_policy == 'plateau':\r\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\r\n",
        "    else:\r\n",
        "        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\r\n",
        "\r\n",
        "    return scheduler\r\n",
        "\r\n",
        "\r\n",
        "def init_weights(net, init_type='normal', gain=0.02):\r\n",
        "    def init_func(m):\r\n",
        "        classname = m.__class__.__name__\r\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\r\n",
        "            if init_type == 'normal':\r\n",
        "                init.normal_(m.weight.data, 0.0, gain)\r\n",
        "            elif init_type == 'xavier':\r\n",
        "                init.xavier_normal_(m.weight.data, gain=gain)\r\n",
        "            elif init_type == 'kaiming':\r\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\r\n",
        "            elif init_type == 'orthogonal':\r\n",
        "                init.orthogonal_(m.weight.data, gain=gain)\r\n",
        "            else:\r\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\r\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\r\n",
        "                init.constant_(m.bias.data, 0.0)\r\n",
        "        elif classname.find('BatchNorm2d') != -1:\r\n",
        "            init.normal_(m.weight.data, 1.0, gain)\r\n",
        "            init.constant_(m.bias.data, 0.0)\r\n",
        "\r\n",
        "    print('initialize network with %s' % init_type)\r\n",
        "    net.apply(init_func)\r\n",
        "\r\n",
        "\r\n",
        "def init_net(net, init_type='normal', init_gain=0.02, gpu_ids=[], init_flag=True):\r\n",
        "    if len(gpu_ids) > 0:\r\n",
        "        assert(torch.cuda.is_available())\r\n",
        "        net = convert_model(net)\r\n",
        "        net.to(gpu_ids[0])\r\n",
        "        net = torch.nn.DataParallel(net, gpu_ids)\r\n",
        "\r\n",
        "    if init_flag:\r\n",
        "\r\n",
        "        init_weights(net, init_type, gain=init_gain)\r\n",
        "\r\n",
        "    return net\r\n",
        "\r\n",
        "\r\n",
        "# compute adaptive instance norm\r\n",
        "def calc_mean_std(feat, eps=1e-5):\r\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\r\n",
        "    size = feat.size()\r\n",
        "    assert (len(size) == 3)\r\n",
        "    C, _ = size[:2]\r\n",
        "    feat_var = feat.contiguous().view(C, -1).var(dim=1) + eps\r\n",
        "    feat_std = feat_var.sqrt().view(C, 1, 1)\r\n",
        "    feat_mean = feat.contiguous().view(C, -1).mean(dim=1).view(C, 1, 1)\r\n",
        "\r\n",
        "    return feat_mean, feat_std\r\n",
        "\r\n",
        "\r\n",
        "def adaptive_instance_normalization(content_feat, style_feat):  # content_feat is degraded feature, style is ref feature\r\n",
        "    assert (content_feat.size()[:1] == style_feat.size()[:1])\r\n",
        "    size = content_feat.size()\r\n",
        "    style_mean, style_std = calc_mean_std(style_feat)\r\n",
        "    content_mean, content_std = calc_mean_std(content_feat)\r\n",
        "\r\n",
        "    normalized_feat = (content_feat - content_mean.expand(\r\n",
        "        size)) / content_std.expand(size)\r\n",
        "\r\n",
        "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\r\n",
        "\r\n",
        "def calc_mean_std_4D(feat, eps=1e-5):\r\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\r\n",
        "    size = feat.size()\r\n",
        "    assert (len(size) == 4)\r\n",
        "    N, C = size[:2]\r\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\r\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\r\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\r\n",
        "    return feat_mean, feat_std\r\n",
        "\r\n",
        "def adaptive_instance_normalization_4D(content_feat, style_feat): # content_feat is ref feature, style is degradate feature\r\n",
        "    # assert (content_feat.size()[:2] == style_feat.size()[:2])\r\n",
        "    size = content_feat.size()\r\n",
        "    style_mean, style_std = calc_mean_std_4D(style_feat)\r\n",
        "    content_mean, content_std = calc_mean_std_4D(content_feat)\r\n",
        "    normalized_feat = (content_feat - content_mean.expand(\r\n",
        "        size)) / content_std.expand(size)\r\n",
        "    return normalized_feat * style_std + style_mean\r\n",
        "\r\n",
        "def define_G(which_model_netG, gpu_ids=[]):\r\n",
        "    if which_model_netG == 'UNetDictFace':\r\n",
        "        netG = UNetDictFace(64)\r\n",
        "        init_flag = False\r\n",
        "    else:\r\n",
        "        raise NotImplementedError('Generator model name [%s] is not recognized' % which_model_netG)\r\n",
        "    return init_net(netG, 'normal', 0.02, gpu_ids, init_flag)\r\n",
        "\r\n",
        "\r\n",
        "##############################################################################\r\n",
        "# Classes\r\n",
        "############################################################################################################################################\r\n",
        "\r\n",
        "\r\n",
        "def convU(in_channels, out_channels,conv_layer, norm_layer, kernel_size=3, stride=1,dilation=1, bias=True):\r\n",
        "    return nn.Sequential(\r\n",
        "        SpectralNorm(conv_layer(in_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias)),\r\n",
        "#         conv_layer(in_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias),\r\n",
        "#         nn.BatchNorm2d(out_channels),\r\n",
        "        nn.LeakyReLU(0.2),\r\n",
        "        SpectralNorm(conv_layer(out_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias)),\r\n",
        "    )\r\n",
        "class MSDilateBlock(nn.Module):\r\n",
        "    def __init__(self, in_channels,conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, kernel_size=3, dilation=[1,1,1,1], bias=True):\r\n",
        "        super(MSDilateBlock, self).__init__()\r\n",
        "        self.conv1 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[0], bias=bias)\r\n",
        "        self.conv2 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[1], bias=bias)\r\n",
        "        self.conv3 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[2], bias=bias)\r\n",
        "        self.conv4 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[3], bias=bias)\r\n",
        "        self.convi =  SpectralNorm(conv_layer(in_channels*4, in_channels, kernel_size=kernel_size, stride=1, padding=(kernel_size-1)//2, bias=bias))\r\n",
        "    def forward(self, x):\r\n",
        "        conv1 = self.conv1(x)\r\n",
        "        conv2 = self.conv2(x)\r\n",
        "        conv3 = self.conv3(x)\r\n",
        "        conv4 = self.conv4(x)\r\n",
        "        cat  = torch.cat([conv1, conv2, conv3, conv4], 1)\r\n",
        "        out = self.convi(cat) + x\r\n",
        "        return out\r\n",
        "\r\n",
        "##############################UNetFace#########################\r\n",
        "class AdaptiveInstanceNorm(nn.Module):\r\n",
        "    def __init__(self, in_channel):\r\n",
        "        super().__init__()\r\n",
        "        self.norm = nn.InstanceNorm2d(in_channel)\r\n",
        "\r\n",
        "    def forward(self, input, style):\r\n",
        "        style_mean, style_std = calc_mean_std_4D(style)\r\n",
        "        out = self.norm(input)\r\n",
        "        size = input.size()\r\n",
        "        out = style_std.expand(size) * out + style_mean.expand(size)\r\n",
        "        return out\r\n",
        "\r\n",
        "class BlurFunctionBackward(Function):\r\n",
        "    @staticmethod\r\n",
        "    def forward(ctx, grad_output, kernel, kernel_flip):\r\n",
        "        ctx.save_for_backward(kernel, kernel_flip)\r\n",
        "\r\n",
        "        grad_input = F.conv2d(\r\n",
        "            grad_output, kernel_flip, padding=1, groups=grad_output.shape[1]\r\n",
        "        )\r\n",
        "        return grad_input\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def backward(ctx, gradgrad_output):\r\n",
        "        kernel, kernel_flip = ctx.saved_tensors\r\n",
        "\r\n",
        "        grad_input = F.conv2d(\r\n",
        "            gradgrad_output, kernel, padding=1, groups=gradgrad_output.shape[1]\r\n",
        "        )\r\n",
        "        return grad_input, None, None\r\n",
        "\r\n",
        "\r\n",
        "class BlurFunction(Function):\r\n",
        "    @staticmethod\r\n",
        "    def forward(ctx, input, kernel, kernel_flip):\r\n",
        "        ctx.save_for_backward(kernel, kernel_flip)\r\n",
        "\r\n",
        "        output = F.conv2d(input, kernel, padding=1, groups=input.shape[1])\r\n",
        "\r\n",
        "        return output\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def backward(ctx, grad_output):\r\n",
        "        kernel, kernel_flip = ctx.saved_tensors\r\n",
        "\r\n",
        "        grad_input = BlurFunctionBackward.apply(grad_output, kernel, kernel_flip)\r\n",
        "\r\n",
        "        return grad_input, None, None\r\n",
        "\r\n",
        "blur = BlurFunction.apply\r\n",
        "\r\n",
        "\r\n",
        "class Blur(nn.Module):\r\n",
        "    def __init__(self, channel):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        weight = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=torch.float32)\r\n",
        "        weight = weight.view(1, 1, 3, 3)\r\n",
        "        weight = weight / weight.sum()\r\n",
        "        weight_flip = torch.flip(weight, [2, 3])\r\n",
        "\r\n",
        "        self.register_buffer('weight', weight.repeat(channel, 1, 1, 1))\r\n",
        "        self.register_buffer('weight_flip', weight_flip.repeat(channel, 1, 1, 1))\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        return blur(input, self.weight, self.weight_flip)\r\n",
        "\r\n",
        "class EqualLR:\r\n",
        "    def __init__(self, name):\r\n",
        "        self.name = name\r\n",
        "\r\n",
        "    def compute_weight(self, module):\r\n",
        "        weight = getattr(module, self.name + '_orig')\r\n",
        "        fan_in = weight.data.size(1) * weight.data[0][0].numel()\r\n",
        "        return weight * sqrt(2 / fan_in)\r\n",
        "    @staticmethod\r\n",
        "    def apply(module, name):\r\n",
        "        fn = EqualLR(name)\r\n",
        "\r\n",
        "        weight = getattr(module, name)\r\n",
        "        del module._parameters[name]\r\n",
        "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\r\n",
        "        module.register_forward_pre_hook(fn)\r\n",
        "\r\n",
        "        return fn\r\n",
        "\r\n",
        "    def __call__(self, module, input):\r\n",
        "        weight = self.compute_weight(module)\r\n",
        "        setattr(module, self.name, weight)\r\n",
        "\r\n",
        "def equal_lr(module, name='weight'):\r\n",
        "    EqualLR.apply(module, name)\r\n",
        "    return module\r\n",
        "\r\n",
        "class EqualConv2d(nn.Module):\r\n",
        "    def __init__(self, *args, **kwargs):\r\n",
        "        super().__init__()\r\n",
        "        conv = nn.Conv2d(*args, **kwargs)\r\n",
        "        conv.weight.data.normal_()\r\n",
        "        conv.bias.data.zero_()\r\n",
        "        self.conv = equal_lr(conv)\r\n",
        "    def forward(self, input):\r\n",
        "        return self.conv(input)\r\n",
        "\r\n",
        "class NoiseInjection(nn.Module):\r\n",
        "    def __init__(self, channel):\r\n",
        "        super().__init__()\r\n",
        "        self.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))\r\n",
        "    def forward(self, image, noise):\r\n",
        "        return image + self.weight * noise\r\n",
        "\r\n",
        "class StyledUpBlock(nn.Module):\r\n",
        "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1,upsample=False):\r\n",
        "        super().__init__()\r\n",
        "        if upsample:\r\n",
        "            self.conv1 = nn.Sequential(\r\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\r\n",
        "                Blur(out_channel),\r\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding),\r\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\r\n",
        "                nn.LeakyReLU(0.2),\r\n",
        "            )\r\n",
        "        else:\r\n",
        "            self.conv1 = nn.Sequential(\r\n",
        "                Blur(in_channel),\r\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding)\r\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\r\n",
        "                nn.LeakyReLU(0.2),\r\n",
        "            )\r\n",
        "        self.convup = nn.Sequential(\r\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\r\n",
        "                # EqualConv2d(out_channel, out_channel, kernel_size, padding=padding),\r\n",
        "                SpectralNorm(nn.Conv2d(out_channel, out_channel, kernel_size, padding=padding)),\r\n",
        "                nn.LeakyReLU(0.2),\r\n",
        "                # Blur(out_channel),\r\n",
        "            )\r\n",
        "        # self.noise1 = equal_lr(NoiseInjection(out_channel))\r\n",
        "        # self.adain1 = AdaptiveInstanceNorm(out_channel)\r\n",
        "        self.lrelu1 = nn.LeakyReLU(0.2)\r\n",
        "\r\n",
        "        # self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\r\n",
        "        # self.noise2 = equal_lr(NoiseInjection(out_channel))\r\n",
        "        # self.adain2 = AdaptiveInstanceNorm(out_channel)\r\n",
        "        # self.lrelu2 = nn.LeakyReLU(0.2)\r\n",
        "\r\n",
        "        self.ScaleModel1 = nn.Sequential(\r\n",
        "            # Blur(in_channel),\r\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\r\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\r\n",
        "            nn.LeakyReLU(0.2, True),\r\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1))\r\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\r\n",
        "        )\r\n",
        "        self.ShiftModel1 = nn.Sequential(\r\n",
        "            # Blur(in_channel),\r\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\r\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\r\n",
        "            nn.LeakyReLU(0.2, True),\r\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1)),\r\n",
        "            nn.Sigmoid(),\r\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\r\n",
        "        )\r\n",
        "       \r\n",
        "    def forward(self, input, style):\r\n",
        "        out = self.conv1(input)\r\n",
        "#         out = self.noise1(out, noise)\r\n",
        "        out = self.lrelu1(out)\r\n",
        "\r\n",
        "        Shift1 = self.ShiftModel1(style)\r\n",
        "        Scale1 = self.ScaleModel1(style)\r\n",
        "        out = out * Scale1 + Shift1\r\n",
        "        # out = self.adain1(out, style)\r\n",
        "        outup = self.convup(out)\r\n",
        "\r\n",
        "        return outup\r\n",
        "\r\n",
        "##############################################################################\r\n",
        "##Face Dictionary\r\n",
        "##############################################################################\r\n",
        "class VGGFeat(torch.nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Input: (B, C, H, W), RGB, [-1, 1]\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, weight_path='./weights/vgg19.pth'):\r\n",
        "        super().__init__()\r\n",
        "        self.model = models.vgg19(pretrained=False)\r\n",
        "        self.build_vgg_layers()\r\n",
        "        \r\n",
        "        self.model.load_state_dict(torch.load(weight_path))\r\n",
        "\r\n",
        "        self.register_parameter(\"RGB_mean\", nn.Parameter(torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)))\r\n",
        "        self.register_parameter(\"RGB_std\", nn.Parameter(torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)))\r\n",
        "        \r\n",
        "        # self.model.eval()\r\n",
        "        for param in self.model.parameters():\r\n",
        "            param.requires_grad = False\r\n",
        "    \r\n",
        "    def build_vgg_layers(self):\r\n",
        "        vgg_pretrained_features = self.model.features\r\n",
        "        self.features = []\r\n",
        "        # feature_layers = [0, 3, 8, 17, 26, 35]\r\n",
        "        feature_layers = [0, 8, 17, 26, 35]\r\n",
        "        for i in range(len(feature_layers)-1): \r\n",
        "            module_layers = torch.nn.Sequential() \r\n",
        "            for j in range(feature_layers[i], feature_layers[i+1]):\r\n",
        "                module_layers.add_module(str(j), vgg_pretrained_features[j])\r\n",
        "            self.features.append(module_layers)\r\n",
        "        self.features = torch.nn.ModuleList(self.features)\r\n",
        "\r\n",
        "    def preprocess(self, x):\r\n",
        "        x = (x + 1) / 2\r\n",
        "        x = (x - self.RGB_mean) / self.RGB_std\r\n",
        "        if x.shape[3] < 224:\r\n",
        "            x = torch.nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.preprocess(x)\r\n",
        "        features = []\r\n",
        "        for m in self.features:\r\n",
        "            # print(m)\r\n",
        "            x = m(x)\r\n",
        "            features.append(x)\r\n",
        "        return features \r\n",
        "\r\n",
        "def compute_sum(x, axis=None, keepdim=False):\r\n",
        "    if not axis:\r\n",
        "        axis = range(len(x.shape))\r\n",
        "    for i in sorted(axis, reverse=True):\r\n",
        "        x = torch.sum(x, dim=i, keepdim=keepdim)\r\n",
        "    return x\r\n",
        "def ToRGB(in_channel):\r\n",
        "    return nn.Sequential(\r\n",
        "        SpectralNorm(nn.Conv2d(in_channel,in_channel,3, 1, 1)),\r\n",
        "        nn.LeakyReLU(0.2),\r\n",
        "        SpectralNorm(nn.Conv2d(in_channel,3,3, 1, 1))\r\n",
        "    )\r\n",
        "\r\n",
        "def AttentionBlock(in_channel):\r\n",
        "    return nn.Sequential(\r\n",
        "        SpectralNorm(nn.Conv2d(in_channel, in_channel, 3, 1, 1)),\r\n",
        "        nn.LeakyReLU(0.2),\r\n",
        "        SpectralNorm(nn.Conv2d(in_channel, in_channel, 3, 1, 1))\r\n",
        "    )\r\n",
        "\r\n",
        "class UNetDictFace(nn.Module):\r\n",
        "    def __init__(self, ngf=64, dictionary_path='./DictionaryCenter512'):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.part_sizes = np.array([80,80,50,110]) # size for 512\r\n",
        "        self.feature_sizes = np.array([256,128,64,32])\r\n",
        "        self.channel_sizes = np.array([128,256,512,512])\r\n",
        "        Parts = ['left_eye','right_eye','nose','mouth']\r\n",
        "        self.Dict_256 = {}\r\n",
        "        self.Dict_128 = {}\r\n",
        "        self.Dict_64 = {}\r\n",
        "        self.Dict_32 = {}\r\n",
        "        \r\n",
        "        for j,i in enumerate(Parts):\r\n",
        "            f_256 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_256_center.npy'.format(i)), allow_pickle=True))\r\n",
        "\r\n",
        "            f_256_reshape = f_256.reshape(f_256.size(0),self.channel_sizes[0],self.part_sizes[j]//2,self.part_sizes[j]//2)\r\n",
        "            max_256 = torch.max(torch.sqrt(compute_sum(torch.pow(f_256_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\r\n",
        "            self.Dict_256[i] = f_256_reshape #/ max_256\r\n",
        "\r\n",
        "            f_128 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_128_center.npy'.format(i)), allow_pickle=True))\r\n",
        "\r\n",
        "            f_128_reshape = f_128.reshape(f_128.size(0),self.channel_sizes[1],self.part_sizes[j]//4,self.part_sizes[j]//4)\r\n",
        "            max_128 = torch.max(torch.sqrt(compute_sum(torch.pow(f_128_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\r\n",
        "            self.Dict_128[i] = f_128_reshape #/ max_128\r\n",
        "\r\n",
        "            f_64 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_64_center.npy'.format(i)), allow_pickle=True))\r\n",
        "\r\n",
        "            f_64_reshape = f_64.reshape(f_64.size(0),self.channel_sizes[2],self.part_sizes[j]//8,self.part_sizes[j]//8)\r\n",
        "            max_64 = torch.max(torch.sqrt(compute_sum(torch.pow(f_64_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\r\n",
        "            self.Dict_64[i] = f_64_reshape #/ max_64\r\n",
        "\r\n",
        "            f_32 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_32_center.npy'.format(i)), allow_pickle=True))\r\n",
        "\r\n",
        "            f_32_reshape = f_32.reshape(f_32.size(0),self.channel_sizes[3],self.part_sizes[j]//16,self.part_sizes[j]//16)\r\n",
        "            max_32 = torch.max(torch.sqrt(compute_sum(torch.pow(f_32_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\r\n",
        "            self.Dict_32[i] = f_32_reshape #/ max_32\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        print(\"self.Dict_256\")\r\n",
        "        print(len(self.Dict_256))\r\n",
        "        print(\"self.Dict_256['left_eye'].shape\")\r\n",
        "        print(self.Dict_256['left_eye'].shape)\r\n",
        "        print(\"self.Dict_256['right_eye'].shape\")\r\n",
        "        print(self.Dict_256['right_eye'].shape)\r\n",
        "        print(\"self.Dict_256['nose'].shape\")\r\n",
        "        print(self.Dict_256['nose'].shape)\r\n",
        "        print(\"self.Dict_256['mouth'].shape\")\r\n",
        "        print(self.Dict_256['mouth'].shape)\r\n",
        "        print(\"-------------------\")\r\n",
        "        print(\"self.Dict_128\")\r\n",
        "        print(len(self.Dict_128))\r\n",
        "        print(\"self.Dict_128['left_eye'].shape\")\r\n",
        "        print(self.Dict_128['left_eye'].shape)\r\n",
        "        print(\"self.Dict_128['right_eye'].shape\")\r\n",
        "        print(self.Dict_128['right_eye'].shape)\r\n",
        "        print(\"self.Dict_128['nose'].shape\")\r\n",
        "        print(self.Dict_128['nose'].shape)\r\n",
        "        print(\"self.Dict_128['mouth'].shape\")\r\n",
        "        print(self.Dict_128['mouth'].shape)\r\n",
        "        print(\"-------------------\")\r\n",
        "        print(\"self.Dict_64\")\r\n",
        "        print(len(self.Dict_64))\r\n",
        "        print(\"self.Dict_64['left_eye'].shape\")\r\n",
        "        print(self.Dict_64['left_eye'].shape)\r\n",
        "        print(\"self.Dict_64['right_eye'].shape\")\r\n",
        "        print(self.Dict_64['right_eye'].shape)\r\n",
        "        print(\"self.Dict_64['nose'].shape\")\r\n",
        "        print(self.Dict_64['nose'].shape)\r\n",
        "        print(\"self.Dict_64['mouth'].shape\")\r\n",
        "        print(self.Dict_64['mouth'].shape)\r\n",
        "        print(\"-------------------\")\r\n",
        "        print(\"self.Dict_32\")\r\n",
        "        print(len(self.Dict_32))\r\n",
        "        print(\"self.Dict_32['left_eye'].shape\")\r\n",
        "        print(self.Dict_32['left_eye'].shape)\r\n",
        "        print(\"self.Dict_32['right_eye'].shape\")\r\n",
        "        print(self.Dict_32['right_eye'].shape)\r\n",
        "        print(\"self.Dict_32['nose'].shape\")\r\n",
        "        print(self.Dict_32['nose'].shape)\r\n",
        "        print(\"self.Dict_32['mouth'].shape\")\r\n",
        "        print(self.Dict_32['mouth'].shape)\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        self.le_256 = AttentionBlock(128)\r\n",
        "        self.le_128 = AttentionBlock(256)\r\n",
        "        self.le_64 = AttentionBlock(512)\r\n",
        "        self.le_32 = AttentionBlock(512)\r\n",
        "\r\n",
        "        self.re_256 = AttentionBlock(128)\r\n",
        "        self.re_128 = AttentionBlock(256)\r\n",
        "        self.re_64 = AttentionBlock(512)\r\n",
        "        self.re_32 = AttentionBlock(512)\r\n",
        "\r\n",
        "        self.no_256 = AttentionBlock(128)\r\n",
        "        self.no_128 = AttentionBlock(256)\r\n",
        "        self.no_64 = AttentionBlock(512)\r\n",
        "        self.no_32 = AttentionBlock(512)\r\n",
        "\r\n",
        "        self.mo_256 = AttentionBlock(128)\r\n",
        "        self.mo_128 = AttentionBlock(256)\r\n",
        "        self.mo_64 = AttentionBlock(512)\r\n",
        "        self.mo_32 = AttentionBlock(512)\r\n",
        "\r\n",
        "        #norm\r\n",
        "        self.VggExtract = VGGFeat()\r\n",
        "        \r\n",
        "        ######################\r\n",
        "        self.MSDilate = MSDilateBlock(ngf*8, dilation = [4,3,2,1])  #\r\n",
        "\r\n",
        "        self.up0 = StyledUpBlock(ngf*8,ngf*8)\r\n",
        "        self.up1 = StyledUpBlock(ngf*8, ngf*4) #\r\n",
        "        self.up2 = StyledUpBlock(ngf*4, ngf*2) #\r\n",
        "        self.up3 = StyledUpBlock(ngf*2, ngf) #\r\n",
        "        self.up4 = nn.Sequential( # 128\r\n",
        "            # nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\r\n",
        "            SpectralNorm(nn.Conv2d(ngf, ngf, 3, 1, 1)),\r\n",
        "            # nn.BatchNorm2d(32),\r\n",
        "            nn.LeakyReLU(0.2),\r\n",
        "            UpResBlock(ngf),\r\n",
        "            UpResBlock(ngf),\r\n",
        "            # SpectralNorm(nn.Conv2d(ngf, 3, kernel_size=3, stride=1, padding=1)),\r\n",
        "            nn.Conv2d(ngf, 3, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.Tanh()\r\n",
        "        )\r\n",
        "        self.to_rgb0 = ToRGB(ngf*8)\r\n",
        "        self.to_rgb1 = ToRGB(ngf*4)\r\n",
        "        self.to_rgb2 = ToRGB(ngf*2)\r\n",
        "        self.to_rgb3 = ToRGB(ngf*1)\r\n",
        "\r\n",
        "        # for param in self.BlurInputConv.parameters():\r\n",
        "        #     param.requires_grad = False\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "        self.count_256 = 0\r\n",
        "        self.count_128 = 0\r\n",
        "        self.count_64 = 0\r\n",
        "        self.count_32 = 0\r\n",
        "        self.amount_features = 20\r\n",
        "\r\n",
        "    def forward(self, input, part_locations):\r\n",
        "        #print(\"input.shape\")\r\n",
        "        #print(input.shape)\r\n",
        "        VggFeatures = self.VggExtract(input) #VggFeatures = list object\r\n",
        "        # for b in range(input.size(0)):\r\n",
        "        b = 0\r\n",
        "        UpdateVggFeatures = []\r\n",
        "        for i, f_size in enumerate(self.feature_sizes):\r\n",
        "            cur_feature = VggFeatures[i]\r\n",
        "            #print(\"cur_feature.shape\")\r\n",
        "            #print(cur_feature.shape)\r\n",
        "\r\n",
        "            update_feature = cur_feature.clone() #* 0\r\n",
        "            cur_part_sizes = self.part_sizes // (512/f_size)\r\n",
        "            dicts_feature = getattr(self, 'Dict_'+str(f_size))\r\n",
        "            \r\n",
        "            LE_Dict_feature = dicts_feature['left_eye'].to(input)\r\n",
        "            RE_Dict_feature = dicts_feature['right_eye'].to(input)\r\n",
        "            NO_Dict_feature = dicts_feature['nose'].to(input)\r\n",
        "            MO_Dict_feature = dicts_feature['mouth'].to(input)\r\n",
        "\r\n",
        "            le_location = (part_locations[0][b] // (512/f_size)).int()\r\n",
        "            re_location = (part_locations[1][b] // (512/f_size)).int()\r\n",
        "            no_location = (part_locations[2][b] // (512/f_size)).int()\r\n",
        "            mo_location = (part_locations[3][b] // (512/f_size)).int()\r\n",
        "\r\n",
        "            LE_feature = cur_feature[:,:,le_location[1]:le_location[3],le_location[0]:le_location[2]].clone()\r\n",
        "            RE_feature = cur_feature[:,:,re_location[1]:re_location[3],re_location[0]:re_location[2]].clone()\r\n",
        "            NO_feature = cur_feature[:,:,no_location[1]:no_location[3],no_location[0]:no_location[2]].clone()\r\n",
        "            MO_feature = cur_feature[:,:,mo_location[1]:mo_location[3],mo_location[0]:mo_location[2]].clone()\r\n",
        "            \r\n",
        "            #resize\r\n",
        "            LE_feature_resize = F.interpolate(LE_feature,(LE_Dict_feature.size(2),LE_Dict_feature.size(3)),mode='bilinear',align_corners=False)\r\n",
        "            RE_feature_resize = F.interpolate(RE_feature,(RE_Dict_feature.size(2),RE_Dict_feature.size(3)),mode='bilinear',align_corners=False)\r\n",
        "            NO_feature_resize = F.interpolate(NO_feature,(NO_Dict_feature.size(2),NO_Dict_feature.size(3)),mode='bilinear',align_corners=False)\r\n",
        "            MO_feature_resize = F.interpolate(MO_feature,(MO_Dict_feature.size(2),MO_Dict_feature.size(3)),mode='bilinear',align_corners=False)\r\n",
        "            \r\n",
        "            #print(\"LE_feature_resize.shape\")\r\n",
        "            #print(LE_feature_resize.shape)\r\n",
        "            \r\n",
        "            #print(\"f_size\")\r\n",
        "            #print(f_size)\r\n",
        "\r\n",
        "            if f_size == 256:\r\n",
        "              if self.count_256 == 0:\r\n",
        "                #print(\"LE_save_256 = cur_feature\")\r\n",
        "                self.LE_save_256 = LE_feature_resize\r\n",
        "                self.RE_save_256 = RE_feature_resize\r\n",
        "                self.NO_save_256 = NO_feature_resize\r\n",
        "                self.MO_save_256 = MO_feature_resize\r\n",
        "                self.count_256 += 1\r\n",
        "              else:\r\n",
        "                #print(\"torch.cat((LE_save_256, LE_feature_resize), 1)\")\r\n",
        "                self.LE_save_256 = torch.cat((self.LE_save_256, LE_feature_resize), 0)\r\n",
        "                self.RE_save_256 = torch.cat((self.RE_save_256, RE_feature_resize), 0)\r\n",
        "                self.NO_save_256 = torch.cat((self.NO_save_256, NO_feature_resize), 0)\r\n",
        "                self.MO_save_256 = torch.cat((self.MO_save_256, MO_feature_resize), 0)\r\n",
        "                self.count_256 += 1\r\n",
        "\r\n",
        "              if self.count_256 == self.amount_features:\r\n",
        "                torch.save(self.LE_save_256, 'LE_feature_resize_256.pt')\r\n",
        "                torch.save(self.RE_save_256, 'RE_feature_resize_256.pt')\r\n",
        "                torch.save(self.NO_save_256, 'NO_feature_resize_256.pt')\r\n",
        "                torch.save(self.MO_save_256, 'MO_feature_resize_256.pt')\r\n",
        "                print(\"generated features for size 256\")\r\n",
        "              \r\n",
        "            #############################################\r\n",
        "            if f_size == 128:\r\n",
        "              if self.count_128 == 0:\r\n",
        "                self.LE_save_128 = LE_feature_resize\r\n",
        "                self.RE_save_128 = RE_feature_resize\r\n",
        "                self.NO_save_128 = NO_feature_resize\r\n",
        "                self.MO_save_128 = MO_feature_resize\r\n",
        "                self.count_128 += 1\r\n",
        "              else:\r\n",
        "                self.LE_save_128 = torch.cat((self.LE_save_128, LE_feature_resize), 0)\r\n",
        "                self.RE_save_128 = torch.cat((self.RE_save_128, RE_feature_resize), 0)\r\n",
        "                self.NO_save_128 = torch.cat((self.NO_save_128, NO_feature_resize), 0)\r\n",
        "                self.MO_save_128 = torch.cat((self.MO_save_128, MO_feature_resize), 0)\r\n",
        "                self.count_128 += 1\r\n",
        "\r\n",
        "              if self.count_128 == self.amount_features:\r\n",
        "                torch.save(self.LE_save_128, 'LE_feature_resize_128.pt')\r\n",
        "                torch.save(self.RE_save_128, 'RE_feature_resize_128.pt')\r\n",
        "                torch.save(self.NO_save_128, 'NO_feature_resize_128.pt')\r\n",
        "                torch.save(self.MO_save_128, 'MO_feature_resize_128.pt')\r\n",
        "                print(\"generated features for size 128\")\r\n",
        "            #############################################\r\n",
        "            if f_size == 64:\r\n",
        "              if self.count_64 == 0:\r\n",
        "                self.LE_save_64 = LE_feature_resize\r\n",
        "                self.RE_save_64 = RE_feature_resize\r\n",
        "                self.NO_save_64 = NO_feature_resize\r\n",
        "                self.MO_save_64 = MO_feature_resize\r\n",
        "                self.count_64 += 1\r\n",
        "              else:\r\n",
        "                self.LE_save_64 = torch.cat((self.LE_save_64, LE_feature_resize), 0)\r\n",
        "                self.RE_save_64 = torch.cat((self.RE_save_64, RE_feature_resize), 0)\r\n",
        "                self.NO_save_64 = torch.cat((self.NO_save_64, NO_feature_resize), 0)\r\n",
        "                self.MO_save_64 = torch.cat((self.MO_save_64, MO_feature_resize), 0)\r\n",
        "                self.count_64 += 1\r\n",
        "\r\n",
        "              if self.count_64 == self.amount_features:\r\n",
        "                torch.save(self.LE_save_64, 'LE_feature_resize_64.pt')\r\n",
        "                torch.save(self.RE_save_64, 'RE_feature_resize_64.pt')\r\n",
        "                torch.save(self.NO_save_64, 'NO_feature_resize_64.pt')\r\n",
        "                torch.save(self.MO_save_64, 'MO_feature_resize_64.pt')\r\n",
        "                print(\"generated features for size 64\")\r\n",
        "\r\n",
        "            #############################################\r\n",
        "            if f_size == 32:\r\n",
        "              if self.count_32 == 0:\r\n",
        "                self.LE_save_32 = LE_feature_resize\r\n",
        "                self.RE_save_32 = RE_feature_resize\r\n",
        "                self.NO_save_32 = NO_feature_resize\r\n",
        "                self.MO_save_32 = MO_feature_resize\r\n",
        "                self.count_32 += 1\r\n",
        "              else:\r\n",
        "                self.LE_save_32 = torch.cat((self.LE_save_32, LE_feature_resize), 0)\r\n",
        "                self.RE_save_32 = torch.cat((self.RE_save_32, RE_feature_resize), 0)\r\n",
        "                self.NO_save_32 = torch.cat((self.NO_save_32, NO_feature_resize), 0)\r\n",
        "                self.MO_save_32 = torch.cat((self.MO_save_32, MO_feature_resize), 0)\r\n",
        "                self.count_32 += 1\r\n",
        "\r\n",
        "              if self.count_32 == self.amount_features:\r\n",
        "                torch.save(self.LE_save_32, 'LE_feature_resize_32.pt')\r\n",
        "                torch.save(self.RE_save_32, 'RE_feature_resize_32.pt')\r\n",
        "                torch.save(self.NO_save_32, 'NO_feature_resize_32.pt')\r\n",
        "                torch.save(self.MO_save_32, 'MO_feature_resize_32.pt')\r\n",
        "                print(\"generated features for size 32\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "            LE_Dict_feature_norm = adaptive_instance_normalization_4D(LE_Dict_feature, LE_feature_resize)\r\n",
        "            RE_Dict_feature_norm = adaptive_instance_normalization_4D(RE_Dict_feature, RE_feature_resize)\r\n",
        "            NO_Dict_feature_norm = adaptive_instance_normalization_4D(NO_Dict_feature, NO_feature_resize)\r\n",
        "            MO_Dict_feature_norm = adaptive_instance_normalization_4D(MO_Dict_feature, MO_feature_resize)\r\n",
        "            \r\n",
        "            LE_score = F.conv2d(LE_feature_resize, LE_Dict_feature_norm)\r\n",
        "\r\n",
        "            LE_score = F.softmax(LE_score.view(-1),dim=0)\r\n",
        "            LE_index = torch.argmax(LE_score)\r\n",
        "            LE_Swap_feature = F.interpolate(LE_Dict_feature_norm[LE_index:LE_index+1], (LE_feature.size(2), LE_feature.size(3)))\r\n",
        "\r\n",
        "            LE_Attention = getattr(self, 'le_'+str(f_size))(LE_Swap_feature-LE_feature)\r\n",
        "            LE_Att_feature = LE_Attention * LE_Swap_feature\r\n",
        "            \r\n",
        "\r\n",
        "            RE_score = F.conv2d(RE_feature_resize, RE_Dict_feature_norm)\r\n",
        "            RE_score = F.softmax(RE_score.view(-1),dim=0)\r\n",
        "            RE_index = torch.argmax(RE_score)\r\n",
        "            RE_Swap_feature = F.interpolate(RE_Dict_feature_norm[RE_index:RE_index+1], (RE_feature.size(2), RE_feature.size(3)))\r\n",
        "            \r\n",
        "            RE_Attention = getattr(self, 're_'+str(f_size))(RE_Swap_feature-RE_feature)\r\n",
        "            RE_Att_feature = RE_Attention * RE_Swap_feature\r\n",
        "\r\n",
        "            NO_score = F.conv2d(NO_feature_resize, NO_Dict_feature_norm)\r\n",
        "            NO_score = F.softmax(NO_score.view(-1),dim=0)\r\n",
        "            NO_index = torch.argmax(NO_score)\r\n",
        "            NO_Swap_feature = F.interpolate(NO_Dict_feature_norm[NO_index:NO_index+1], (NO_feature.size(2), NO_feature.size(3)))\r\n",
        "            \r\n",
        "            NO_Attention = getattr(self, 'no_'+str(f_size))(NO_Swap_feature-NO_feature)\r\n",
        "            NO_Att_feature = NO_Attention * NO_Swap_feature\r\n",
        "\r\n",
        "            \r\n",
        "            MO_score = F.conv2d(MO_feature_resize, MO_Dict_feature_norm)\r\n",
        "            MO_score = F.softmax(MO_score.view(-1),dim=0)\r\n",
        "            MO_index = torch.argmax(MO_score)\r\n",
        "            MO_Swap_feature = F.interpolate(MO_Dict_feature_norm[MO_index:MO_index+1], (MO_feature.size(2), MO_feature.size(3)))\r\n",
        "            \r\n",
        "            MO_Attention = getattr(self, 'mo_'+str(f_size))(MO_Swap_feature-MO_feature)\r\n",
        "            MO_Att_feature = MO_Attention * MO_Swap_feature\r\n",
        "\r\n",
        "            update_feature[:,:,le_location[1]:le_location[3],le_location[0]:le_location[2]] = LE_Att_feature + LE_feature\r\n",
        "            update_feature[:,:,re_location[1]:re_location[3],re_location[0]:re_location[2]] = RE_Att_feature + RE_feature\r\n",
        "            update_feature[:,:,no_location[1]:no_location[3],no_location[0]:no_location[2]] = NO_Att_feature + NO_feature\r\n",
        "            update_feature[:,:,mo_location[1]:mo_location[3],mo_location[0]:mo_location[2]] = MO_Att_feature + MO_feature\r\n",
        "\r\n",
        "            UpdateVggFeatures.append(update_feature) \r\n",
        "        \r\n",
        "        fea_vgg = self.MSDilate(VggFeatures[3])\r\n",
        "        #new version\r\n",
        "        fea_up0 = self.up0(fea_vgg, UpdateVggFeatures[3])\r\n",
        "        # out1 = F.interpolate(fea_up0,(512,512))\r\n",
        "        # out1 = self.to_rgb0(out1)\r\n",
        "\r\n",
        "        fea_up1 = self.up1( fea_up0, UpdateVggFeatures[2]) #\r\n",
        "        # out2 = F.interpolate(fea_up1,(512,512))\r\n",
        "        # out2 = self.to_rgb1(out2)\r\n",
        "\r\n",
        "        fea_up2 = self.up2(fea_up1, UpdateVggFeatures[1]) #\r\n",
        "        # out3 = F.interpolate(fea_up2,(512,512))\r\n",
        "        # out3 = self.to_rgb2(out3)\r\n",
        "\r\n",
        "        fea_up3 = self.up3(fea_up2, UpdateVggFeatures[0]) #\r\n",
        "        # out4 = F.interpolate(fea_up3,(512,512))\r\n",
        "        # out4 = self.to_rgb3(out4)\r\n",
        "\r\n",
        "        output = self.up4(fea_up3) #\r\n",
        "        \r\n",
        "    \r\n",
        "        return output  #+ out4 + out3 + out2 + out1\r\n",
        "        #0 128 * 256 * 256\r\n",
        "        #1 256 * 128 * 128\r\n",
        "        #2 512 * 64 * 64\r\n",
        "        #3 512 * 32 * 32\r\n",
        "\r\n",
        "\r\n",
        "class UpResBlock(nn.Module):\r\n",
        "    def __init__(self, dim, conv_layer = nn.Conv2d, norm_layer = nn.BatchNorm2d):\r\n",
        "        super(UpResBlock, self).__init__()\r\n",
        "        self.Model = nn.Sequential(\r\n",
        "            # SpectralNorm(conv_layer(dim, dim, 3, 1, 1)),\r\n",
        "            conv_layer(dim, dim, 3, 1, 1),\r\n",
        "            # norm_layer(dim),\r\n",
        "            nn.LeakyReLU(0.2,True),\r\n",
        "            # SpectralNorm(conv_layer(dim, dim, 3, 1, 1)),\r\n",
        "            conv_layer(dim, dim, 3, 1, 1),\r\n",
        "        )\r\n",
        "    def forward(self, x):\r\n",
        "        out = x + self.Model(x)\r\n",
        "        return out\r\n",
        "\r\n",
        "class VggClassNet(nn.Module):\r\n",
        "    def __init__(self, select_layer = ['0','5','10','19']):\r\n",
        "        super(VggClassNet, self).__init__()\r\n",
        "        self.select = select_layer\r\n",
        "        self.vgg = models.vgg19(pretrained=True).features\r\n",
        "        for param in self.parameters():\r\n",
        "            param.requires_grad = False\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        features = []\r\n",
        "        for name, layer in self.vgg._modules.items():\r\n",
        "            x = layer(x)\r\n",
        "            if name in self.select:\r\n",
        "                features.append(x)\r\n",
        "        return features\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    print('this is network')\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuPBRINv90vZ",
        "cellView": "form"
      },
      "source": [
        "#@title custom_dataset.py (paths, disabling augmentations)\r\n",
        "%%writefile /content/DFDNet/data/custom_dataset.py\r\n",
        "# -- coding: utf-8 --\r\n",
        "import os.path\r\n",
        "import os\r\n",
        "import random\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torch\r\n",
        "from PIL import Image, ImageFilter\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import math\r\n",
        "from scipy.io import loadmat\r\n",
        "from PIL import Image\r\n",
        "import PIL\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "import glob\r\n",
        "\r\n",
        "class AlignedDataset(Dataset):\r\n",
        "    \r\n",
        "    def __init__(self, root_dir, fine_size=512, transform=None):\r\n",
        "        self.root_dir = '/content/DFDNet/ffhq'\r\n",
        "        #self.pathes = [os.path.join(self.root_dir, x) for x in os.listdir(self.root_dir) if x[-3:] == 'jpg']\r\n",
        "        \r\n",
        "        self.pathes = glob.glob(self.root_dir + '/**/*.png', recursive=True)\r\n",
        "\r\n",
        "        #print(\"self.pathes\")\r\n",
        "        #print(self.pathes)\r\n",
        "        self.transform = transform\r\n",
        "        self.fine_size = fine_size\r\n",
        "        self.partpath = '/content/DFDNet/landmarks'\r\n",
        "        \r\n",
        "    def AddNoise(self,img): # noise\r\n",
        "        if random.random() > 0.9: #\r\n",
        "            return img\r\n",
        "        self.sigma = np.random.randint(1, 11)\r\n",
        "        img_tensor = torch.from_numpy(np.array(img)).float()\r\n",
        "        noise = torch.randn(img_tensor.size()).mul_(self.sigma/1.0)\r\n",
        "\r\n",
        "        noiseimg = torch.clamp(noise+img_tensor,0,255)\r\n",
        "        return Image.fromarray(np.uint8(noiseimg.numpy()))\r\n",
        "\r\n",
        "    def AddBlur(self,img): # gaussian blur or motion blur\r\n",
        "        if random.random() > 0.9: #\r\n",
        "            return img\r\n",
        "        img = np.array(img)\r\n",
        "        if random.random() > 0.35: ##gaussian blur\r\n",
        "            blursize = random.randint(1,17) * 2 + 1 ##3,5,7,9,11,13,15\r\n",
        "            blursigma = random.randint(3, 20)\r\n",
        "            img = cv2.GaussianBlur(img, (blursize,blursize), blursigma/10)\r\n",
        "        else: #motion blur\r\n",
        "            M = random.randint(1,32)\r\n",
        "            KName = './data/MotionBlurKernel/m_%02d.mat' % M\r\n",
        "            k = loadmat(KName)['kernel']\r\n",
        "            k = k.astype(np.float32)\r\n",
        "            k /= np.sum(k)\r\n",
        "            img = cv2.filter2D(img,-1,k)\r\n",
        "        return Image.fromarray(img)\r\n",
        "\r\n",
        "    def AddDownSample(self,img): # downsampling\r\n",
        "        if random.random() > 0.95: #\r\n",
        "            return img\r\n",
        "        sampler = random.randint(20, 100)*1.0\r\n",
        "        img = img.resize((int(self.fine_size/sampler*10.0), int(self.fine_size/sampler*10.0)), Image.BICUBIC)\r\n",
        "        return img\r\n",
        "\r\n",
        "    def AddJPEG(self,img): # JPEG compression\r\n",
        "        if random.random() > 0.6:\r\n",
        "            return img\r\n",
        "        imQ = random.randint(40, 80)\r\n",
        "        img = np.array(img)\r\n",
        "        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY),imQ] # (0,100),higher is better,default is 95\r\n",
        "        _, encA = cv2.imencode('.jpg', img, encode_param)\r\n",
        "        img = cv2.imdecode(encA,1)\r\n",
        "        return Image.fromarray(img)\r\n",
        "\r\n",
        "    def AddUpSample(self,img):\r\n",
        "        return img.resize((self.fine_size, self.fine_size), Image.BICUBIC)\r\n",
        "\r\n",
        "    def __getitem__(self, index): # indexation\r\n",
        "\r\n",
        "        path = self.pathes[index]\r\n",
        "        Imgs = Image.open(path).convert('RGB')\r\n",
        "        \r\n",
        "        A = Imgs.resize((self.fine_size, self.fine_size))\r\n",
        "        #A = Imgs\r\n",
        "        #A = transforms.ColorJitter(0.3, 0.3, 0.3, 0)(A)\r\n",
        "        C = A\r\n",
        "        #A = self.AddBlur(A)\r\n",
        "        \r\n",
        "        tmps = path.split('/')\r\n",
        "        ImgName = tmps[-1]\r\n",
        "        part_locations = self.get_part_location(self.partpath, ImgName, 2)\r\n",
        "        \r\n",
        "        A = transforms.ToTensor()(A)\r\n",
        "        C = transforms.ToTensor()(C)\r\n",
        "        \r\n",
        "        A = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(A) \r\n",
        "        C = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(C)\r\n",
        "        \r\n",
        "        return {'A': A, 'C': C, 'path': path, 'part_locations': part_locations}\r\n",
        "\r\n",
        "    def get_part_location(self, landmarkpath, imgname, downscale=1):\r\n",
        "        Landmarks = []\r\n",
        "        with open(os.path.join(landmarkpath, imgname + '.txt'),'r') as f:\r\n",
        "            for line in f:\r\n",
        "                tmp = [np.float(i) for i in line.split(' ') if i != '\\n']\r\n",
        "                Landmarks.append(tmp)\r\n",
        "        Landmarks = np.array(Landmarks)/downscale # 512 * 512\r\n",
        "        \r\n",
        "        Map_LE = list(np.hstack((range(17,22), range(36,42))))\r\n",
        "        Map_RE = list(np.hstack((range(22,27), range(42,48))))\r\n",
        "        Map_NO = list(range(29,36))\r\n",
        "        Map_MO = list(range(48,68))\r\n",
        "        #left eye\r\n",
        "        Mean_LE = np.mean(Landmarks[Map_LE],0)\r\n",
        "        L_LE = np.max((np.max(np.max(Landmarks[Map_LE],0) - np.min(Landmarks[Map_LE],0))/2,16))\r\n",
        "        Location_LE = np.hstack((Mean_LE - L_LE + 1, Mean_LE + L_LE)).astype(int)\r\n",
        "        #right eye\r\n",
        "        Mean_RE = np.mean(Landmarks[Map_RE],0)\r\n",
        "        L_RE = np.max((np.max(np.max(Landmarks[Map_RE],0) - np.min(Landmarks[Map_RE],0))/2,16))\r\n",
        "        Location_RE = np.hstack((Mean_RE - L_RE + 1, Mean_RE + L_RE)).astype(int)\r\n",
        "        #nose\r\n",
        "        Mean_NO = np.mean(Landmarks[Map_NO],0)\r\n",
        "        L_NO = np.max((np.max(np.max(Landmarks[Map_NO],0) - np.min(Landmarks[Map_NO],0))/2,16))\r\n",
        "        Location_NO = np.hstack((Mean_NO - L_NO + 1, Mean_NO + L_NO)).astype(int)\r\n",
        "        #mouth\r\n",
        "        Mean_MO = np.mean(Landmarks[Map_MO],0)\r\n",
        "        L_MO = np.max((np.max(np.max(Landmarks[Map_MO],0) - np.min(Landmarks[Map_MO],0))/2,16))\r\n",
        "\r\n",
        "        Location_MO = np.hstack((Mean_MO - L_MO + 1, Mean_MO + L_MO)).astype(int)\r\n",
        "        return Location_LE, Location_RE, Location_NO, Location_MO\r\n",
        "\r\n",
        "    def __len__(self): #\r\n",
        "        return len(self.pathes)\r\n",
        "\r\n",
        "    def name(self):\r\n",
        "        return 'AlignedDataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTzUnATuV7Nb"
      },
      "source": [
        "# generating feature files\r\n",
        "%cd /content/DFDNet\r\n",
        "!python run.py --batchSize 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyiL_FCPZ1sI",
        "cellView": "form"
      },
      "source": [
        "#@title viewing shape of saved file\r\n",
        "import torch\r\n",
        "test = torch.load('/content/DFDNet/LE_feature_resize_256.pt')\r\n",
        "print(test.shape)\r\n",
        "print(test.is_cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W8UmBgeumW7"
      },
      "source": [
        "The loading of features is not done from ```.npy``` files anymore, but directly with torch. The generated features won't be compatible with the old code and will be loaded into a Dict with ```torch.load```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwibBxMkfhKS",
        "cellView": "form"
      },
      "source": [
        "#@title networks.py (train with own features, replacing numpy code)\r\n",
        "%%writefile /content/DFDNet/models/networks.py\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.nn import init\r\n",
        "import functools\r\n",
        "from torch.optim import lr_scheduler\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.nn import Parameter as P\r\n",
        "from util import util\r\n",
        "from torchvision import models\r\n",
        "import scipy.io as sio\r\n",
        "import numpy as np\r\n",
        "import scipy.ndimage\r\n",
        "import torch.nn.utils.spectral_norm as SpectralNorm\r\n",
        "\r\n",
        "from torch.autograd import Function\r\n",
        "from math import sqrt\r\n",
        "import random\r\n",
        "import os\r\n",
        "import math\r\n",
        "\r\n",
        "from sync_batchnorm import convert_model\r\n",
        "####\r\n",
        "\r\n",
        "###############################################################################\r\n",
        "# Helper Functions\r\n",
        "###############################################################################\r\n",
        "def get_norm_layer(norm_type='instance'):\r\n",
        "    if norm_type == 'batch':\r\n",
        "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\r\n",
        "    elif norm_type == 'instance':\r\n",
        "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\r\n",
        "    elif norm_type == 'none':\r\n",
        "        norm_layer = None\r\n",
        "    else:\r\n",
        "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\r\n",
        "\r\n",
        "    return norm_layer\r\n",
        "\r\n",
        "\r\n",
        "def get_scheduler(optimizer, opt):\r\n",
        "    if opt.lr_policy == 'lambda':\r\n",
        "        def lambda_rule(epoch):\r\n",
        "            lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\r\n",
        "            return lr_l\r\n",
        "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\r\n",
        "    elif opt.lr_policy == 'step':\r\n",
        "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\r\n",
        "    elif opt.lr_policy == 'plateau':\r\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\r\n",
        "    else:\r\n",
        "        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\r\n",
        "\r\n",
        "    return scheduler\r\n",
        "\r\n",
        "\r\n",
        "def init_weights(net, init_type='normal', gain=0.02):\r\n",
        "    def init_func(m):\r\n",
        "        classname = m.__class__.__name__\r\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\r\n",
        "            if init_type == 'normal':\r\n",
        "                init.normal_(m.weight.data, 0.0, gain)\r\n",
        "            elif init_type == 'xavier':\r\n",
        "                init.xavier_normal_(m.weight.data, gain=gain)\r\n",
        "            elif init_type == 'kaiming':\r\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\r\n",
        "            elif init_type == 'orthogonal':\r\n",
        "                init.orthogonal_(m.weight.data, gain=gain)\r\n",
        "            else:\r\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\r\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\r\n",
        "                init.constant_(m.bias.data, 0.0)\r\n",
        "        elif classname.find('BatchNorm2d') != -1:\r\n",
        "            init.normal_(m.weight.data, 1.0, gain)\r\n",
        "            init.constant_(m.bias.data, 0.0)\r\n",
        "\r\n",
        "    print('initialize network with %s' % init_type)\r\n",
        "    net.apply(init_func)\r\n",
        "\r\n",
        "\r\n",
        "def init_net(net, init_type='normal', init_gain=0.02, gpu_ids=[], init_flag=True):\r\n",
        "    if len(gpu_ids) > 0:\r\n",
        "        assert(torch.cuda.is_available())\r\n",
        "        net = convert_model(net)\r\n",
        "        net.to(gpu_ids[0])\r\n",
        "        net = torch.nn.DataParallel(net, gpu_ids)\r\n",
        "\r\n",
        "    if init_flag:\r\n",
        "\r\n",
        "        init_weights(net, init_type, gain=init_gain)\r\n",
        "\r\n",
        "    return net\r\n",
        "\r\n",
        "\r\n",
        "# compute adaptive instance norm\r\n",
        "def calc_mean_std(feat, eps=1e-5):\r\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\r\n",
        "    size = feat.size()\r\n",
        "    assert (len(size) == 3)\r\n",
        "    C, _ = size[:2]\r\n",
        "    feat_var = feat.contiguous().view(C, -1).var(dim=1) + eps\r\n",
        "    feat_std = feat_var.sqrt().view(C, 1, 1)\r\n",
        "    feat_mean = feat.contiguous().view(C, -1).mean(dim=1).view(C, 1, 1)\r\n",
        "\r\n",
        "    return feat_mean, feat_std\r\n",
        "\r\n",
        "\r\n",
        "def adaptive_instance_normalization(content_feat, style_feat):  # content_feat is degraded feature, style is ref feature\r\n",
        "    assert (content_feat.size()[:1] == style_feat.size()[:1])\r\n",
        "    size = content_feat.size()\r\n",
        "    style_mean, style_std = calc_mean_std(style_feat)\r\n",
        "    content_mean, content_std = calc_mean_std(content_feat)\r\n",
        "\r\n",
        "    normalized_feat = (content_feat - content_mean.expand(\r\n",
        "        size)) / content_std.expand(size)\r\n",
        "\r\n",
        "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\r\n",
        "\r\n",
        "def calc_mean_std_4D(feat, eps=1e-5):\r\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\r\n",
        "    size = feat.size()\r\n",
        "    assert (len(size) == 4)\r\n",
        "    N, C = size[:2]\r\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\r\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\r\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\r\n",
        "    return feat_mean, feat_std\r\n",
        "\r\n",
        "def adaptive_instance_normalization_4D(content_feat, style_feat): # content_feat is ref feature, style is degradate feature\r\n",
        "    # assert (content_feat.size()[:2] == style_feat.size()[:2])\r\n",
        "    size = content_feat.size()\r\n",
        "    style_mean, style_std = calc_mean_std_4D(style_feat)\r\n",
        "    content_mean, content_std = calc_mean_std_4D(content_feat)\r\n",
        "    normalized_feat = (content_feat - content_mean.expand(\r\n",
        "        size)) / content_std.expand(size)\r\n",
        "    return normalized_feat * style_std + style_mean\r\n",
        "\r\n",
        "def define_G(which_model_netG, gpu_ids=[]):\r\n",
        "    if which_model_netG == 'UNetDictFace':\r\n",
        "        netG = UNetDictFace(64)\r\n",
        "        init_flag = False\r\n",
        "    else:\r\n",
        "        raise NotImplementedError('Generator model name [%s] is not recognized' % which_model_netG)\r\n",
        "    return init_net(netG, 'normal', 0.02, gpu_ids, init_flag)\r\n",
        "\r\n",
        "\r\n",
        "##############################################################################\r\n",
        "# Classes\r\n",
        "############################################################################################################################################\r\n",
        "\r\n",
        "\r\n",
        "def convU(in_channels, out_channels,conv_layer, norm_layer, kernel_size=3, stride=1,dilation=1, bias=True):\r\n",
        "    return nn.Sequential(\r\n",
        "        SpectralNorm(conv_layer(in_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias)),\r\n",
        "#         conv_layer(in_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias),\r\n",
        "#         nn.BatchNorm2d(out_channels),\r\n",
        "        nn.LeakyReLU(0.2),\r\n",
        "        SpectralNorm(conv_layer(out_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias)),\r\n",
        "    )\r\n",
        "class MSDilateBlock(nn.Module):\r\n",
        "    def __init__(self, in_channels,conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, kernel_size=3, dilation=[1,1,1,1], bias=True):\r\n",
        "        super(MSDilateBlock, self).__init__()\r\n",
        "        self.conv1 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[0], bias=bias)\r\n",
        "        self.conv2 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[1], bias=bias)\r\n",
        "        self.conv3 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[2], bias=bias)\r\n",
        "        self.conv4 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[3], bias=bias)\r\n",
        "        self.convi =  SpectralNorm(conv_layer(in_channels*4, in_channels, kernel_size=kernel_size, stride=1, padding=(kernel_size-1)//2, bias=bias))\r\n",
        "    def forward(self, x):\r\n",
        "        conv1 = self.conv1(x)\r\n",
        "        conv2 = self.conv2(x)\r\n",
        "        conv3 = self.conv3(x)\r\n",
        "        conv4 = self.conv4(x)\r\n",
        "        cat  = torch.cat([conv1, conv2, conv3, conv4], 1)\r\n",
        "        out = self.convi(cat) + x\r\n",
        "        return out\r\n",
        "\r\n",
        "##############################UNetFace#########################\r\n",
        "class AdaptiveInstanceNorm(nn.Module):\r\n",
        "    def __init__(self, in_channel):\r\n",
        "        super().__init__()\r\n",
        "        self.norm = nn.InstanceNorm2d(in_channel)\r\n",
        "\r\n",
        "    def forward(self, input, style):\r\n",
        "        style_mean, style_std = calc_mean_std_4D(style)\r\n",
        "        out = self.norm(input)\r\n",
        "        size = input.size()\r\n",
        "        out = style_std.expand(size) * out + style_mean.expand(size)\r\n",
        "        return out\r\n",
        "\r\n",
        "class BlurFunctionBackward(Function):\r\n",
        "    @staticmethod\r\n",
        "    def forward(ctx, grad_output, kernel, kernel_flip):\r\n",
        "        ctx.save_for_backward(kernel, kernel_flip)\r\n",
        "\r\n",
        "        grad_input = F.conv2d(\r\n",
        "            grad_output, kernel_flip, padding=1, groups=grad_output.shape[1]\r\n",
        "        )\r\n",
        "        return grad_input\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def backward(ctx, gradgrad_output):\r\n",
        "        kernel, kernel_flip = ctx.saved_tensors\r\n",
        "\r\n",
        "        grad_input = F.conv2d(\r\n",
        "            gradgrad_output, kernel, padding=1, groups=gradgrad_output.shape[1]\r\n",
        "        )\r\n",
        "        return grad_input, None, None\r\n",
        "\r\n",
        "\r\n",
        "class BlurFunction(Function):\r\n",
        "    @staticmethod\r\n",
        "    def forward(ctx, input, kernel, kernel_flip):\r\n",
        "        ctx.save_for_backward(kernel, kernel_flip)\r\n",
        "\r\n",
        "        output = F.conv2d(input, kernel, padding=1, groups=input.shape[1])\r\n",
        "\r\n",
        "        return output\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def backward(ctx, grad_output):\r\n",
        "        kernel, kernel_flip = ctx.saved_tensors\r\n",
        "\r\n",
        "        grad_input = BlurFunctionBackward.apply(grad_output, kernel, kernel_flip)\r\n",
        "\r\n",
        "        return grad_input, None, None\r\n",
        "\r\n",
        "blur = BlurFunction.apply\r\n",
        "\r\n",
        "\r\n",
        "class Blur(nn.Module):\r\n",
        "    def __init__(self, channel):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        weight = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=torch.float32)\r\n",
        "        weight = weight.view(1, 1, 3, 3)\r\n",
        "        weight = weight / weight.sum()\r\n",
        "        weight_flip = torch.flip(weight, [2, 3])\r\n",
        "\r\n",
        "        self.register_buffer('weight', weight.repeat(channel, 1, 1, 1))\r\n",
        "        self.register_buffer('weight_flip', weight_flip.repeat(channel, 1, 1, 1))\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        return blur(input, self.weight, self.weight_flip)\r\n",
        "\r\n",
        "class EqualLR:\r\n",
        "    def __init__(self, name):\r\n",
        "        self.name = name\r\n",
        "\r\n",
        "    def compute_weight(self, module):\r\n",
        "        weight = getattr(module, self.name + '_orig')\r\n",
        "        fan_in = weight.data.size(1) * weight.data[0][0].numel()\r\n",
        "        return weight * sqrt(2 / fan_in)\r\n",
        "    @staticmethod\r\n",
        "    def apply(module, name):\r\n",
        "        fn = EqualLR(name)\r\n",
        "\r\n",
        "        weight = getattr(module, name)\r\n",
        "        del module._parameters[name]\r\n",
        "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\r\n",
        "        module.register_forward_pre_hook(fn)\r\n",
        "\r\n",
        "        return fn\r\n",
        "\r\n",
        "    def __call__(self, module, input):\r\n",
        "        weight = self.compute_weight(module)\r\n",
        "        setattr(module, self.name, weight)\r\n",
        "\r\n",
        "def equal_lr(module, name='weight'):\r\n",
        "    EqualLR.apply(module, name)\r\n",
        "    return module\r\n",
        "\r\n",
        "class EqualConv2d(nn.Module):\r\n",
        "    def __init__(self, *args, **kwargs):\r\n",
        "        super().__init__()\r\n",
        "        conv = nn.Conv2d(*args, **kwargs)\r\n",
        "        conv.weight.data.normal_()\r\n",
        "        conv.bias.data.zero_()\r\n",
        "        self.conv = equal_lr(conv)\r\n",
        "    def forward(self, input):\r\n",
        "        return self.conv(input)\r\n",
        "\r\n",
        "class NoiseInjection(nn.Module):\r\n",
        "    def __init__(self, channel):\r\n",
        "        super().__init__()\r\n",
        "        self.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))\r\n",
        "    def forward(self, image, noise):\r\n",
        "        return image + self.weight * noise\r\n",
        "\r\n",
        "class StyledUpBlock(nn.Module):\r\n",
        "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1,upsample=False):\r\n",
        "        super().__init__()\r\n",
        "        if upsample:\r\n",
        "            self.conv1 = nn.Sequential(\r\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\r\n",
        "                Blur(out_channel),\r\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding),\r\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\r\n",
        "                nn.LeakyReLU(0.2),\r\n",
        "            )\r\n",
        "        else:\r\n",
        "            self.conv1 = nn.Sequential(\r\n",
        "                Blur(in_channel),\r\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding)\r\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\r\n",
        "                nn.LeakyReLU(0.2),\r\n",
        "            )\r\n",
        "        self.convup = nn.Sequential(\r\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\r\n",
        "                # EqualConv2d(out_channel, out_channel, kernel_size, padding=padding),\r\n",
        "                SpectralNorm(nn.Conv2d(out_channel, out_channel, kernel_size, padding=padding)),\r\n",
        "                nn.LeakyReLU(0.2),\r\n",
        "                # Blur(out_channel),\r\n",
        "            )\r\n",
        "        # self.noise1 = equal_lr(NoiseInjection(out_channel))\r\n",
        "        # self.adain1 = AdaptiveInstanceNorm(out_channel)\r\n",
        "        self.lrelu1 = nn.LeakyReLU(0.2)\r\n",
        "\r\n",
        "        # self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\r\n",
        "        # self.noise2 = equal_lr(NoiseInjection(out_channel))\r\n",
        "        # self.adain2 = AdaptiveInstanceNorm(out_channel)\r\n",
        "        # self.lrelu2 = nn.LeakyReLU(0.2)\r\n",
        "\r\n",
        "        self.ScaleModel1 = nn.Sequential(\r\n",
        "            # Blur(in_channel),\r\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\r\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\r\n",
        "            nn.LeakyReLU(0.2, True),\r\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1))\r\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\r\n",
        "        )\r\n",
        "        self.ShiftModel1 = nn.Sequential(\r\n",
        "            # Blur(in_channel),\r\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\r\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\r\n",
        "            nn.LeakyReLU(0.2, True),\r\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1)),\r\n",
        "            nn.Sigmoid(),\r\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\r\n",
        "        )\r\n",
        "       \r\n",
        "    def forward(self, input, style):\r\n",
        "        out = self.conv1(input)\r\n",
        "#         out = self.noise1(out, noise)\r\n",
        "        out = self.lrelu1(out)\r\n",
        "\r\n",
        "        Shift1 = self.ShiftModel1(style)\r\n",
        "        Scale1 = self.ScaleModel1(style)\r\n",
        "        out = out * Scale1 + Shift1\r\n",
        "        # out = self.adain1(out, style)\r\n",
        "        outup = self.convup(out)\r\n",
        "\r\n",
        "        return outup\r\n",
        "\r\n",
        "##############################################################################\r\n",
        "##Face Dictionary\r\n",
        "##############################################################################\r\n",
        "class VGGFeat(torch.nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Input: (B, C, H, W), RGB, [-1, 1]\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, weight_path='./weights/vgg19.pth'):\r\n",
        "        super().__init__()\r\n",
        "        self.model = models.vgg19(pretrained=False)\r\n",
        "        self.build_vgg_layers()\r\n",
        "        \r\n",
        "        self.model.load_state_dict(torch.load(weight_path))\r\n",
        "\r\n",
        "        self.register_parameter(\"RGB_mean\", nn.Parameter(torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)))\r\n",
        "        self.register_parameter(\"RGB_std\", nn.Parameter(torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)))\r\n",
        "        \r\n",
        "        # self.model.eval()\r\n",
        "        for param in self.model.parameters():\r\n",
        "            param.requires_grad = False\r\n",
        "    \r\n",
        "    def build_vgg_layers(self):\r\n",
        "        vgg_pretrained_features = self.model.features\r\n",
        "        self.features = []\r\n",
        "        # feature_layers = [0, 3, 8, 17, 26, 35]\r\n",
        "        feature_layers = [0, 8, 17, 26, 35]\r\n",
        "        for i in range(len(feature_layers)-1): \r\n",
        "            module_layers = torch.nn.Sequential() \r\n",
        "            for j in range(feature_layers[i], feature_layers[i+1]):\r\n",
        "                module_layers.add_module(str(j), vgg_pretrained_features[j])\r\n",
        "            self.features.append(module_layers)\r\n",
        "        self.features = torch.nn.ModuleList(self.features)\r\n",
        "\r\n",
        "    def preprocess(self, x):\r\n",
        "        x = (x + 1) / 2\r\n",
        "        x = (x - self.RGB_mean) / self.RGB_std\r\n",
        "        if x.shape[3] < 224:\r\n",
        "            x = torch.nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.preprocess(x)\r\n",
        "        features = []\r\n",
        "        for m in self.features:\r\n",
        "            # print(m)\r\n",
        "            x = m(x)\r\n",
        "            features.append(x)\r\n",
        "        return features \r\n",
        "\r\n",
        "def compute_sum(x, axis=None, keepdim=False):\r\n",
        "    if not axis:\r\n",
        "        axis = range(len(x.shape))\r\n",
        "    for i in sorted(axis, reverse=True):\r\n",
        "        x = torch.sum(x, dim=i, keepdim=keepdim)\r\n",
        "    return x\r\n",
        "def ToRGB(in_channel):\r\n",
        "    return nn.Sequential(\r\n",
        "        SpectralNorm(nn.Conv2d(in_channel,in_channel,3, 1, 1)),\r\n",
        "        nn.LeakyReLU(0.2),\r\n",
        "        SpectralNorm(nn.Conv2d(in_channel,3,3, 1, 1))\r\n",
        "    )\r\n",
        "\r\n",
        "def AttentionBlock(in_channel):\r\n",
        "    return nn.Sequential(\r\n",
        "        SpectralNorm(nn.Conv2d(in_channel, in_channel, 3, 1, 1)),\r\n",
        "        nn.LeakyReLU(0.2),\r\n",
        "        SpectralNorm(nn.Conv2d(in_channel, in_channel, 3, 1, 1))\r\n",
        "    )\r\n",
        "\r\n",
        "class UNetDictFace(nn.Module):\r\n",
        "    def __init__(self, ngf=64, dictionary_path='./DictionaryCenter512'):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.part_sizes = np.array([80,80,50,110]) # size for 512\r\n",
        "        self.feature_sizes = np.array([256,128,64,32])\r\n",
        "        self.channel_sizes = np.array([128,256,512,512])\r\n",
        "        Parts = ['left_eye','right_eye','nose','mouth']\r\n",
        "        self.Dict_256 = {}\r\n",
        "        self.Dict_128 = {}\r\n",
        "        self.Dict_64 = {}\r\n",
        "        self.Dict_32 = {}\r\n",
        "        \"\"\"\r\n",
        "        for j,i in enumerate(Parts):\r\n",
        "            f_256 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_256_center.npy'.format(i)), allow_pickle=True))\r\n",
        "\r\n",
        "            f_256_reshape = f_256.reshape(f_256.size(0),self.channel_sizes[0],self.part_sizes[j]//2,self.part_sizes[j]//2)\r\n",
        "            max_256 = torch.max(torch.sqrt(compute_sum(torch.pow(f_256_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\r\n",
        "            self.Dict_256[i] = f_256_reshape #/ max_256\r\n",
        "\r\n",
        "            f_128 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_128_center.npy'.format(i)), allow_pickle=True))\r\n",
        "\r\n",
        "            f_128_reshape = f_128.reshape(f_128.size(0),self.channel_sizes[1],self.part_sizes[j]//4,self.part_sizes[j]//4)\r\n",
        "            max_128 = torch.max(torch.sqrt(compute_sum(torch.pow(f_128_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\r\n",
        "            self.Dict_128[i] = f_128_reshape #/ max_128\r\n",
        "\r\n",
        "            f_64 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_64_center.npy'.format(i)), allow_pickle=True))\r\n",
        "\r\n",
        "            f_64_reshape = f_64.reshape(f_64.size(0),self.channel_sizes[2],self.part_sizes[j]//8,self.part_sizes[j]//8)\r\n",
        "            max_64 = torch.max(torch.sqrt(compute_sum(torch.pow(f_64_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\r\n",
        "            self.Dict_64[i] = f_64_reshape #/ max_64\r\n",
        "\r\n",
        "            f_32 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_32_center.npy'.format(i)), allow_pickle=True))\r\n",
        "\r\n",
        "            f_32_reshape = f_32.reshape(f_32.size(0),self.channel_sizes[3],self.part_sizes[j]//16,self.part_sizes[j]//16)\r\n",
        "            max_32 = torch.max(torch.sqrt(compute_sum(torch.pow(f_32_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\r\n",
        "            self.Dict_32[i] = f_32_reshape #/ max_32\r\n",
        "        \"\"\"\r\n",
        "        #Parts = ['left_eye','right_eye','nose','mouth']\r\n",
        "        self.Dict_256['left_eye'] = torch.load('/content/DFDNet/LE_feature_resize_256.pt')\r\n",
        "        self.Dict_256['right_eye'] = torch.load('/content/DFDNet/RE_feature_resize_256.pt')\r\n",
        "        self.Dict_256['nose'] = torch.load('/content/DFDNet/NO_feature_resize_256.pt')\r\n",
        "        self.Dict_256['mouth'] = torch.load('/content/DFDNet/MO_feature_resize_256.pt')\r\n",
        "\r\n",
        "        self.Dict_128['left_eye'] = torch.load('/content/DFDNet/LE_feature_resize_128.pt')\r\n",
        "        self.Dict_128['right_eye'] = torch.load('/content/DFDNet/RE_feature_resize_128.pt')\r\n",
        "        self.Dict_128['nose'] = torch.load('/content/DFDNet/NO_feature_resize_128.pt')\r\n",
        "        self.Dict_128['mouth'] = torch.load('/content/DFDNet/MO_feature_resize_128.pt')\r\n",
        "\r\n",
        "        self.Dict_64['left_eye'] = torch.load('/content/DFDNet/LE_feature_resize_64.pt')\r\n",
        "        self.Dict_64['right_eye'] = torch.load('/content/DFDNet/RE_feature_resize_64.pt')\r\n",
        "        self.Dict_64['nose'] = torch.load('/content/DFDNet/NO_feature_resize_64.pt')\r\n",
        "        self.Dict_64['mouth'] = torch.load('/content/DFDNet/MO_feature_resize_64.pt')\r\n",
        "\r\n",
        "        self.Dict_32['left_eye'] = torch.load('/content/DFDNet/LE_feature_resize_32.pt')\r\n",
        "        self.Dict_32['right_eye'] = torch.load('/content/DFDNet/RE_feature_resize_32.pt')\r\n",
        "        self.Dict_32['nose'] = torch.load('/content/DFDNet/NO_feature_resize_32.pt')\r\n",
        "        self.Dict_32['mouth'] = torch.load('/content/DFDNet/MO_feature_resize_32.pt')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        print(\"self.Dict_256\")\r\n",
        "        print(len(self.Dict_256))\r\n",
        "        print(\"self.Dict_256['left_eye'].shape\")\r\n",
        "        print(self.Dict_256['left_eye'].shape)\r\n",
        "        print(\"self.Dict_256['right_eye'].shape\")\r\n",
        "        print(self.Dict_256['right_eye'].shape)\r\n",
        "        print(\"self.Dict_256['nose'].shape\")\r\n",
        "        print(self.Dict_256['nose'].shape)\r\n",
        "        print(\"self.Dict_256['mouth'].shape\")\r\n",
        "        print(self.Dict_256['mouth'].shape)\r\n",
        "        print(\"-------------------\")\r\n",
        "        print(\"self.Dict_128\")\r\n",
        "        print(len(self.Dict_128))\r\n",
        "        print(\"self.Dict_128['left_eye'].shape\")\r\n",
        "        print(self.Dict_128['left_eye'].shape)\r\n",
        "        print(\"self.Dict_128['right_eye'].shape\")\r\n",
        "        print(self.Dict_128['right_eye'].shape)\r\n",
        "        print(\"self.Dict_128['nose'].shape\")\r\n",
        "        print(self.Dict_128['nose'].shape)\r\n",
        "        print(\"self.Dict_128['mouth'].shape\")\r\n",
        "        print(self.Dict_128['mouth'].shape)\r\n",
        "        print(\"-------------------\")\r\n",
        "        print(\"self.Dict_64\")\r\n",
        "        print(len(self.Dict_64))\r\n",
        "        print(\"self.Dict_64['left_eye'].shape\")\r\n",
        "        print(self.Dict_64['left_eye'].shape)\r\n",
        "        print(\"self.Dict_64['right_eye'].shape\")\r\n",
        "        print(self.Dict_64['right_eye'].shape)\r\n",
        "        print(\"self.Dict_64['nose'].shape\")\r\n",
        "        print(self.Dict_64['nose'].shape)\r\n",
        "        print(\"self.Dict_64['mouth'].shape\")\r\n",
        "        print(self.Dict_64['mouth'].shape)\r\n",
        "        print(\"-------------------\")\r\n",
        "        print(\"self.Dict_32\")\r\n",
        "        print(len(self.Dict_32))\r\n",
        "        print(\"self.Dict_32['left_eye'].shape\")\r\n",
        "        print(self.Dict_32['left_eye'].shape)\r\n",
        "        print(\"self.Dict_32['right_eye'].shape\")\r\n",
        "        print(self.Dict_32['right_eye'].shape)\r\n",
        "        print(\"self.Dict_32['nose'].shape\")\r\n",
        "        print(self.Dict_32['nose'].shape)\r\n",
        "        print(\"self.Dict_32['mouth'].shape\")\r\n",
        "        print(self.Dict_32['mouth'].shape)\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        self.le_256 = AttentionBlock(128)\r\n",
        "        self.le_128 = AttentionBlock(256)\r\n",
        "        self.le_64 = AttentionBlock(512)\r\n",
        "        self.le_32 = AttentionBlock(512)\r\n",
        "\r\n",
        "        self.re_256 = AttentionBlock(128)\r\n",
        "        self.re_128 = AttentionBlock(256)\r\n",
        "        self.re_64 = AttentionBlock(512)\r\n",
        "        self.re_32 = AttentionBlock(512)\r\n",
        "\r\n",
        "        self.no_256 = AttentionBlock(128)\r\n",
        "        self.no_128 = AttentionBlock(256)\r\n",
        "        self.no_64 = AttentionBlock(512)\r\n",
        "        self.no_32 = AttentionBlock(512)\r\n",
        "\r\n",
        "        self.mo_256 = AttentionBlock(128)\r\n",
        "        self.mo_128 = AttentionBlock(256)\r\n",
        "        self.mo_64 = AttentionBlock(512)\r\n",
        "        self.mo_32 = AttentionBlock(512)\r\n",
        "\r\n",
        "        #norm\r\n",
        "        self.VggExtract = VGGFeat()\r\n",
        "        \r\n",
        "        ######################\r\n",
        "        self.MSDilate = MSDilateBlock(ngf*8, dilation = [4,3,2,1])  #\r\n",
        "\r\n",
        "        self.up0 = StyledUpBlock(ngf*8,ngf*8)\r\n",
        "        self.up1 = StyledUpBlock(ngf*8, ngf*4) #\r\n",
        "        self.up2 = StyledUpBlock(ngf*4, ngf*2) #\r\n",
        "        self.up3 = StyledUpBlock(ngf*2, ngf) #\r\n",
        "        self.up4 = nn.Sequential( # 128\r\n",
        "            # nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\r\n",
        "            SpectralNorm(nn.Conv2d(ngf, ngf, 3, 1, 1)),\r\n",
        "            # nn.BatchNorm2d(32),\r\n",
        "            nn.LeakyReLU(0.2),\r\n",
        "            UpResBlock(ngf),\r\n",
        "            UpResBlock(ngf),\r\n",
        "            # SpectralNorm(nn.Conv2d(ngf, 3, kernel_size=3, stride=1, padding=1)),\r\n",
        "            nn.Conv2d(ngf, 3, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.Tanh()\r\n",
        "        )\r\n",
        "        self.to_rgb0 = ToRGB(ngf*8)\r\n",
        "        self.to_rgb1 = ToRGB(ngf*4)\r\n",
        "        self.to_rgb2 = ToRGB(ngf*2)\r\n",
        "        self.to_rgb3 = ToRGB(ngf*1)\r\n",
        "\r\n",
        "        # for param in self.BlurInputConv.parameters():\r\n",
        "        #     param.requires_grad = False\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "        self.count_256 = 0\r\n",
        "        self.count_128 = 0\r\n",
        "        self.count_64 = 0\r\n",
        "        self.count_32 = 0\r\n",
        "\r\n",
        "    def forward(self, input, part_locations):\r\n",
        "        #print(\"input.shape\")\r\n",
        "        #print(input.shape)\r\n",
        "        VggFeatures = self.VggExtract(input) #VggFeatures = list object\r\n",
        "        # for b in range(input.size(0)):\r\n",
        "        b = 0\r\n",
        "        UpdateVggFeatures = []\r\n",
        "        for i, f_size in enumerate(self.feature_sizes):\r\n",
        "            cur_feature = VggFeatures[i]\r\n",
        "            #print(\"cur_feature.shape\")\r\n",
        "            #print(cur_feature.shape)\r\n",
        "\r\n",
        "            update_feature = cur_feature.clone() #* 0\r\n",
        "            cur_part_sizes = self.part_sizes // (512/f_size)\r\n",
        "            dicts_feature = getattr(self, 'Dict_'+str(f_size))\r\n",
        "            \r\n",
        "            LE_Dict_feature = dicts_feature['left_eye'].to(input)\r\n",
        "            RE_Dict_feature = dicts_feature['right_eye'].to(input)\r\n",
        "            NO_Dict_feature = dicts_feature['nose'].to(input)\r\n",
        "            MO_Dict_feature = dicts_feature['mouth'].to(input)\r\n",
        "\r\n",
        "            le_location = (part_locations[0][b] // (512/f_size)).int()\r\n",
        "            re_location = (part_locations[1][b] // (512/f_size)).int()\r\n",
        "            no_location = (part_locations[2][b] // (512/f_size)).int()\r\n",
        "            mo_location = (part_locations[3][b] // (512/f_size)).int()\r\n",
        "\r\n",
        "            LE_feature = cur_feature[:,:,le_location[1]:le_location[3],le_location[0]:le_location[2]].clone()\r\n",
        "            RE_feature = cur_feature[:,:,re_location[1]:re_location[3],re_location[0]:re_location[2]].clone()\r\n",
        "            NO_feature = cur_feature[:,:,no_location[1]:no_location[3],no_location[0]:no_location[2]].clone()\r\n",
        "            MO_feature = cur_feature[:,:,mo_location[1]:mo_location[3],mo_location[0]:mo_location[2]].clone()\r\n",
        "            \r\n",
        "            #resize\r\n",
        "            LE_feature_resize = F.interpolate(LE_feature,(LE_Dict_feature.size(2),LE_Dict_feature.size(3)),mode='bilinear',align_corners=False)\r\n",
        "            RE_feature_resize = F.interpolate(RE_feature,(RE_Dict_feature.size(2),RE_Dict_feature.size(3)),mode='bilinear',align_corners=False)\r\n",
        "            NO_feature_resize = F.interpolate(NO_feature,(NO_Dict_feature.size(2),NO_Dict_feature.size(3)),mode='bilinear',align_corners=False)\r\n",
        "            MO_feature_resize = F.interpolate(MO_feature,(MO_Dict_feature.size(2),MO_Dict_feature.size(3)),mode='bilinear',align_corners=False)\r\n",
        "            \r\n",
        "            #print(\"LE_feature_resize.shape\")\r\n",
        "            #print(LE_feature_resize.shape)\r\n",
        "            \r\n",
        "            #print(\"f_size\")\r\n",
        "            #print(f_size)\r\n",
        "            \"\"\"\r\n",
        "            if f_size == 256:\r\n",
        "              if self.count_256 == 0:\r\n",
        "                #print(\"LE_save_256 = cur_feature\")\r\n",
        "                self.LE_save_256 = LE_feature_resize\r\n",
        "                self.RE_save_256 = RE_feature_resize\r\n",
        "                self.NO_save_256 = NO_feature_resize\r\n",
        "                self.MO_save_256 = MO_feature_resize\r\n",
        "                self.count_256 += 1\r\n",
        "              else:\r\n",
        "                #print(\"torch.cat((LE_save_256, LE_feature_resize), 1)\")\r\n",
        "                self.LE_save_256 = torch.cat((self.LE_save_256, LE_feature_resize), 0)\r\n",
        "                self.RE_save_256 = torch.cat((self.RE_save_256, RE_feature_resize), 0)\r\n",
        "                self.NO_save_256 = torch.cat((self.NO_save_256, NO_feature_resize), 0)\r\n",
        "                self.MO_save_256 = torch.cat((self.MO_save_256, MO_feature_resize), 0)\r\n",
        "                self.count_256 += 1\r\n",
        "\r\n",
        "              if self.count_256 == 20:\r\n",
        "                torch.save(self.LE_save_256, 'LE_feature_resize_256.pt')\r\n",
        "                torch.save(self.RE_save_256, 'RE_feature_resize_256.pt')\r\n",
        "                torch.save(self.NO_save_256, 'NO_feature_resize_256.pt')\r\n",
        "                torch.save(self.MO_save_256, 'MO_feature_resize_256.pt')\r\n",
        "              \r\n",
        "            #############################################\r\n",
        "            if f_size == 128:\r\n",
        "              if self.count_128 == 0:\r\n",
        "                self.LE_save_128 = LE_feature_resize\r\n",
        "                self.RE_save_128 = RE_feature_resize\r\n",
        "                self.NO_save_128 = NO_feature_resize\r\n",
        "                self.MO_save_128 = MO_feature_resize\r\n",
        "                self.count_128 += 1\r\n",
        "              else:\r\n",
        "                self.LE_save_128 = torch.cat((self.LE_save_128, LE_feature_resize), 0)\r\n",
        "                self.RE_save_128 = torch.cat((self.RE_save_128, RE_feature_resize), 0)\r\n",
        "                self.NO_save_128 = torch.cat((self.NO_save_128, NO_feature_resize), 0)\r\n",
        "                self.MO_save_128 = torch.cat((self.MO_save_128, MO_feature_resize), 0)\r\n",
        "                self.count_256 += 1\r\n",
        "\r\n",
        "              if self.count_128 == 20:\r\n",
        "                torch.save(self.LE_save_128, 'LE_feature_resize_128.pt')\r\n",
        "                torch.save(self.RE_save_128, 'RE_feature_resize_128.pt')\r\n",
        "                torch.save(self.NO_save_128, 'NO_feature_resize_128.pt')\r\n",
        "                torch.save(self.MO_save_128, 'MO_feature_resize_128.pt')\r\n",
        "            #############################################\r\n",
        "            if f_size == 64:\r\n",
        "              if self.count_64 == 0:\r\n",
        "                self.LE_save_64 = LE_feature_resize\r\n",
        "                self.RE_save_64 = RE_feature_resize\r\n",
        "                self.NO_save_64 = NO_feature_resize\r\n",
        "                self.MO_save_64 = MO_feature_resize\r\n",
        "                self.count_64 += 1\r\n",
        "              else:\r\n",
        "                self.LE_save_64 = torch.cat((self.LE_save_64, LE_feature_resize), 0)\r\n",
        "                self.RE_save_64 = torch.cat((self.RE_save_64, RE_feature_resize), 0)\r\n",
        "                self.NO_save_64 = torch.cat((self.NO_save_64, NO_feature_resize), 0)\r\n",
        "                self.MO_save_64 = torch.cat((self.MO_save_64, MO_feature_resize), 0)\r\n",
        "                self.count_256 += 1\r\n",
        "\r\n",
        "              if self.count_64 == 20:\r\n",
        "                torch.save(self.LE_save_64, 'LE_feature_resize_64.pt')\r\n",
        "                torch.save(self.RE_save_64, 'RE_feature_resize_64.pt')\r\n",
        "                torch.save(self.NO_save_64, 'NO_feature_resize_64.pt')\r\n",
        "                torch.save(self.MO_save_64, 'MO_feature_resize_64.pt')\r\n",
        "\r\n",
        "            #############################################\r\n",
        "            if f_size == 32:\r\n",
        "              if self.count_32 == 0:\r\n",
        "                self.LE_save_32 = LE_feature_resize\r\n",
        "                self.RE_save_32 = RE_feature_resize\r\n",
        "                self.NO_save_32 = NO_feature_resize\r\n",
        "                self.MO_save_32 = MO_feature_resize\r\n",
        "                self.count_32 += 1\r\n",
        "              else:\r\n",
        "                self.LE_save_32 = torch.cat((self.LE_save_32, LE_feature_resize), 0)\r\n",
        "                self.RE_save_32 = torch.cat((self.RE_save_32, RE_feature_resize), 0)\r\n",
        "                self.NO_save_32 = torch.cat((self.NO_save_32, NO_feature_resize), 0)\r\n",
        "                self.MO_save_32 = torch.cat((self.MO_save_32, MO_feature_resize), 0)\r\n",
        "                self.count_256 += 1\r\n",
        "\r\n",
        "              if self.count_32 == 20:\r\n",
        "                torch.save(self.LE_save_32, 'LE_feature_resize_32.pt')\r\n",
        "                torch.save(self.RE_save_32, 'RE_feature_resize_32.pt')\r\n",
        "                torch.save(self.NO_save_32, 'NO_feature_resize_32.pt')\r\n",
        "                torch.save(self.MO_save_32, 'MO_feature_resize_32.pt')\r\n",
        "\r\n",
        "            if self.count_256 == 20:\r\n",
        "              print(\"features generated\")\r\n",
        "            \"\"\"\r\n",
        "            LE_Dict_feature_norm = adaptive_instance_normalization_4D(LE_Dict_feature, LE_feature_resize)\r\n",
        "            RE_Dict_feature_norm = adaptive_instance_normalization_4D(RE_Dict_feature, RE_feature_resize)\r\n",
        "            NO_Dict_feature_norm = adaptive_instance_normalization_4D(NO_Dict_feature, NO_feature_resize)\r\n",
        "            MO_Dict_feature_norm = adaptive_instance_normalization_4D(MO_Dict_feature, MO_feature_resize)\r\n",
        "            \r\n",
        "            LE_score = F.conv2d(LE_feature_resize, LE_Dict_feature_norm)\r\n",
        "\r\n",
        "            LE_score = F.softmax(LE_score.view(-1),dim=0)\r\n",
        "            LE_index = torch.argmax(LE_score)\r\n",
        "            LE_Swap_feature = F.interpolate(LE_Dict_feature_norm[LE_index:LE_index+1], (LE_feature.size(2), LE_feature.size(3)))\r\n",
        "\r\n",
        "            LE_Attention = getattr(self, 'le_'+str(f_size))(LE_Swap_feature-LE_feature)\r\n",
        "            LE_Att_feature = LE_Attention * LE_Swap_feature\r\n",
        "            \r\n",
        "\r\n",
        "            RE_score = F.conv2d(RE_feature_resize, RE_Dict_feature_norm)\r\n",
        "            RE_score = F.softmax(RE_score.view(-1),dim=0)\r\n",
        "            RE_index = torch.argmax(RE_score)\r\n",
        "            RE_Swap_feature = F.interpolate(RE_Dict_feature_norm[RE_index:RE_index+1], (RE_feature.size(2), RE_feature.size(3)))\r\n",
        "            \r\n",
        "            RE_Attention = getattr(self, 're_'+str(f_size))(RE_Swap_feature-RE_feature)\r\n",
        "            RE_Att_feature = RE_Attention * RE_Swap_feature\r\n",
        "\r\n",
        "            NO_score = F.conv2d(NO_feature_resize, NO_Dict_feature_norm)\r\n",
        "            NO_score = F.softmax(NO_score.view(-1),dim=0)\r\n",
        "            NO_index = torch.argmax(NO_score)\r\n",
        "            NO_Swap_feature = F.interpolate(NO_Dict_feature_norm[NO_index:NO_index+1], (NO_feature.size(2), NO_feature.size(3)))\r\n",
        "            \r\n",
        "            NO_Attention = getattr(self, 'no_'+str(f_size))(NO_Swap_feature-NO_feature)\r\n",
        "            NO_Att_feature = NO_Attention * NO_Swap_feature\r\n",
        "\r\n",
        "            \r\n",
        "            MO_score = F.conv2d(MO_feature_resize, MO_Dict_feature_norm)\r\n",
        "            MO_score = F.softmax(MO_score.view(-1),dim=0)\r\n",
        "            MO_index = torch.argmax(MO_score)\r\n",
        "            MO_Swap_feature = F.interpolate(MO_Dict_feature_norm[MO_index:MO_index+1], (MO_feature.size(2), MO_feature.size(3)))\r\n",
        "            \r\n",
        "            MO_Attention = getattr(self, 'mo_'+str(f_size))(MO_Swap_feature-MO_feature)\r\n",
        "            MO_Att_feature = MO_Attention * MO_Swap_feature\r\n",
        "\r\n",
        "            update_feature[:,:,le_location[1]:le_location[3],le_location[0]:le_location[2]] = LE_Att_feature + LE_feature\r\n",
        "            update_feature[:,:,re_location[1]:re_location[3],re_location[0]:re_location[2]] = RE_Att_feature + RE_feature\r\n",
        "            update_feature[:,:,no_location[1]:no_location[3],no_location[0]:no_location[2]] = NO_Att_feature + NO_feature\r\n",
        "            update_feature[:,:,mo_location[1]:mo_location[3],mo_location[0]:mo_location[2]] = MO_Att_feature + MO_feature\r\n",
        "\r\n",
        "            UpdateVggFeatures.append(update_feature) \r\n",
        "        \r\n",
        "        fea_vgg = self.MSDilate(VggFeatures[3])\r\n",
        "        #new version\r\n",
        "        fea_up0 = self.up0(fea_vgg, UpdateVggFeatures[3])\r\n",
        "        # out1 = F.interpolate(fea_up0,(512,512))\r\n",
        "        # out1 = self.to_rgb0(out1)\r\n",
        "\r\n",
        "        fea_up1 = self.up1( fea_up0, UpdateVggFeatures[2]) #\r\n",
        "        # out2 = F.interpolate(fea_up1,(512,512))\r\n",
        "        # out2 = self.to_rgb1(out2)\r\n",
        "\r\n",
        "        fea_up2 = self.up2(fea_up1, UpdateVggFeatures[1]) #\r\n",
        "        # out3 = F.interpolate(fea_up2,(512,512))\r\n",
        "        # out3 = self.to_rgb2(out3)\r\n",
        "\r\n",
        "        fea_up3 = self.up3(fea_up2, UpdateVggFeatures[0]) #\r\n",
        "        # out4 = F.interpolate(fea_up3,(512,512))\r\n",
        "        # out4 = self.to_rgb3(out4)\r\n",
        "\r\n",
        "        output = self.up4(fea_up3) #\r\n",
        "        \r\n",
        "    \r\n",
        "        return output  #+ out4 + out3 + out2 + out1\r\n",
        "        #0 128 * 256 * 256\r\n",
        "        #1 256 * 128 * 128\r\n",
        "        #2 512 * 64 * 64\r\n",
        "        #3 512 * 32 * 32\r\n",
        "\r\n",
        "\r\n",
        "class UpResBlock(nn.Module):\r\n",
        "    def __init__(self, dim, conv_layer = nn.Conv2d, norm_layer = nn.BatchNorm2d):\r\n",
        "        super(UpResBlock, self).__init__()\r\n",
        "        self.Model = nn.Sequential(\r\n",
        "            # SpectralNorm(conv_layer(dim, dim, 3, 1, 1)),\r\n",
        "            conv_layer(dim, dim, 3, 1, 1),\r\n",
        "            # norm_layer(dim),\r\n",
        "            nn.LeakyReLU(0.2,True),\r\n",
        "            # SpectralNorm(conv_layer(dim, dim, 3, 1, 1)),\r\n",
        "            conv_layer(dim, dim, 3, 1, 1),\r\n",
        "        )\r\n",
        "    def forward(self, x):\r\n",
        "        out = x + self.Model(x)\r\n",
        "        return out\r\n",
        "\r\n",
        "class VggClassNet(nn.Module):\r\n",
        "    def __init__(self, select_layer = ['0','5','10','19']):\r\n",
        "        super(VggClassNet, self).__init__()\r\n",
        "        self.select = select_layer\r\n",
        "        self.vgg = models.vgg19(pretrained=True).features\r\n",
        "        for param in self.parameters():\r\n",
        "            param.requires_grad = False\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        features = []\r\n",
        "        for name, layer in self.vgg._modules.items():\r\n",
        "            x = layer(x)\r\n",
        "            if name in self.select:\r\n",
        "                features.append(x)\r\n",
        "        return features\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    print('this is network')\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "c1N3DOW_RH3Q"
      },
      "source": [
        "#@title custom_dataset.py (adding augmentations back)\n",
        "%%writefile /content/DFDNet/data/custom_dataset.py\n",
        "# -- coding: utf-8 --\n",
        "import os.path\n",
        "import os\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from PIL import Image, ImageFilter\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "from scipy.io import loadmat\n",
        "from PIL import Image\n",
        "import PIL\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import glob\n",
        "\n",
        "class AlignedDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, root_dir, fine_size=512, transform=None):\n",
        "        self.root_dir = '/content/DFDNet/ffhq'\n",
        "        #self.pathes = [os.path.join(self.root_dir, x) for x in os.listdir(self.root_dir) if x[-3:] == 'jpg']\n",
        "        \n",
        "        self.pathes = glob.glob(self.root_dir + '/**/*.png', recursive=True)\n",
        "\n",
        "        #print(\"self.pathes\")\n",
        "        #print(self.pathes)\n",
        "        self.transform = transform\n",
        "        self.fine_size = fine_size\n",
        "        self.partpath = '/content/DFDNet/landmarks'\n",
        "        \n",
        "    def AddNoise(self,img): # noise\n",
        "        if random.random() > 0.9: #\n",
        "            return img\n",
        "        self.sigma = np.random.randint(1, 11)\n",
        "        img_tensor = torch.from_numpy(np.array(img)).float()\n",
        "        noise = torch.randn(img_tensor.size()).mul_(self.sigma/1.0)\n",
        "\n",
        "        noiseimg = torch.clamp(noise+img_tensor,0,255)\n",
        "        return Image.fromarray(np.uint8(noiseimg.numpy()))\n",
        "\n",
        "    def AddBlur(self,img): # gaussian blur or motion blur\n",
        "        if random.random() > 0.9: #\n",
        "            return img\n",
        "        img = np.array(img)\n",
        "        if random.random() > 0.35: ##gaussian blur\n",
        "            blursize = random.randint(1,17) * 2 + 1 ##3,5,7,9,11,13,15\n",
        "            blursigma = random.randint(3, 20)\n",
        "            img = cv2.GaussianBlur(img, (blursize,blursize), blursigma/10)\n",
        "        else: #motion blur\n",
        "            M = random.randint(1,32)\n",
        "            KName = './data/MotionBlurKernel/m_%02d.mat' % M\n",
        "            k = loadmat(KName)['kernel']\n",
        "            k = k.astype(np.float32)\n",
        "            k /= np.sum(k)\n",
        "            img = cv2.filter2D(img,-1,k)\n",
        "        return Image.fromarray(img)\n",
        "\n",
        "    def AddDownSample(self,img): # downsampling\n",
        "        if random.random() > 0.95: #\n",
        "            return img\n",
        "        sampler = random.randint(20, 100)*1.0\n",
        "        img = img.resize((int(self.fine_size/sampler*10.0), int(self.fine_size/sampler*10.0)), Image.BICUBIC)\n",
        "        return img\n",
        "\n",
        "    def AddJPEG(self,img): # JPEG compression\n",
        "        if random.random() > 0.6:\n",
        "            return img\n",
        "        imQ = random.randint(40, 80)\n",
        "        img = np.array(img)\n",
        "        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY),imQ] # (0,100),higher is better,default is 95\n",
        "        _, encA = cv2.imencode('.jpg', img, encode_param)\n",
        "        img = cv2.imdecode(encA,1)\n",
        "        return Image.fromarray(img)\n",
        "\n",
        "    def AddUpSample(self,img):\n",
        "        return img.resize((self.fine_size, self.fine_size), Image.BICUBIC)\n",
        "\n",
        "    def __getitem__(self, index): # indexation\n",
        "\n",
        "        path = self.pathes[index]\n",
        "        Imgs = Image.open(path).convert('RGB')\n",
        "        \n",
        "        A = Imgs.resize((self.fine_size, self.fine_size))\n",
        "        A = transforms.ColorJitter(0.3, 0.3, 0.3, 0)(A)\n",
        "        C = A\n",
        "        A = self.AddBlur(A)\n",
        "        \n",
        "        tmps = path.split('/')\n",
        "        ImgName = tmps[-1]\n",
        "        part_locations = self.get_part_location(self.partpath, ImgName, 2)\n",
        "        \n",
        "        A = transforms.ToTensor()(A)\n",
        "        C = transforms.ToTensor()(C)\n",
        "        \n",
        "        A = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(A) \n",
        "        C = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(C)\n",
        "        \n",
        "        return {'A': A, 'C': C, 'path': path, 'part_locations': part_locations}\n",
        "\n",
        "    def get_part_location(self, landmarkpath, imgname, downscale=1):\n",
        "        Landmarks = []\n",
        "        with open(os.path.join(landmarkpath, imgname + '.txt'),'r') as f:\n",
        "            for line in f:\n",
        "                tmp = [np.float(i) for i in line.split(' ') if i != '\\n']\n",
        "                Landmarks.append(tmp)\n",
        "        Landmarks = np.array(Landmarks)/downscale # 512 * 512\n",
        "        \n",
        "        Map_LE = list(np.hstack((range(17,22), range(36,42))))\n",
        "        Map_RE = list(np.hstack((range(22,27), range(42,48))))\n",
        "        Map_NO = list(range(29,36))\n",
        "        Map_MO = list(range(48,68))\n",
        "        #left eye\n",
        "        Mean_LE = np.mean(Landmarks[Map_LE],0)\n",
        "        L_LE = np.max((np.max(np.max(Landmarks[Map_LE],0) - np.min(Landmarks[Map_LE],0))/2,16))\n",
        "        Location_LE = np.hstack((Mean_LE - L_LE + 1, Mean_LE + L_LE)).astype(int)\n",
        "        #right eye\n",
        "        Mean_RE = np.mean(Landmarks[Map_RE],0)\n",
        "        L_RE = np.max((np.max(np.max(Landmarks[Map_RE],0) - np.min(Landmarks[Map_RE],0))/2,16))\n",
        "        Location_RE = np.hstack((Mean_RE - L_RE + 1, Mean_RE + L_RE)).astype(int)\n",
        "        #nose\n",
        "        Mean_NO = np.mean(Landmarks[Map_NO],0)\n",
        "        L_NO = np.max((np.max(np.max(Landmarks[Map_NO],0) - np.min(Landmarks[Map_NO],0))/2,16))\n",
        "        Location_NO = np.hstack((Mean_NO - L_NO + 1, Mean_NO + L_NO)).astype(int)\n",
        "        #mouth\n",
        "        Mean_MO = np.mean(Landmarks[Map_MO],0)\n",
        "        L_MO = np.max((np.max(np.max(Landmarks[Map_MO],0) - np.min(Landmarks[Map_MO],0))/2,16))\n",
        "\n",
        "        Location_MO = np.hstack((Mean_MO - L_MO + 1, Mean_MO + L_MO)).astype(int)\n",
        "        return Location_LE, Location_RE, Location_NO, Location_MO\n",
        "\n",
        "    def __len__(self): #\n",
        "        return len(self.pathes)\n",
        "\n",
        "    def name(self):\n",
        "        return 'AlignedDataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-kQPkSHpFL6"
      },
      "source": [
        "# training with extracted features\r\n",
        "%cd /content/DFDNet\r\n",
        "!python run.py --batchSize 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkMgfyCiUFCg"
      },
      "source": [
        "Now you can use the above code from ``Testing``, but **don't** change ``networks.py`` and avoid using the original version of that file. Stick to ``networks.py (train with own features, replacing numpy code)``. You need to load the custom features with the modified code."
      ]
    }
  ]
}
