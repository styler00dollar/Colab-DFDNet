{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-DFDNet-lightning-train.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh4lKuAB9Fu0"
      },
      "source": [
        "# Colab-DFDNet-lightning\n",
        "\n",
        "Official repo: [csxmli2016/DFDNet](https://github.com/csxmli2016/DFDNet)\n",
        "\n",
        "Original repo: [max-vasyuk/DFDNet](https://github.com/max-vasyuk/DFDNet)\n",
        "\n",
        "My fork: [styler00dollar/Colab-DFDNet](https://github.com/styler00dollar/Colab-DFDNet)\n",
        "\n",
        "Porting everything to pytorch lightning. Warning: Currently only with MSE loss!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY6ZIFNg8-98"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5m5HGOQ9ESX",
        "cellView": "form"
      },
      "source": [
        "#@title GPU\n",
        "!pip install pytorch-lightning -U\n",
        "!git clone https://github.com/max-vasyuk/DFDNet\n",
        "%cd /content/DFDNet\n",
        "!pip install -r requirements.txt\n",
        "!pip install pytorch-msssim\n",
        "!pip install trains\n",
        "!pip install PyJWT==1.7.1\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6eLIneSv9jXR"
      },
      "source": [
        "#@title TPU  (restart runtime afterwards)\n",
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n",
        "#!pip install pytorch-lightning\n",
        "!pip install lightning-flash\n",
        "\n",
        "import collections\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\n",
        "VERSION = \"xrt==1.15.0\"  #@param [\"xrt==1.15.0\", \"torch_xla==nightly\"]\n",
        "CONFIG = {\n",
        "    'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),\n",
        "    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n",
        "        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n",
        "}[VERSION]\n",
        "DIST_BUCKET = 'gs://tpu-pytorch/wheels'\n",
        "TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "\n",
        "# Update TPU XRT version\n",
        "def update_server_xrt():\n",
        "  print('Updating server-side XRT to {} ...'.format(CONFIG.server))\n",
        "  url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(\n",
        "      TPU_ADDRESS=os.environ['COLAB_TPU_ADDR'].split(':')[0],\n",
        "      XRT_VERSION=CONFIG.server,\n",
        "  )\n",
        "  print('Done updating server-side XRT: {}'.format(requests.post(url)))\n",
        "\n",
        "update = threading.Thread(target=update_server_xrt)\n",
        "update.start()\n",
        "\n",
        "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
        "!pip uninstall -y torch torchvision\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
        "!pip install \"$TORCH_WHEEL\"\n",
        "!pip install \"$TORCH_XLA_WHEEL\"\n",
        "!pip install \"$TORCHVISION_WHEEL\"\n",
        "!sudo apt-get install libomp5\n",
        "update.join()\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "\n",
        "\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev > /dev/null\n",
        "!pip install pytorch-lightning > /dev/null\n",
        "\n",
        "\n",
        "!git clone https://github.com/max-vasyuk/DFDNet\n",
        "%cd /content/DFDNet\n",
        "!pip install -r requirements.txt\n",
        "!pip install pytorch-msssim\n",
        "!pip install trains\n",
        "!pip install PyJWT==1.7.1\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcqfHteR9dBs",
        "cellView": "form"
      },
      "source": [
        "#@title download data\n",
        "!mkdir /content/DFDNet/DictionaryCenter512\n",
        "%cd /content/DFDNet/DictionaryCenter512\n",
        "!gdown --id 1sEB9j3s7Wj9aqPai1NF-MR7B-c0zfTin\n",
        "!gdown --id 1H4kByBiVmZuS9TbrWUR5uSNY770Goid6\n",
        "!gdown --id 10ctK3d9znZ9nGN3d1Z77xW3GGshbeKBb\n",
        "!gdown --id 1gcwmrIZjPFVu-cHjdQD6P4luohkPsil-\n",
        "!gdown --id 1rJ8cORPxbJsIVAiNrjBag0ihaY_Mvurn\n",
        "!gdown --id 1LkfJv2a3ud-mefAc1eZMJuINuNdSYgYO\n",
        "!gdown --id 1LH-nxD__icSJvTiAbXAXDch03oDtbpkZ\n",
        "!gdown --id 1JRTStLFsQ8dwaQjQ8qG5fNyrOvo6Tcvd\n",
        "!gdown --id 1Z4AkU1pOYTYpdbfljCgNMmPilhdEd0Kl\n",
        "!gdown --id 1Z4e1ltB3ACbYKzkoMBuVtzZ7a310G4xc\n",
        "!gdown --id 1fqWmi6-8ZQzUtZTp9UH4hyom7n4nl8aZ\n",
        "!gdown --id 1wfHtsExLvSgfH_EWtCPjTF5xsw3YyvjC\n",
        "!gdown --id 1Jr3Luf6tmcdKANcSLzvt0sjXr0QUIQ2g\n",
        "!gdown --id 1sPd4_IMYgqGLol0gqhHjBedKKxFAxswR\n",
        "!gdown --id 1eVFjXJRnBH4mx7ZbAmZRwVXZNUbgCQec\n",
        "!gdown --id 1w0GfO_KY775ZVF3KMk74ya6QL_bNU4cJ\n",
        "%cd /content/DFDNet\n",
        "!gdown --id 1VE5tnOKcfL6MoV839IVCCw5FhJxIgml5\n",
        "!7z x data.zip\n",
        "!mkdir /content/DFDNet/weights/\n",
        "%cd /content/DFDNet/weights/\n",
        "!gdown --id 1SfKKZJduOGhDD27Xl01yDx0-YSEkL2Aa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KewGGnkevRP"
      },
      "source": [
        "Preconfigured paths:\n",
        "```\n",
        "/content/DFDNet/DictionaryCenter512 (numpy dictionary files)\n",
        "/content/DFDNet/ffhq (dataset path)\n",
        "/content/DFDNet/landmarks (landmarks for that dataset)\n",
        "/content/DFDNet/weights/vgg19.pth (vgg19 path)\n",
        "\n",
        "/content/validation (validation path)\n",
        "/content/landmarks (landmark path for validation)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5GYCQ5c_lyF",
        "cellView": "form"
      },
      "source": [
        "#@title init.py\n",
        "import torch.nn.init as init\n",
        "\n",
        "def weights_init(net, init_type = 'kaiming', init_gain = 0.02):\n",
        "    #Initialize network weights.\n",
        "    #Parameters:\n",
        "    #    net (network)       -- network to be initialized\n",
        "    #    init_type (str)     -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "    #    init_var (float)    -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "\n",
        "        if hasattr(m, 'weight') and classname.find('Conv') != -1:\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, init_gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain = init_gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain = init_gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('Linear') != -1:\n",
        "            init.normal_(m.weight, 0, 0.01)\n",
        "            init.constant_(m.bias, 0)\n",
        "\n",
        "    # Apply the initialization function <init_func>\n",
        "    print('Initialization method [{:s}]'.format(init_type))\n",
        "    net.apply(init_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqpDwSEWAf0a",
        "cellView": "form"
      },
      "source": [
        "#@title loss.py\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class TVLoss(pl.LightningModule):\n",
        "    def __init__(self,TVLoss_weight=1):\n",
        "        super(TVLoss,self).__init__()\n",
        "        self.TVLoss_weight = TVLoss_weight\n",
        "\n",
        "    def forward(self,x):\n",
        "        batch_size = x.size()[0]\n",
        "        h_x = x.size()[2]\n",
        "        w_x = x.size()[3]\n",
        "        count_h = self._tensor_size(x[:,:,1:,:])\n",
        "        count_w = self._tensor_size(x[:,:,:,1:])\n",
        "        h_tv = torch.pow((x[:,:,1:,:]-x[:,:,:h_x-1,:]),2).sum()\n",
        "        w_tv = torch.pow((x[:,:,:,1:]-x[:,:,:,:w_x-1]),2).sum()\n",
        "        return self.TVLoss_weight*2*(h_tv/count_h+w_tv/count_w)/batch_size\n",
        "\n",
        "    def _tensor_size(self,t):\n",
        "        return t.size()[1]*t.size()[2]*t.size()[3]\n",
        "\n",
        "\n",
        "class hinge_loss(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(hinge_loss, self).__init__()\n",
        "\n",
        "    def forward(self, dis_fake, dis_real):\n",
        "        loss_real = torch.mean(F.relu(1. - dis_real))\n",
        "        loss_fake = torch.mean(F.relu(1. + dis_fake))\n",
        "        return loss_real + loss_fake\n",
        "\n",
        "\n",
        "class hinge_loss_G(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(hinge_loss_G, self).__init__()\n",
        "\n",
        "    def forward(self, dis_fake):\n",
        "        loss_fake = -torch.mean(dis_fake)\n",
        "        return loss_fake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmNzK1bmFgeg",
        "cellView": "form"
      },
      "source": [
        "#@title checkpoint.py\n",
        "#https://github.com/PyTorchLightning/pytorch-lightning/issues/2534\n",
        "import os\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class CheckpointEveryNSteps(pl.Callback):\n",
        "    \"\"\"\n",
        "    Save a checkpoint every N steps, instead of Lightning's default that checkpoints\n",
        "    based on validation loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        save_step_frequency,\n",
        "        prefix=\"Checkpoint\",\n",
        "        use_modelcheckpoint_filename=False,\n",
        "        save_path = '/content/'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            save_step_frequency: how often to save in steps\n",
        "            prefix: add a prefix to the name, only used if\n",
        "                use_modelcheckpoint_filename=False\n",
        "            use_modelcheckpoint_filename: just use the ModelCheckpoint callback's\n",
        "                default filename, don't use ours.\n",
        "        \"\"\"\n",
        "        self.save_step_frequency = save_step_frequency\n",
        "        self.prefix = prefix\n",
        "        self.use_modelcheckpoint_filename = use_modelcheckpoint_filename\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def on_batch_end(self, trainer: pl.Trainer, _):\n",
        "        \"\"\" Check if we should save a checkpoint after every train batch \"\"\"\n",
        "        epoch = trainer.current_epoch\n",
        "        global_step = trainer.global_step\n",
        "        if global_step % self.save_step_frequency == 0:\n",
        "            if self.use_modelcheckpoint_filename:\n",
        "                filename = trainer.checkpoint_callback.filename\n",
        "            else:\n",
        "                filename = f\"{self.prefix}_{epoch}_{global_step}.ckpt\"\n",
        "            #ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
        "            ckpt_path = os.path.join(self.save_path, filename)\n",
        "            trainer.save_checkpoint(ckpt_path)\n",
        "            # run validation once checkpoint was made\n",
        "            trainer.run_evaluation()\n",
        "\n",
        "#Trainer(callbacks=[CheckpointEveryNSteps()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5aCGLgr9m6L"
      },
      "source": [
        "Edit motion blur kernel path inside ``data``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nAoSz0QGAw4",
        "cellView": "form"
      },
      "source": [
        "#@title data.py\n",
        "import os.path\n",
        "import os\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from PIL import Image, ImageFilter\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "from scipy.io import loadmat\n",
        "from PIL import Image\n",
        "import PIL\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class DS(Dataset):\n",
        "    \n",
        "    def __init__(self, root_dir, fine_size=512, transform=None, partpath=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.pathes = [os.path.join(self.root_dir, x) for x in os.listdir(self.root_dir) if x[-3:] == 'png']\n",
        "        self.transform = transform\n",
        "        self.fine_size = fine_size\n",
        "        self.partpath = partpath\n",
        "        \n",
        "    def AddNoise(self,img): # noise\n",
        "        if random.random() > 0.9: #\n",
        "            return img\n",
        "        self.sigma = np.random.randint(1, 11)\n",
        "        img_tensor = torch.from_numpy(np.array(img)).float()\n",
        "        noise = torch.randn(img_tensor.size()).mul_(self.sigma/1.0)\n",
        "\n",
        "        noiseimg = torch.clamp(noise+img_tensor,0,255)\n",
        "        return Image.fromarray(np.uint8(noiseimg.numpy()))\n",
        "\n",
        "    def AddBlur(self,img): # gaussian blur or motion blur\n",
        "        if random.random() > 0.9: #\n",
        "            return img\n",
        "        img = np.array(img)\n",
        "        if random.random() > 0.35: ##gaussian blur\n",
        "            blursize = random.randint(1,17) * 2 + 1 ##3,5,7,9,11,13,15\n",
        "            blursigma = random.randint(3, 20)\n",
        "            img = cv2.GaussianBlur(img, (blursize,blursize), blursigma/10)\n",
        "        else: #motion blur\n",
        "            M = random.randint(1,32)\n",
        "            KName = '/content/DFDNet/data/MotionBlurKernel/m_%02d.mat' % M\n",
        "            k = loadmat(KName)['kernel']\n",
        "            k = k.astype(np.float32)\n",
        "            k /= np.sum(k)\n",
        "            img = cv2.filter2D(img,-1,k)\n",
        "        return Image.fromarray(img)\n",
        "\n",
        "    def AddDownSample(self,img): # downsampling\n",
        "        if random.random() > 0.95: #\n",
        "            return img\n",
        "        sampler = random.randint(20, 100)*1.0\n",
        "        img = img.resize((int(self.fine_size/sampler*10.0), int(self.fine_size/sampler*10.0)), Image.BICUBIC)\n",
        "        return img\n",
        "\n",
        "    def AddJPEG(self,img): # JPEG compression\n",
        "        if random.random() > 0.6:\n",
        "            return img\n",
        "        imQ = random.randint(40, 80)\n",
        "        img = np.array(img)\n",
        "        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY),imQ] # (0,100),higher is better,default is 95\n",
        "        _, encA = cv2.imencode('.jpg', img, encode_param)\n",
        "        img = cv2.imdecode(encA,1)\n",
        "        return Image.fromarray(img)\n",
        "\n",
        "    def AddUpSample(self,img):\n",
        "        return img.resize((self.fine_size, self.fine_size), Image.BICUBIC)\n",
        "\n",
        "    def __getitem__(self, index): # indexation\n",
        "\n",
        "        path = self.pathes[index]\n",
        "        Imgs = Image.open(path).convert('RGB')\n",
        "        \n",
        "        #A = Imgs.resize((self.fine_size, self.fine_size))\n",
        "        if self.transform:\n",
        "          A = self.transform(Imgs)\n",
        "\n",
        "        A = transforms.ColorJitter(0.3, 0.3, 0.3, 0)(A)\n",
        "        C = A\n",
        "        A = self.AddBlur(A)\n",
        "        A = self.AddJPEG(A)\n",
        "        \n",
        "        tmps = path.split('/')\n",
        "        ImgName = tmps[-1]\n",
        "        part_locations = self.get_part_location(self.partpath, ImgName, 2)\n",
        "        \n",
        "        A = transforms.ToTensor()(A)\n",
        "        C = transforms.ToTensor()(C)\n",
        "        \n",
        "        A = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(A) \n",
        "        C = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(C)\n",
        "\n",
        "        #A = transforms.ToTensor()(A)\n",
        "        #C = transforms.ToTensor()(C)\n",
        "\n",
        "        #return {'A': A, 'C': C, 'path': path, 'part_locations': part_locations}\n",
        "        return A, C, path, part_locations\n",
        "\n",
        "    def get_part_location(self, landmarkpath, imgname, downscale=1):\n",
        "        Landmarks = []\n",
        "        with open(os.path.join(landmarkpath, imgname + '.txt'),'r') as f:\n",
        "            for line in f:\n",
        "                tmp = [np.float(i) for i in line.split(' ') if i != '\\n']\n",
        "                Landmarks.append(tmp)\n",
        "        Landmarks = np.array(Landmarks)/downscale # 512 * 512\n",
        "        \n",
        "        Map_LE = list(np.hstack((range(17,22), range(36,42))))\n",
        "        Map_RE = list(np.hstack((range(22,27), range(42,48))))\n",
        "        Map_NO = list(range(29,36))\n",
        "        Map_MO = list(range(48,68))\n",
        "        #left eye\n",
        "        Mean_LE = np.mean(Landmarks[Map_LE],0)\n",
        "        L_LE = np.max((np.max(np.max(Landmarks[Map_LE],0) - np.min(Landmarks[Map_LE],0))/2,16))\n",
        "        Location_LE = np.hstack((Mean_LE - L_LE + 1, Mean_LE + L_LE)).astype(int)\n",
        "        #right eye\n",
        "        Mean_RE = np.mean(Landmarks[Map_RE],0)\n",
        "        L_RE = np.max((np.max(np.max(Landmarks[Map_RE],0) - np.min(Landmarks[Map_RE],0))/2,16))\n",
        "        Location_RE = np.hstack((Mean_RE - L_RE + 1, Mean_RE + L_RE)).astype(int)\n",
        "        #nose\n",
        "        Mean_NO = np.mean(Landmarks[Map_NO],0)\n",
        "        L_NO = np.max((np.max(np.max(Landmarks[Map_NO],0) - np.min(Landmarks[Map_NO],0))/2,16))\n",
        "        Location_NO = np.hstack((Mean_NO - L_NO + 1, Mean_NO + L_NO)).astype(int)\n",
        "        #mouth\n",
        "        Mean_MO = np.mean(Landmarks[Map_MO],0)\n",
        "        L_MO = np.max((np.max(np.max(Landmarks[Map_MO],0) - np.min(Landmarks[Map_MO],0))/2,16))\n",
        "\n",
        "        Location_MO = np.hstack((Mean_MO - L_MO + 1, Mean_MO + L_MO)).astype(int)\n",
        "        return Location_LE, Location_RE, Location_NO, Location_MO\n",
        "\n",
        "    def __len__(self): #\n",
        "        return len(self.pathes)\n",
        "\n",
        "    def name(self):\n",
        "        return 'AlignedDataset'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DS_val(Dataset):\n",
        "    \n",
        "    def __init__(self, root_dir, fine_size=512, transform=None, partpath=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.pathes = [os.path.join(self.root_dir, x) for x in os.listdir(self.root_dir) if x[-3:] == 'png']\n",
        "        self.transform = transform\n",
        "        self.fine_size = fine_size\n",
        "        self.partpath = partpath\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index): # indexation\n",
        "\n",
        "        path = self.pathes[index]\n",
        "        Imgs = Image.open(path).convert('RGB')\n",
        "        \n",
        "        A = Imgs\n",
        "        #A = Imgs.resize((self.fine_size, self.fine_size))\n",
        "        #A = transforms.ColorJitter(0.3, 0.3, 0.3, 0)(A)\n",
        "        #C = A\n",
        "        #A = self.AddBlur(A)\n",
        "        #A = self.AddJPEG(A)\n",
        "        \n",
        "        tmps = path.split('/')\n",
        "        ImgName = tmps[-1]\n",
        "        part_locations = self.get_part_location(self.partpath, ImgName, 2)\n",
        "        \n",
        "        A = transforms.ToTensor()(A)\n",
        "        #C = transforms.ToTensor()(C)\n",
        "        \n",
        "        A = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(A) \n",
        "        #C = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(C)\n",
        "        \n",
        "        #return {'A': A, 'path': path, 'part_locations': part_locations}\n",
        "        return A, path, part_locations\n",
        "\n",
        "    def get_part_location(self, landmarkpath, imgname, downscale=1):\n",
        "        Landmarks = []\n",
        "        with open(os.path.join(landmarkpath, imgname + '.txt'),'r') as f:\n",
        "            for line in f:\n",
        "                tmp = [np.float(i) for i in line.split(' ') if i != '\\n']\n",
        "                Landmarks.append(tmp)\n",
        "        Landmarks = np.array(Landmarks)/downscale # 512 * 512\n",
        "        \n",
        "        Map_LE = list(np.hstack((range(17,22), range(36,42))))\n",
        "        Map_RE = list(np.hstack((range(22,27), range(42,48))))\n",
        "        Map_NO = list(range(29,36))\n",
        "        Map_MO = list(range(48,68))\n",
        "        #left eye\n",
        "        Mean_LE = np.mean(Landmarks[Map_LE],0)\n",
        "        L_LE = np.max((np.max(np.max(Landmarks[Map_LE],0) - np.min(Landmarks[Map_LE],0))/2,16))\n",
        "        Location_LE = np.hstack((Mean_LE - L_LE + 1, Mean_LE + L_LE)).astype(int)\n",
        "        #right eye\n",
        "        Mean_RE = np.mean(Landmarks[Map_RE],0)\n",
        "        L_RE = np.max((np.max(np.max(Landmarks[Map_RE],0) - np.min(Landmarks[Map_RE],0))/2,16))\n",
        "        Location_RE = np.hstack((Mean_RE - L_RE + 1, Mean_RE + L_RE)).astype(int)\n",
        "        #nose\n",
        "        Mean_NO = np.mean(Landmarks[Map_NO],0)\n",
        "        L_NO = np.max((np.max(np.max(Landmarks[Map_NO],0) - np.min(Landmarks[Map_NO],0))/2,16))\n",
        "        Location_NO = np.hstack((Mean_NO - L_NO + 1, Mean_NO + L_NO)).astype(int)\n",
        "        #mouth\n",
        "        Mean_MO = np.mean(Landmarks[Map_MO],0)\n",
        "        L_MO = np.max((np.max(np.max(Landmarks[Map_MO],0) - np.min(Landmarks[Map_MO],0))/2,16))\n",
        "\n",
        "        Location_MO = np.hstack((Mean_MO - L_MO + 1, Mean_MO + L_MO)).astype(int)\n",
        "        return Location_LE, Location_RE, Location_NO, Location_MO\n",
        "\n",
        "    def __len__(self): #\n",
        "        return len(self.pathes)\n",
        "\n",
        "    def name(self):\n",
        "        return 'ValDataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu7rwTczGGkt",
        "cellView": "form"
      },
      "source": [
        "#@title dataloader.py\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class DFNetDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, training_path: str = './', train_partpath: str = './', validation_path: str = './', val_partpath: str = './', test_path: str = './', batch_size: int = 5, num_workers: int = 2):\n",
        "        super().__init__()\n",
        "        self.training_dir = training_path\n",
        "        self.validation_dir = validation_path\n",
        "        self.test_dir = test_path\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.size = 512\n",
        "\n",
        "\n",
        "        self.train_partpath = train_partpath\n",
        "        self.val_partpath = val_partpath\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(size=self.size),\n",
        "            transforms.CenterCrop(size=self.size),\n",
        "            #transforms.RandomHorizontalFlip()\n",
        "            #transforms.ToTensor()\n",
        "        ])\n",
        "        \n",
        "        self.DFDNetdataset_train = DS(root_dir=self.training_dir, transform=transform, fine_size=self.size, partpath=self.train_partpath)\n",
        "        self.DFDNetdataset_validation = DS_val(root_dir=self.validation_dir, transform=transform, partpath=self.val_partpath)\n",
        "        self.DFDNetdataset_test = DS_val(root_dir=self.validation_dir, transform=transform, partpath='/content/landmarks')\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.DFDNetdataset_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.DFDNetdataset_validation, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.DFDNetdataset_test, batch_size=self.batch_size, num_workers=self.num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdLqmPn9sw_",
        "cellView": "form"
      },
      "source": [
        "#@title CustomTrainClass.py\n",
        "import pytorch_lightning as pl\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "class CustomTrainClass(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.netG = UNetDictFace(64)\n",
        "\n",
        "    #init_net(self.netG, 'normal', 0.02, gpu_ids, init_flag)\n",
        "    weights_init(self.netG, 'kaiming')\n",
        "\n",
        "\n",
        "\n",
        "    # Establish convention for real and fake labels during training\n",
        "    self.real_label = 1.\n",
        "    self.fake_label = 0.\n",
        "\n",
        "    self.criterionG = torch.nn.MSELoss()\n",
        "    #criterionD = torch.nn.BCELoss()\n",
        "\n",
        "    #self.hinge_G = hinge_loss_G()\n",
        "\n",
        "\n",
        "  def forward(self, ):\n",
        "    return self.netG()\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "\n",
        "      #return {'A': A, 'C': C, 'path': path, 'part_locations': part_locations}\n",
        "      # train_batch[0] = A\n",
        "      # train_batch[1] = C\n",
        "      # train_batch[3] = part_locations\n",
        "\n",
        "      data_a = train_batch[0] #.to(device)\n",
        "      data_c = train_batch[1] #.to(device)\n",
        "\n",
        "      data_part_locations = train_batch[2]\n",
        "\n",
        "      ####################################\n",
        "      # generator training\n",
        "      fake = self.netG(data_a, part_locations=train_batch[3])\n",
        "      mse_loss = self.criterionG(fake, data_c)\n",
        "\n",
        "      return mse_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.netG.parameters(), lr=2e-4, betas=(0.5, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "    return optimizer\n",
        "\n",
        "  def validation_step(self, train_batch, train_idx):\n",
        "    # return {'A': A, 'path': path, 'part_locations': part_locations}\n",
        "    # train_batch[0] = lr\n",
        "    # train_batch[1] = path\n",
        "    # train_batch[2] = part_locations\n",
        "    out = self.netG(train_batch[0], part_locations=train_batch[2])\n",
        "\n",
        "    validation_output = '/content/validation_output/'\n",
        "\n",
        "    # train_batch[3] can contain multiple files, depending on the batch_size\n",
        "    for f in train_batch[1]:\n",
        "      # data is processed as a batch, to save indididual files, a counter is used\n",
        "      counter = 0\n",
        "      if not os.path.exists(os.path.join(validation_output, os.path.splitext(os.path.basename(f))[0])):\n",
        "        os.makedirs(os.path.join(validation_output, os.path.splitext(os.path.basename(f))[0]))\n",
        "\n",
        "      filename_with_extention = os.path.basename(f)\n",
        "      filename = os.path.splitext(filename_with_extention)[0]\n",
        "      save_image(out[counter], os.path.join(validation_output, filename, str(self.trainer.global_step) + '.png'))\n",
        "\n",
        "\n",
        "  def test_step(self, train_batch, train_idx):\n",
        "    # train_batch[0] = masked\n",
        "    # train_batch[1] = mask\n",
        "    # train_batch[2] = path\n",
        "    print(\"testing\")\n",
        "    test_output = '/content/test_output/'\n",
        "    if not os.path.exists(test_output):\n",
        "      os.makedirs(test_output)\n",
        "\n",
        "    out = self(train_batch[0].unsqueeze(0),train_batch[1].unsqueeze(0))\n",
        "    out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    save_image(out, os.path.join(test_output, os.path.splitext(os.path.basename(train_batch[2]))[0] + '.png'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x9wN88gDykW"
      },
      "source": [
        "Edit ``vgg path`` and ``dictionary_path`` inside ``model``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0en1qQQM-4XU",
        "cellView": "form"
      },
      "source": [
        "#@title model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import functools\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter as P\n",
        "#from util import util\n",
        "from torchvision import models\n",
        "#import scipy.io as sio\n",
        "import numpy as np\n",
        "#import scipy.ndimage\n",
        "import torch.nn.utils.spectral_norm as SpectralNorm\n",
        "\n",
        "from torch.autograd import Function\n",
        "from math import sqrt\n",
        "import random\n",
        "import os\n",
        "import math\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "class VGGFeat(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Input: (B, C, H, W), RGB, [-1, 1]\n",
        "    \"\"\"\n",
        "    def __init__(self, weight_path='/content/DFDNet/weights/vgg19.pth'):\n",
        "        super().__init__()\n",
        "        self.model = models.vgg19(pretrained=False)\n",
        "        self.build_vgg_layers()\n",
        "        \n",
        "        self.model.load_state_dict(torch.load(weight_path))\n",
        "\n",
        "        self.register_parameter(\"RGB_mean\", nn.Parameter(torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)))\n",
        "        self.register_parameter(\"RGB_std\", nn.Parameter(torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)))\n",
        "        \n",
        "        # self.model.eval()\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def build_vgg_layers(self):\n",
        "        vgg_pretrained_features = self.model.features\n",
        "        self.features = []\n",
        "        # feature_layers = [0, 3, 8, 17, 26, 35]\n",
        "        feature_layers = [0, 8, 17, 26, 35]\n",
        "        for i in range(len(feature_layers)-1): \n",
        "            module_layers = torch.nn.Sequential() \n",
        "            for j in range(feature_layers[i], feature_layers[i+1]):\n",
        "                module_layers.add_module(str(j), vgg_pretrained_features[j])\n",
        "            self.features.append(module_layers)\n",
        "        self.features = torch.nn.ModuleList(self.features)\n",
        "\n",
        "    def preprocess(self, x):\n",
        "        x = (x + 1) / 2\n",
        "        x = (x - self.RGB_mean) / self.RGB_std\n",
        "        if x.shape[3] < 224:\n",
        "            x = torch.nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.preprocess(x)\n",
        "        features = []\n",
        "        for m in self.features:\n",
        "            # print(m)\n",
        "            x = m(x)\n",
        "            features.append(x)\n",
        "        return features \n",
        "\n",
        "\n",
        "class StyledUpBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1,upsample=False):\n",
        "        super().__init__()\n",
        "        if upsample:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                Blur(out_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                Blur(in_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding)\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        self.convup = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                # EqualConv2d(out_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(out_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                # Blur(out_channel),\n",
        "            )\n",
        "        # self.noise1 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain1 = AdaptiveInstanceNorm(out_channel)\n",
        "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        # self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\n",
        "        # self.noise2 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain2 = AdaptiveInstanceNorm(out_channel)\n",
        "        # self.lrelu2 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.ScaleModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1))\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "        self.ShiftModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1)),\n",
        "            nn.Sigmoid(),\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "       \n",
        "    def forward(self, input, style):\n",
        "        out = self.conv1(input)\n",
        "#         out = self.noise1(out, noise)\n",
        "        out = self.lrelu1(out)\n",
        "\n",
        "        Shift1 = self.ShiftModel1(style)\n",
        "        Scale1 = self.ScaleModel1(style)\n",
        "        out = out * Scale1 + Shift1\n",
        "        # out = self.adain1(out, style)\n",
        "        outup = self.convup(out)\n",
        "\n",
        "        return outup\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class StyledUpBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1,upsample=False):\n",
        "        super().__init__()\n",
        "        if upsample:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                Blur(out_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                Blur(in_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding)\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        self.convup = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                # EqualConv2d(out_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(out_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                # Blur(out_channel),\n",
        "            )\n",
        "        # self.noise1 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain1 = AdaptiveInstanceNorm(out_channel)\n",
        "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        # self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\n",
        "        # self.noise2 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain2 = AdaptiveInstanceNorm(out_channel)\n",
        "        # self.lrelu2 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.ScaleModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1))\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "        self.ShiftModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1)),\n",
        "            nn.Sigmoid(),\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "       \n",
        "    def forward(self, input, style):\n",
        "        out = self.conv1(input)\n",
        "#         out = self.noise1(out, noise)\n",
        "        out = self.lrelu1(out)\n",
        "\n",
        "        Shift1 = self.ShiftModel1(style)\n",
        "        Scale1 = self.ScaleModel1(style)\n",
        "        out = out * Scale1 + Shift1\n",
        "        # out = self.adain1(out, style)\n",
        "        outup = self.convup(out)\n",
        "\n",
        "        return outup\n",
        "\n",
        "class UNetDictFace(pl.LightningModule):\n",
        "    def __init__(self, ngf=64, dictionary_path='/content/DFDNet/DictionaryCenter512'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.part_sizes = np.array([80,80,50,110]) # size for 512\n",
        "        self.feature_sizes = np.array([256,128,64,32])\n",
        "        self.channel_sizes = np.array([128,256,512,512])\n",
        "        Parts = ['left_eye','right_eye','nose','mouth']\n",
        "        self.Dict_256 = {}\n",
        "        self.Dict_128 = {}\n",
        "        self.Dict_64 = {}\n",
        "        self.Dict_32 = {}\n",
        "        for j,i in enumerate(Parts):\n",
        "            f_256 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_256_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_256_reshape = f_256.reshape(f_256.size(0),self.channel_sizes[0],self.part_sizes[j]//2,self.part_sizes[j]//2)\n",
        "            max_256 = torch.max(torch.sqrt(compute_sum(torch.pow(f_256_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_256[i] = f_256_reshape #/ max_256\n",
        "            \n",
        "            f_128 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_128_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_128_reshape = f_128.reshape(f_128.size(0),self.channel_sizes[1],self.part_sizes[j]//4,self.part_sizes[j]//4)\n",
        "            max_128 = torch.max(torch.sqrt(compute_sum(torch.pow(f_128_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_128[i] = f_128_reshape #/ max_128\n",
        "\n",
        "            f_64 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_64_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_64_reshape = f_64.reshape(f_64.size(0),self.channel_sizes[2],self.part_sizes[j]//8,self.part_sizes[j]//8)\n",
        "            max_64 = torch.max(torch.sqrt(compute_sum(torch.pow(f_64_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_64[i] = f_64_reshape #/ max_64\n",
        "\n",
        "            f_32 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_32_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_32_reshape = f_32.reshape(f_32.size(0),self.channel_sizes[3],self.part_sizes[j]//16,self.part_sizes[j]//16)\n",
        "            max_32 = torch.max(torch.sqrt(compute_sum(torch.pow(f_32_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_32[i] = f_32_reshape #/ max_32\n",
        "\n",
        "        self.le_256 = AttentionBlock(128)\n",
        "        self.le_128 = AttentionBlock(256)\n",
        "        self.le_64 = AttentionBlock(512)\n",
        "        self.le_32 = AttentionBlock(512)\n",
        "\n",
        "        self.re_256 = AttentionBlock(128)\n",
        "        self.re_128 = AttentionBlock(256)\n",
        "        self.re_64 = AttentionBlock(512)\n",
        "        self.re_32 = AttentionBlock(512)\n",
        "\n",
        "        self.no_256 = AttentionBlock(128)\n",
        "        self.no_128 = AttentionBlock(256)\n",
        "        self.no_64 = AttentionBlock(512)\n",
        "        self.no_32 = AttentionBlock(512)\n",
        "\n",
        "        self.mo_256 = AttentionBlock(128)\n",
        "        self.mo_128 = AttentionBlock(256)\n",
        "        self.mo_64 = AttentionBlock(512)\n",
        "        self.mo_32 = AttentionBlock(512)\n",
        "\n",
        "        #norm\n",
        "        self.VggExtract = VGGFeat()\n",
        "        \n",
        "        ######################\n",
        "        self.MSDilate = MSDilateBlock(ngf*8, dilation = [4,3,2,1])  #\n",
        "\n",
        "        self.up0 = StyledUpBlock(ngf*8,ngf*8)\n",
        "        self.up1 = StyledUpBlock(ngf*8, ngf*4) #\n",
        "        self.up2 = StyledUpBlock(ngf*4, ngf*2) #\n",
        "        self.up3 = StyledUpBlock(ngf*2, ngf) #\n",
        "        self.up4 = nn.Sequential( # 128\n",
        "            # nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            SpectralNorm(nn.Conv2d(ngf, ngf, 3, 1, 1)),\n",
        "            # nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            UpResBlock(ngf),\n",
        "            UpResBlock(ngf),\n",
        "            # SpectralNorm(nn.Conv2d(ngf, 3, kernel_size=3, stride=1, padding=1)),\n",
        "            nn.Conv2d(ngf, 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.to_rgb0 = ToRGB(ngf*8)\n",
        "        self.to_rgb1 = ToRGB(ngf*4)\n",
        "        self.to_rgb2 = ToRGB(ngf*2)\n",
        "        self.to_rgb3 = ToRGB(ngf*1)\n",
        "\n",
        "        # for param in self.BlurInputConv.parameters():\n",
        "        #     param.requires_grad = False\n",
        "    \n",
        "    def forward(self, input, part_locations):\n",
        "        VggFeatures = self.VggExtract(input)\n",
        "        # for b in range(input.size(0)):\n",
        "        b = 0\n",
        "        UpdateVggFeatures = []\n",
        "        for i, f_size in enumerate(self.feature_sizes):\n",
        "            cur_feature = VggFeatures[i]\n",
        "\n",
        "            update_feature = cur_feature.clone() #* 0\n",
        "            cur_part_sizes = self.part_sizes // (512/f_size)\n",
        "            dicts_feature = getattr(self, 'Dict_'+str(f_size))\n",
        "            \n",
        "            LE_Dict_feature = dicts_feature['left_eye'].to(input)\n",
        "            RE_Dict_feature = dicts_feature['right_eye'].to(input)\n",
        "            NO_Dict_feature = dicts_feature['nose'].to(input)\n",
        "            MO_Dict_feature = dicts_feature['mouth'].to(input)\n",
        "\n",
        "            le_location = (part_locations[0][b] // (512/f_size)).int()\n",
        "            re_location = (part_locations[1][b] // (512/f_size)).int()\n",
        "            no_location = (part_locations[2][b] // (512/f_size)).int()\n",
        "            mo_location = (part_locations[3][b] // (512/f_size)).int()\n",
        "\n",
        "            LE_feature = cur_feature[:,:,le_location[1]:le_location[3],le_location[0]:le_location[2]].clone()\n",
        "            RE_feature = cur_feature[:,:,re_location[1]:re_location[3],re_location[0]:re_location[2]].clone()\n",
        "            NO_feature = cur_feature[:,:,no_location[1]:no_location[3],no_location[0]:no_location[2]].clone()\n",
        "            MO_feature = cur_feature[:,:,mo_location[1]:mo_location[3],mo_location[0]:mo_location[2]].clone()\n",
        "            \n",
        "            #resize\n",
        "            LE_feature_resize = F.interpolate(LE_feature,(LE_Dict_feature.size(2),LE_Dict_feature.size(3)),mode='bilinear',align_corners=False)\n",
        "            RE_feature_resize = F.interpolate(RE_feature,(RE_Dict_feature.size(2),RE_Dict_feature.size(3)),mode='bilinear',align_corners=False)\n",
        "            NO_feature_resize = F.interpolate(NO_feature,(NO_Dict_feature.size(2),NO_Dict_feature.size(3)),mode='bilinear',align_corners=False)\n",
        "            MO_feature_resize = F.interpolate(MO_feature,(MO_Dict_feature.size(2),MO_Dict_feature.size(3)),mode='bilinear',align_corners=False)\n",
        "            \n",
        "            LE_Dict_feature_norm = adaptive_instance_normalization_4D(LE_Dict_feature, LE_feature_resize)\n",
        "            RE_Dict_feature_norm = adaptive_instance_normalization_4D(RE_Dict_feature, RE_feature_resize)\n",
        "            NO_Dict_feature_norm = adaptive_instance_normalization_4D(NO_Dict_feature, NO_feature_resize)\n",
        "            MO_Dict_feature_norm = adaptive_instance_normalization_4D(MO_Dict_feature, MO_feature_resize)\n",
        "            \n",
        "            LE_score = F.conv2d(LE_feature_resize, LE_Dict_feature_norm)\n",
        "\n",
        "            LE_score = F.softmax(LE_score.view(-1),dim=0)\n",
        "            LE_index = torch.argmax(LE_score)\n",
        "            LE_Swap_feature = F.interpolate(LE_Dict_feature_norm[LE_index:LE_index+1], (LE_feature.size(2), LE_feature.size(3)))\n",
        "\n",
        "            LE_Attention = getattr(self, 'le_'+str(f_size))(LE_Swap_feature-LE_feature)\n",
        "            LE_Att_feature = LE_Attention * LE_Swap_feature\n",
        "            \n",
        "\n",
        "            RE_score = F.conv2d(RE_feature_resize, RE_Dict_feature_norm)\n",
        "            RE_score = F.softmax(RE_score.view(-1),dim=0)\n",
        "            RE_index = torch.argmax(RE_score)\n",
        "            RE_Swap_feature = F.interpolate(RE_Dict_feature_norm[RE_index:RE_index+1], (RE_feature.size(2), RE_feature.size(3)))\n",
        "            \n",
        "            RE_Attention = getattr(self, 're_'+str(f_size))(RE_Swap_feature-RE_feature)\n",
        "            RE_Att_feature = RE_Attention * RE_Swap_feature\n",
        "\n",
        "            NO_score = F.conv2d(NO_feature_resize, NO_Dict_feature_norm)\n",
        "            NO_score = F.softmax(NO_score.view(-1),dim=0)\n",
        "            NO_index = torch.argmax(NO_score)\n",
        "            NO_Swap_feature = F.interpolate(NO_Dict_feature_norm[NO_index:NO_index+1], (NO_feature.size(2), NO_feature.size(3)))\n",
        "            \n",
        "            NO_Attention = getattr(self, 'no_'+str(f_size))(NO_Swap_feature-NO_feature)\n",
        "            NO_Att_feature = NO_Attention * NO_Swap_feature\n",
        "\n",
        "            \n",
        "            MO_score = F.conv2d(MO_feature_resize, MO_Dict_feature_norm)\n",
        "            MO_score = F.softmax(MO_score.view(-1),dim=0)\n",
        "            MO_index = torch.argmax(MO_score)\n",
        "            MO_Swap_feature = F.interpolate(MO_Dict_feature_norm[MO_index:MO_index+1], (MO_feature.size(2), MO_feature.size(3)))\n",
        "            \n",
        "            MO_Attention = getattr(self, 'mo_'+str(f_size))(MO_Swap_feature-MO_feature)\n",
        "            MO_Att_feature = MO_Attention * MO_Swap_feature\n",
        "\n",
        "            update_feature[:,:,le_location[1]:le_location[3],le_location[0]:le_location[2]] = LE_Att_feature + LE_feature\n",
        "            update_feature[:,:,re_location[1]:re_location[3],re_location[0]:re_location[2]] = RE_Att_feature + RE_feature\n",
        "            update_feature[:,:,no_location[1]:no_location[3],no_location[0]:no_location[2]] = NO_Att_feature + NO_feature\n",
        "            update_feature[:,:,mo_location[1]:mo_location[3],mo_location[0]:mo_location[2]] = MO_Att_feature + MO_feature\n",
        "\n",
        "            UpdateVggFeatures.append(update_feature) \n",
        "        \n",
        "        fea_vgg = self.MSDilate(VggFeatures[3])\n",
        "        #new version\n",
        "        fea_up0 = self.up0(fea_vgg, UpdateVggFeatures[3])\n",
        "        # out1 = F.interpolate(fea_up0,(512,512))\n",
        "        # out1 = self.to_rgb0(out1)\n",
        "\n",
        "        fea_up1 = self.up1( fea_up0, UpdateVggFeatures[2]) #\n",
        "        # out2 = F.interpolate(fea_up1,(512,512))\n",
        "        # out2 = self.to_rgb1(out2)\n",
        "\n",
        "        fea_up2 = self.up2(fea_up1, UpdateVggFeatures[1]) #\n",
        "        # out3 = F.interpolate(fea_up2,(512,512))\n",
        "        # out3 = self.to_rgb2(out3)\n",
        "\n",
        "        fea_up3 = self.up3(fea_up2, UpdateVggFeatures[0]) #\n",
        "        # out4 = F.interpolate(fea_up3,(512,512))\n",
        "        # out4 = self.to_rgb3(out4)\n",
        "\n",
        "        output = self.up4(fea_up3) #\n",
        "        \n",
        "    \n",
        "        return output  #+ out4 + out3 + out2 + out1\n",
        "        #0 128 * 256 * 256\n",
        "        #1 256 * 128 * 128\n",
        "        #2 512 * 64 * 64\n",
        "        #3 512 * 32 * 32\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# smaller blocks\n",
        "def AttentionBlock(in_channel):\n",
        "    return nn.Sequential(\n",
        "        SpectralNorm(nn.Conv2d(in_channel, in_channel, 3, 1, 1)),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        SpectralNorm(nn.Conv2d(in_channel, in_channel, 3, 1, 1))\n",
        "    )\n",
        "\n",
        "\n",
        "class MSDilateBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channels,conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, kernel_size=3, dilation=[1,1,1,1], bias=True):\n",
        "        super(MSDilateBlock, self).__init__()\n",
        "        self.conv1 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[0], bias=bias)\n",
        "        self.conv2 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[1], bias=bias)\n",
        "        self.conv3 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[2], bias=bias)\n",
        "        self.conv4 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[3], bias=bias)\n",
        "        self.convi =  SpectralNorm(conv_layer(in_channels*4, in_channels, kernel_size=kernel_size, stride=1, padding=(kernel_size-1)//2, bias=bias))\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv1(x)\n",
        "        conv2 = self.conv2(x)\n",
        "        conv3 = self.conv3(x)\n",
        "        conv4 = self.conv4(x)\n",
        "        cat  = torch.cat([conv1, conv2, conv3, conv4], 1)\n",
        "        out = self.convi(cat) + x\n",
        "        return out\n",
        "\n",
        "\n",
        "class VggClassNet(pl.LightningModule):\n",
        "    def __init__(self, select_layer = ['0','5','10','19']):\n",
        "        super(VggClassNet, self).__init__()\n",
        "        self.select = select_layer\n",
        "        self.vgg = models.vgg19(pretrained=True).features\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for name, layer in self.vgg._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in self.select:\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class UpResBlock(pl.LightningModule):\n",
        "    def __init__(self, dim, conv_layer = nn.Conv2d, norm_layer = nn.BatchNorm2d):\n",
        "        super(UpResBlock, self).__init__()\n",
        "        self.Model = nn.Sequential(\n",
        "            # SpectralNorm(conv_layer(dim, dim, 3, 1, 1)),\n",
        "            conv_layer(dim, dim, 3, 1, 1),\n",
        "            # norm_layer(dim),\n",
        "            nn.LeakyReLU(0.2,True),\n",
        "            # SpectralNorm(conv_layer(dim, dim, 3, 1, 1)),\n",
        "            conv_layer(dim, dim, 3, 1, 1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        out = x + self.Model(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "L-8r5VTPWBpK"
      },
      "source": [
        "#@title functions.py\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "def compute_sum(x, axis=None, keepdim=False):\n",
        "    if not axis:\n",
        "        axis = range(len(x.shape))\n",
        "    for i in sorted(axis, reverse=True):\n",
        "        x = torch.sum(x, dim=i, keepdim=keepdim)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def convU(in_channels, out_channels,conv_layer, norm_layer, kernel_size=3, stride=1,dilation=1, bias=True):\n",
        "    return nn.Sequential(\n",
        "        SpectralNorm(conv_layer(in_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias)),\n",
        "#         conv_layer(in_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        SpectralNorm(conv_layer(out_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias)),\n",
        "    )\n",
        "\n",
        "\n",
        "class AdaptiveInstanceNorm(pl.LightningModule):\n",
        "    def __init__(self, in_channel):\n",
        "        super().__init__()\n",
        "        self.norm = nn.InstanceNorm2d(in_channel)\n",
        "\n",
        "    def forward(self, input, style):\n",
        "        style_mean, style_std = calc_mean_std_4D(style)\n",
        "        out = self.norm(input)\n",
        "        size = input.size()\n",
        "        out = style_std.expand(size) * out + style_mean.expand(size)\n",
        "        return out\n",
        "\n",
        "class BlurFunctionBackward(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, grad_output, kernel, kernel_flip):\n",
        "        ctx.save_for_backward(kernel, kernel_flip)\n",
        "\n",
        "        grad_input = F.conv2d(\n",
        "            grad_output, kernel_flip, padding=1, groups=grad_output.shape[1]\n",
        "        )\n",
        "        return grad_input\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, gradgrad_output):\n",
        "        kernel, kernel_flip = ctx.saved_tensors\n",
        "\n",
        "        grad_input = F.conv2d(\n",
        "            gradgrad_output, kernel, padding=1, groups=gradgrad_output.shape[1]\n",
        "        )\n",
        "        return grad_input, None, None\n",
        "\n",
        "\n",
        "class BlurFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, kernel, kernel_flip):\n",
        "        ctx.save_for_backward(kernel, kernel_flip)\n",
        "\n",
        "        output = F.conv2d(input, kernel, padding=1, groups=input.shape[1])\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        kernel, kernel_flip = ctx.saved_tensors\n",
        "\n",
        "        grad_input = BlurFunctionBackward.apply(grad_output, kernel, kernel_flip)\n",
        "\n",
        "        return grad_input, None, None\n",
        "\n",
        "blur = BlurFunction.apply\n",
        "\n",
        "\n",
        "class Blur(pl.LightningModule):\n",
        "    def __init__(self, channel):\n",
        "        super().__init__()\n",
        "\n",
        "        weight = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=torch.float32)\n",
        "        weight = weight.view(1, 1, 3, 3)\n",
        "        weight = weight / weight.sum()\n",
        "        weight_flip = torch.flip(weight, [2, 3])\n",
        "\n",
        "        self.register_buffer('weight', weight.repeat(channel, 1, 1, 1))\n",
        "        self.register_buffer('weight_flip', weight_flip.repeat(channel, 1, 1, 1))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return blur(input, self.weight, self.weight_flip)\n",
        "\n",
        "class EqualLR:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def compute_weight(self, module):\n",
        "        weight = getattr(module, self.name + '_orig')\n",
        "        fan_in = weight.data.size(1) * weight.data[0][0].numel()\n",
        "        return weight * sqrt(2 / fan_in)\n",
        "    @staticmethod\n",
        "    def apply(module, name):\n",
        "        fn = EqualLR(name)\n",
        "\n",
        "        weight = getattr(module, name)\n",
        "        del module._parameters[name]\n",
        "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
        "        module.register_forward_pre_hook(fn)\n",
        "\n",
        "        return fn\n",
        "\n",
        "    def __call__(self, module, input):\n",
        "        weight = self.compute_weight(module)\n",
        "        setattr(module, self.name, weight)\n",
        "\n",
        "def equal_lr(module, name='weight'):\n",
        "    EqualLR.apply(module, name)\n",
        "    return module\n",
        "\n",
        "class EqualConv2d(pl.LightningModule):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        conv = nn.Conv2d(*args, **kwargs)\n",
        "        conv.weight.data.normal_()\n",
        "        conv.bias.data.zero_()\n",
        "        self.conv = equal_lr(conv)\n",
        "    def forward(self, input):\n",
        "        return self.conv(input)\n",
        "\n",
        "class NoiseInjection(pl.LightningModule):\n",
        "    def __init__(self, channel):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))\n",
        "    def forward(self, image, noise):\n",
        "        return image + self.weight * noise\n",
        "\n",
        "\n",
        "def ToRGB(in_channel):\n",
        "    return nn.Sequential(\n",
        "        SpectralNorm(nn.Conv2d(in_channel,in_channel,3, 1, 1)),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        SpectralNorm(nn.Conv2d(in_channel,3,3, 1, 1))\n",
        "    )\n",
        "\n",
        "def adaptive_instance_normalization_4D(content_feat, style_feat): # content_feat is ref feature, style is degradate feature\n",
        "    # assert (content_feat.size()[:2] == style_feat.size()[:2])\n",
        "    size = content_feat.size()\n",
        "    style_mean, style_std = calc_mean_std_4D(style_feat)\n",
        "    content_mean, content_std = calc_mean_std_4D(content_feat)\n",
        "    normalized_feat = (content_feat - content_mean.expand(\n",
        "        size)) / content_std.expand(size)\n",
        "    return normalized_feat * style_std + style_mean\n",
        "\n",
        "def calc_mean_std_4D(feat, eps=1e-5):\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
        "    size = feat.size()\n",
        "    assert (len(size) == 4)\n",
        "    N, C = size[:2]\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
        "    return feat_mean, feat_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVZEeiVEF8h-"
      },
      "source": [
        "Restart with ``runtime > restart runtime`` once if you see ``AttributeError: module 'PIL.TiffTags' has no attribute 'IFD'``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZu_zf7CGU76"
      },
      "source": [
        "# Training\n",
        "%cd /content/\n",
        "dm = DFNetDataModule(training_path = '/content/DFDNet/ffhq/', train_partpath = '/content/DFDNet/landmarks', validation_path = '/content/validation/', val_partpath='/content/landmarks', batch_size=1)\n",
        "model = CustomTrainClass()\n",
        "#model = model.load_from_checkpoint('/content/Checkpoint_0_450.ckpt') # start training from checkpoint, warning: apperantly global_step will be reset to zero and overwriting validation images, you could manually make an offset\n",
        "\n",
        "#weights_init(model, 'kaiming')\n",
        "# GPU\n",
        "#trainer = pl.Trainer(gpus=1, max_epochs=10, progress_bar_refresh_rate=1, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=10, save_path='/content/')])\n",
        "# GPU with AMP (amp_level='O1' = mixed precision)\n",
        "# currently not working\n",
        "#trainer = pl.Trainer(gpus=1, precision=16, amp_level='O1', max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "# TPU\n",
        "#trainer = pl.Trainer(tpu_cores=8, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "trainer.fit(model, dm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe53B8ghd2h3"
      },
      "source": [
        "# Landmark generation\n",
        "\n",
        "Install that and restart runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5PWc4-3XfIH"
      },
      "source": [
        "!pip install face-alignment\n",
        "!pip install matplotlib --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Gf-mz6Ljd3-B"
      },
      "source": [
        "%cd /content/\n",
        "import face_alignment\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False)\n",
        "\n",
        "unchecked_input_path = '/content/validation' #@param {type:\"string\"}\n",
        "checked_output_path = '/content/validation' #@param {type:\"string\"}\n",
        "failed_output_path = '/content/validation' #@param {type:\"string\"}\n",
        "landmark_output_path = '/content/landmarks' #@param {type:\"string\"}\n",
        "\n",
        "if not os.path.exists(unchecked_input_path):\n",
        "    os.makedirs(unchecked_input_path)\n",
        "if not os.path.exists(checked_output_path):\n",
        "    os.makedirs(checked_output_path)\n",
        "if not os.path.exists(failed_output_path):\n",
        "    os.makedirs(failed_output_path)\n",
        "if not os.path.exists(landmark_output_path):\n",
        "    os.makedirs(landmark_output_path)\n",
        "\n",
        "files = glob.glob(unchecked_input_path + '/**/*.png', recursive=True)\n",
        "files_jpg = glob.glob(unchecked_input_path + '/**/*.jpg', recursive=True)\n",
        "files.extend(files_jpg)\n",
        "err_files=[]\n",
        "\n",
        "for f in tqdm(files):\n",
        "  input = io.imread(f)\n",
        "  preds = fa.get_landmarks(input)\n",
        "  #print(preds)\n",
        "  if preds is not None:\n",
        "    np.savetxt(os.path.join(landmark_output_path, os.path.basename(f)+\".txt\"), preds[0], delimiter=' ', fmt='%1.3f')   # X is an array\n",
        "    shutil.move(f, os.path.join(checked_output_path,os.path.basename(f)))\n",
        "  else:\n",
        "    shutil.move(f, os.path.join(failed_output_path,os.path.basename(f)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}