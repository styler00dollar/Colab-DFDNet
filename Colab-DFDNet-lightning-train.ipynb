{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-DFDNet-lightning-train.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "3eIUSjKnP0Bc",
        "EpsH6ZX2PtZf",
        "jsDK_eA4PwTu",
        "xZRGxo99trki",
        "dURET3riP8W7"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh4lKuAB9Fu0"
      },
      "source": [
        "# Colab-DFDNet-lightning\n",
        "\n",
        "Official repo: [csxmli2016/DFDNet](https://github.com/csxmli2016/DFDNet)\n",
        "\n",
        "Original repo: [max-vasyuk/DFDNet](https://github.com/max-vasyuk/DFDNet)\n",
        "\n",
        "My fork: [styler00dollar/Colab-DFDNet](https://github.com/styler00dollar/Colab-DFDNet)\n",
        "\n",
        "Porting everything to pytorch lightning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY6ZIFNg8-98"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5m5HGOQ9ESX",
        "cellView": "form"
      },
      "source": [
        "#@title GPU\n",
        "!pip install pytorch-lightning -U\n",
        "!git clone https://github.com/max-vasyuk/DFDNet\n",
        "%cd /content/DFDNet\n",
        "!pip install -r requirements.txt\n",
        "!pip install pytorch-msssim\n",
        "!pip install trains\n",
        "!pip install PyJWT==1.7.1\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eLIneSv9jXR",
        "cellView": "form"
      },
      "source": [
        "#@title TPU  (restart runtime afterwards)\n",
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n",
        "#!pip install pytorch-lightning\n",
        "!pip install lightning-flash\n",
        "\n",
        "import collections\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\n",
        "VERSION = \"xrt==1.15.0\"  #@param [\"xrt==1.15.0\", \"torch_xla==nightly\"]\n",
        "CONFIG = {\n",
        "    'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),\n",
        "    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n",
        "        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n",
        "}[VERSION]\n",
        "DIST_BUCKET = 'gs://tpu-pytorch/wheels'\n",
        "TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "\n",
        "# Update TPU XRT version\n",
        "def update_server_xrt():\n",
        "  print('Updating server-side XRT to {} ...'.format(CONFIG.server))\n",
        "  url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(\n",
        "      TPU_ADDRESS=os.environ['COLAB_TPU_ADDR'].split(':')[0],\n",
        "      XRT_VERSION=CONFIG.server,\n",
        "  )\n",
        "  print('Done updating server-side XRT: {}'.format(requests.post(url)))\n",
        "\n",
        "update = threading.Thread(target=update_server_xrt)\n",
        "update.start()\n",
        "\n",
        "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
        "!pip uninstall -y torch torchvision\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
        "!pip install \"$TORCH_WHEEL\"\n",
        "!pip install \"$TORCH_XLA_WHEEL\"\n",
        "!pip install \"$TORCHVISION_WHEEL\"\n",
        "!sudo apt-get install libomp5\n",
        "update.join()\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "\n",
        "\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev > /dev/null\n",
        "!pip install pytorch-lightning > /dev/null\n",
        "\n",
        "\n",
        "!git clone https://github.com/max-vasyuk/DFDNet\n",
        "%cd /content/DFDNet\n",
        "!pip install -r requirements.txt\n",
        "!pip install pytorch-msssim\n",
        "!pip install trains\n",
        "!pip install PyJWT==1.7.1\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcqfHteR9dBs",
        "cellView": "form"
      },
      "source": [
        "#@title download data\n",
        "!mkdir /content/DFDNet/DictionaryCenter512\n",
        "%cd /content/DFDNet/DictionaryCenter512\n",
        "!gdown --id 1sEB9j3s7Wj9aqPai1NF-MR7B-c0zfTin\n",
        "!gdown --id 1H4kByBiVmZuS9TbrWUR5uSNY770Goid6\n",
        "!gdown --id 10ctK3d9znZ9nGN3d1Z77xW3GGshbeKBb\n",
        "!gdown --id 1gcwmrIZjPFVu-cHjdQD6P4luohkPsil-\n",
        "!gdown --id 1rJ8cORPxbJsIVAiNrjBag0ihaY_Mvurn\n",
        "!gdown --id 1LkfJv2a3ud-mefAc1eZMJuINuNdSYgYO\n",
        "!gdown --id 1LH-nxD__icSJvTiAbXAXDch03oDtbpkZ\n",
        "!gdown --id 1JRTStLFsQ8dwaQjQ8qG5fNyrOvo6Tcvd\n",
        "!gdown --id 1Z4AkU1pOYTYpdbfljCgNMmPilhdEd0Kl\n",
        "!gdown --id 1Z4e1ltB3ACbYKzkoMBuVtzZ7a310G4xc\n",
        "!gdown --id 1fqWmi6-8ZQzUtZTp9UH4hyom7n4nl8aZ\n",
        "!gdown --id 1wfHtsExLvSgfH_EWtCPjTF5xsw3YyvjC\n",
        "!gdown --id 1Jr3Luf6tmcdKANcSLzvt0sjXr0QUIQ2g\n",
        "!gdown --id 1sPd4_IMYgqGLol0gqhHjBedKKxFAxswR\n",
        "!gdown --id 1eVFjXJRnBH4mx7ZbAmZRwVXZNUbgCQec\n",
        "!gdown --id 1w0GfO_KY775ZVF3KMk74ya6QL_bNU4cJ\n",
        "%cd /content/DFDNet\n",
        "!gdown --id 1VE5tnOKcfL6MoV839IVCCw5FhJxIgml5\n",
        "!7z x data.zip\n",
        "!mkdir /content/DFDNet/weights/\n",
        "%cd /content/DFDNet/weights/\n",
        "!gdown --id 1SfKKZJduOGhDD27Xl01yDx0-YSEkL2Aa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KewGGnkevRP"
      },
      "source": [
        "Preconfigured paths:\n",
        "```\n",
        "/content/DFDNet/DictionaryCenter512 (numpy dictionary files)\n",
        "/content/DFDNet/ffhq (dataset path)\n",
        "/content/DFDNet/landmarks (landmarks for that dataset)\n",
        "/content/DFDNet/weights/vgg19.pth (vgg19 path)\n",
        "\n",
        "/content/validation (validation path)\n",
        "/content/landmarks (landmark path for validation)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cpbKp5cZNz5x"
      },
      "source": [
        "#@title getting pytorch-loss-functions\n",
        "%cd /content\n",
        "!git clone https://github.com/styler00dollar/pytorch-loss-functions pytorchloss\n",
        "%cd /content/pytorchloss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppsn-NgBN2zP"
      },
      "source": [
        "# restart from here if you reset your notebook\n",
        "%cd /content/pytorchloss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eIUSjKnP0Bc"
      },
      "source": [
        "# Additional to training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5GYCQ5c_lyF",
        "cellView": "form"
      },
      "source": [
        "#@title init.py\n",
        "import torch.nn.init as init\n",
        "\n",
        "def weights_init(net, init_type = 'kaiming', init_gain = 0.02):\n",
        "    #Initialize network weights.\n",
        "    #Parameters:\n",
        "    #    net (network)       -- network to be initialized\n",
        "    #    init_type (str)     -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "    #    init_var (float)    -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "\n",
        "        if hasattr(m, 'weight') and classname.find('Conv') != -1:\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, init_gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain = init_gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain = init_gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('Linear') != -1:\n",
        "            init.normal_(m.weight, 0, 0.01)\n",
        "            init.constant_(m.bias, 0)\n",
        "\n",
        "    # Apply the initialization function <init_func>\n",
        "    print('Initialization method [{:s}]'.format(init_type))\n",
        "    net.apply(init_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmNzK1bmFgeg",
        "cellView": "form"
      },
      "source": [
        "#@title checkpoint.py\n",
        "#https://github.com/PyTorchLightning/pytorch-lightning/issues/2534\n",
        "import os\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class CheckpointEveryNSteps(pl.Callback):\n",
        "    \"\"\"\n",
        "    Save a checkpoint every N steps, instead of Lightning's default that checkpoints\n",
        "    based on validation loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        save_step_frequency,\n",
        "        prefix=\"Checkpoint\",\n",
        "        use_modelcheckpoint_filename=False,\n",
        "        save_path = '/content/'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            save_step_frequency: how often to save in steps\n",
        "            prefix: add a prefix to the name, only used if\n",
        "                use_modelcheckpoint_filename=False\n",
        "            use_modelcheckpoint_filename: just use the ModelCheckpoint callback's\n",
        "                default filename, don't use ours.\n",
        "        \"\"\"\n",
        "        self.save_step_frequency = save_step_frequency\n",
        "        self.prefix = prefix\n",
        "        self.use_modelcheckpoint_filename = use_modelcheckpoint_filename\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def on_batch_end(self, trainer: pl.Trainer, _):\n",
        "        \"\"\" Check if we should save a checkpoint after every train batch \"\"\"\n",
        "        epoch = trainer.current_epoch\n",
        "        global_step = trainer.global_step\n",
        "        if global_step % self.save_step_frequency == 0:\n",
        "            if self.use_modelcheckpoint_filename:\n",
        "                filename = trainer.checkpoint_callback.filename\n",
        "            else:\n",
        "                filename = f\"{self.prefix}_{epoch}_{global_step}.ckpt\"\n",
        "            #ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
        "            ckpt_path = os.path.join(self.save_path, filename)\n",
        "            trainer.save_checkpoint(ckpt_path)\n",
        "            # run validation once checkpoint was made\n",
        "            trainer.run_evaluation()\n",
        "\n",
        "#Trainer(callbacks=[CheckpointEveryNSteps()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpsH6ZX2PtZf"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5aCGLgr9m6L"
      },
      "source": [
        "Edit motion blur kernel path inside ``data``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nAoSz0QGAw4",
        "cellView": "form"
      },
      "source": [
        "#@title data.py\n",
        "import os.path\n",
        "import os\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from PIL import Image, ImageFilter\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "from scipy.io import loadmat\n",
        "from PIL import Image\n",
        "import PIL\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class DS(Dataset):\n",
        "    \n",
        "    def __init__(self, root_dir, fine_size=512, transform=None, partpath=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.pathes = [os.path.join(self.root_dir, x) for x in os.listdir(self.root_dir) if x[-3:] == 'png']\n",
        "        self.transform = transform\n",
        "        self.fine_size = fine_size\n",
        "        self.partpath = partpath\n",
        "        \n",
        "    def AddNoise(self,img): # noise\n",
        "        if random.random() > 0.9: #\n",
        "            return img\n",
        "        self.sigma = np.random.randint(1, 11)\n",
        "        img_tensor = torch.from_numpy(np.array(img)).float()\n",
        "        noise = torch.randn(img_tensor.size()).mul_(self.sigma/1.0)\n",
        "\n",
        "        noiseimg = torch.clamp(noise+img_tensor,0,255)\n",
        "        return Image.fromarray(np.uint8(noiseimg.numpy()))\n",
        "\n",
        "    def AddBlur(self,img): # gaussian blur or motion blur\n",
        "        if random.random() > 0.9: #\n",
        "            return img\n",
        "        img = np.array(img)\n",
        "        if random.random() > 0.35: ##gaussian blur\n",
        "            blursize = random.randint(1,17) * 2 + 1 ##3,5,7,9,11,13,15\n",
        "            blursigma = random.randint(3, 20)\n",
        "            img = cv2.GaussianBlur(img, (blursize,blursize), blursigma/10)\n",
        "        else: #motion blur\n",
        "            M = random.randint(1,32)\n",
        "            KName = '/content/DFDNet/data/MotionBlurKernel/m_%02d.mat' % M\n",
        "            k = loadmat(KName)['kernel']\n",
        "            k = k.astype(np.float32)\n",
        "            k /= np.sum(k)\n",
        "            img = cv2.filter2D(img,-1,k)\n",
        "        return Image.fromarray(img)\n",
        "\n",
        "    def AddDownSample(self,img): # downsampling\n",
        "        if random.random() > 0.95: #\n",
        "            return img\n",
        "        sampler = random.randint(20, 100)*1.0\n",
        "        img = img.resize((int(self.fine_size/sampler*10.0), int(self.fine_size/sampler*10.0)), Image.BICUBIC)\n",
        "        return img\n",
        "\n",
        "    def AddJPEG(self,img): # JPEG compression\n",
        "        if random.random() > 0.6:\n",
        "            return img\n",
        "        imQ = random.randint(40, 80)\n",
        "        img = np.array(img)\n",
        "        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY),imQ] # (0,100),higher is better,default is 95\n",
        "        _, encA = cv2.imencode('.jpg', img, encode_param)\n",
        "        img = cv2.imdecode(encA,1)\n",
        "        return Image.fromarray(img)\n",
        "\n",
        "    def AddUpSample(self,img):\n",
        "        return img.resize((self.fine_size, self.fine_size), Image.BICUBIC)\n",
        "\n",
        "    def __getitem__(self, index): # indexation\n",
        "\n",
        "        path = self.pathes[index]\n",
        "        Imgs = Image.open(path).convert('RGB')\n",
        "        \n",
        "        #A = Imgs.resize((self.fine_size, self.fine_size))\n",
        "        if self.transform:\n",
        "          A = self.transform(Imgs)\n",
        "\n",
        "        A = transforms.ColorJitter(0.3, 0.3, 0.3, 0)(A)\n",
        "        C = A\n",
        "        A = self.AddBlur(A)\n",
        "        A = self.AddJPEG(A)\n",
        "        \n",
        "        tmps = path.split('/')\n",
        "        ImgName = tmps[-1]\n",
        "        part_locations = self.get_part_location(self.partpath, ImgName, 2)\n",
        "        \n",
        "        A = transforms.ToTensor()(A)\n",
        "        C = transforms.ToTensor()(C)\n",
        "        \n",
        "        A = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(A) \n",
        "        C = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(C)\n",
        "\n",
        "        #A = transforms.ToTensor()(A)\n",
        "        #C = transforms.ToTensor()(C)\n",
        "\n",
        "        #return {'A': A, 'C': C, 'path': path, 'part_locations': part_locations}\n",
        "        return A, C, path, part_locations\n",
        "\n",
        "    def get_part_location(self, landmarkpath, imgname, downscale=1):\n",
        "        Landmarks = []\n",
        "        with open(os.path.join(landmarkpath, imgname + '.txt'),'r') as f:\n",
        "            for line in f:\n",
        "                tmp = [np.float(i) for i in line.split(' ') if i != '\\n']\n",
        "                Landmarks.append(tmp)\n",
        "        Landmarks = np.array(Landmarks)/downscale # 512 * 512\n",
        "        \n",
        "        Map_LE = list(np.hstack((range(17,22), range(36,42))))\n",
        "        Map_RE = list(np.hstack((range(22,27), range(42,48))))\n",
        "        Map_NO = list(range(29,36))\n",
        "        Map_MO = list(range(48,68))\n",
        "        #left eye\n",
        "        Mean_LE = np.mean(Landmarks[Map_LE],0)\n",
        "        L_LE = np.max((np.max(np.max(Landmarks[Map_LE],0) - np.min(Landmarks[Map_LE],0))/2,16))\n",
        "        Location_LE = np.hstack((Mean_LE - L_LE + 1, Mean_LE + L_LE)).astype(int)\n",
        "        #right eye\n",
        "        Mean_RE = np.mean(Landmarks[Map_RE],0)\n",
        "        L_RE = np.max((np.max(np.max(Landmarks[Map_RE],0) - np.min(Landmarks[Map_RE],0))/2,16))\n",
        "        Location_RE = np.hstack((Mean_RE - L_RE + 1, Mean_RE + L_RE)).astype(int)\n",
        "        #nose\n",
        "        Mean_NO = np.mean(Landmarks[Map_NO],0)\n",
        "        L_NO = np.max((np.max(np.max(Landmarks[Map_NO],0) - np.min(Landmarks[Map_NO],0))/2,16))\n",
        "        Location_NO = np.hstack((Mean_NO - L_NO + 1, Mean_NO + L_NO)).astype(int)\n",
        "        #mouth\n",
        "        Mean_MO = np.mean(Landmarks[Map_MO],0)\n",
        "        L_MO = np.max((np.max(np.max(Landmarks[Map_MO],0) - np.min(Landmarks[Map_MO],0))/2,16))\n",
        "\n",
        "        Location_MO = np.hstack((Mean_MO - L_MO + 1, Mean_MO + L_MO)).astype(int)\n",
        "        return Location_LE, Location_RE, Location_NO, Location_MO\n",
        "\n",
        "    def __len__(self): #\n",
        "        return len(self.pathes)\n",
        "\n",
        "    def name(self):\n",
        "        return 'AlignedDataset'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DS_val(Dataset):\n",
        "    \n",
        "    def __init__(self, root_dir, fine_size=512, transform=None, partpath=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.pathes = [os.path.join(self.root_dir, x) for x in os.listdir(self.root_dir) if x[-3:] == 'png']\n",
        "        self.transform = transform\n",
        "        self.fine_size = fine_size\n",
        "        self.partpath = partpath\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index): # indexation\n",
        "\n",
        "        path = self.pathes[index]\n",
        "        Imgs = Image.open(path).convert('RGB')\n",
        "        \n",
        "        A = Imgs\n",
        "        #A = Imgs.resize((self.fine_size, self.fine_size))\n",
        "        #A = transforms.ColorJitter(0.3, 0.3, 0.3, 0)(A)\n",
        "        #C = A\n",
        "        #A = self.AddBlur(A)\n",
        "        #A = self.AddJPEG(A)\n",
        "        \n",
        "        tmps = path.split('/')\n",
        "        ImgName = tmps[-1]\n",
        "        part_locations = self.get_part_location(self.partpath, ImgName, 2)\n",
        "        \n",
        "        A = transforms.ToTensor()(A)\n",
        "        #C = transforms.ToTensor()(C)\n",
        "        \n",
        "        A = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(A) \n",
        "        #C = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(C)\n",
        "        \n",
        "        #return {'A': A, 'path': path, 'part_locations': part_locations}\n",
        "        return A, path, part_locations\n",
        "\n",
        "    def get_part_location(self, landmarkpath, imgname, downscale=1):\n",
        "        Landmarks = []\n",
        "        with open(os.path.join(landmarkpath, imgname + '.txt'),'r') as f:\n",
        "            for line in f:\n",
        "                tmp = [np.float(i) for i in line.split(' ') if i != '\\n']\n",
        "                Landmarks.append(tmp)\n",
        "        Landmarks = np.array(Landmarks)/downscale # 512 * 512\n",
        "        \n",
        "        Map_LE = list(np.hstack((range(17,22), range(36,42))))\n",
        "        Map_RE = list(np.hstack((range(22,27), range(42,48))))\n",
        "        Map_NO = list(range(29,36))\n",
        "        Map_MO = list(range(48,68))\n",
        "        #left eye\n",
        "        Mean_LE = np.mean(Landmarks[Map_LE],0)\n",
        "        L_LE = np.max((np.max(np.max(Landmarks[Map_LE],0) - np.min(Landmarks[Map_LE],0))/2,16))\n",
        "        Location_LE = np.hstack((Mean_LE - L_LE + 1, Mean_LE + L_LE)).astype(int)\n",
        "        #right eye\n",
        "        Mean_RE = np.mean(Landmarks[Map_RE],0)\n",
        "        L_RE = np.max((np.max(np.max(Landmarks[Map_RE],0) - np.min(Landmarks[Map_RE],0))/2,16))\n",
        "        Location_RE = np.hstack((Mean_RE - L_RE + 1, Mean_RE + L_RE)).astype(int)\n",
        "        #nose\n",
        "        Mean_NO = np.mean(Landmarks[Map_NO],0)\n",
        "        L_NO = np.max((np.max(np.max(Landmarks[Map_NO],0) - np.min(Landmarks[Map_NO],0))/2,16))\n",
        "        Location_NO = np.hstack((Mean_NO - L_NO + 1, Mean_NO + L_NO)).astype(int)\n",
        "        #mouth\n",
        "        Mean_MO = np.mean(Landmarks[Map_MO],0)\n",
        "        L_MO = np.max((np.max(np.max(Landmarks[Map_MO],0) - np.min(Landmarks[Map_MO],0))/2,16))\n",
        "\n",
        "        Location_MO = np.hstack((Mean_MO - L_MO + 1, Mean_MO + L_MO)).astype(int)\n",
        "        return Location_LE, Location_RE, Location_NO, Location_MO\n",
        "\n",
        "    def __len__(self): #\n",
        "        return len(self.pathes)\n",
        "\n",
        "    def name(self):\n",
        "        return 'ValDataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu7rwTczGGkt"
      },
      "source": [
        "#@title dataloader.py\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class DFNetDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, training_path: str = './', train_partpath: str = './', validation_path: str = './', val_partpath: str = './', test_path: str = './', batch_size: int = 5, num_workers: int = 2):\n",
        "        super().__init__()\n",
        "        self.training_dir = training_path\n",
        "        self.validation_dir = validation_path\n",
        "        self.test_dir = test_path\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.size = 512\n",
        "\n",
        "\n",
        "        self.train_partpath = train_partpath\n",
        "        self.val_partpath = val_partpath\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(size=self.size),\n",
        "            transforms.CenterCrop(size=self.size),\n",
        "            #transforms.RandomHorizontalFlip()\n",
        "            #transforms.ToTensor()\n",
        "        ])\n",
        "        \n",
        "        self.DFDNetdataset_train = DS(root_dir=self.training_dir, transform=transform, fine_size=self.size, partpath=self.train_partpath)\n",
        "        self.DFDNetdataset_validation = DS_val(root_dir=self.validation_dir, transform=transform, partpath=self.val_partpath)\n",
        "        self.DFDNetdataset_test = DS_val(root_dir=self.validation_dir, transform=transform, partpath='/content/landmarks')\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.DFDNetdataset_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.DFDNetdataset_validation, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.DFDNetdataset_test, batch_size=self.batch_size, num_workers=self.num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsDK_eA4PwTu"
      },
      "source": [
        "# Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "-inyB7ZlNIAM"
      },
      "source": [
        "#@title metrics.py (removing lpips import)\n",
        "%%writefile /content/pytorchloss/metrics.py\n",
        "#https://github.com/huster-wgm/Pytorch-metrics/blob/master/metrics.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "# -*- coding: UTF-8 -*-\n",
        "\"\"\"\n",
        "  @Email:  guangmingwu2010@gmail.com \\\n",
        "           guozhilingty@gmail.com\n",
        "  @Copyright: go-hiroaki & Chokurei\n",
        "  @License: MIT\n",
        "\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#import lpips\n",
        "\n",
        "eps = 1e-6\n",
        "\n",
        "def _binarize(y_data, threshold):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_data : [float] 4-d tensor in [batch_size, channels, img_rows, img_cols]\n",
        "        threshold : [float] [0.0, 1.0]\n",
        "    return 4-d binarized y_data\n",
        "    \"\"\"\n",
        "    y_data[y_data < threshold] = 0.0\n",
        "    y_data[y_data >= threshold] = 1.0\n",
        "    return y_data\n",
        "\n",
        "def _argmax(y_data, dim):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_data : 4-d tensor in [batch_size, chs, img_rows, img_cols]\n",
        "        dim : int\n",
        "    return 3-d [int] y_data\n",
        "    \"\"\"\n",
        "    return torch.argmax(y_data, dim).int()\n",
        "\n",
        "\n",
        "def _get_tp(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : [int] 3-d in [batch_size, img_rows, img_cols]\n",
        "        y_pred : [int] 3-d in [batch_size, img_rows, img_cols]\n",
        "    return [float] true_positive\n",
        "    \"\"\"\n",
        "    return torch.sum(y_true * y_pred).float()\n",
        "\n",
        "\n",
        "def _get_fp(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        y_pred : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "    return [float] false_positive\n",
        "    \"\"\"\n",
        "    return torch.sum((1 - y_true) * y_pred).float()\n",
        "\n",
        "\n",
        "def _get_tn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        y_pred : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "    return [float] true_negative\n",
        "    \"\"\"\n",
        "    return torch.sum((1 - y_true) * (1 - y_pred)).float()\n",
        "\n",
        "\n",
        "def _get_fn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        y_pred : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "    return [float] false_negative\n",
        "    \"\"\"\n",
        "    return torch.sum(y_true * (1 - y_pred)).float()\n",
        "\n",
        "\n",
        "def _get_weights(y_true, nb_ch):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        nb_ch : int\n",
        "    return [float] weights\n",
        "    \"\"\"\n",
        "    batch_size, img_rows, img_cols = y_true.shape\n",
        "    pixels = batch_size * img_rows * img_cols\n",
        "    weights = [torch.sum(y_true==ch).item() / pixels for ch in range(nb_ch)]\n",
        "    return weights\n",
        "\n",
        "\n",
        "class CFMatrix(object):\n",
        "    def __init__(self, des=None):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ConfusionMatrix\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return confusion matrix\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            nb_tn = _get_tn(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            mperforms = [nb_tp, nb_fp, nb_tn, nb_fn]\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 4).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                nb_tn = _get_tn(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                performs[int(ch), :] = [nb_tp, nb_fp, nb_tn, nb_fn]\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class OAAcc(object):\n",
        "    def __init__(self, des=\"Overall Accuracy\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"OAcc\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return (tp+tn)/total\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "\n",
        "        nb_tp_tn = torch.sum(y_true == y_pred).float()\n",
        "        mperforms = nb_tp_tn / (batch_size * img_rows * img_cols)\n",
        "        performs = None\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Precision(object):\n",
        "    def __init__(self, des=\"Precision\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Prec\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return tp/(tp+fp)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            mperforms = nb_tp / (nb_tp + nb_fp + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                performs[int(ch)] = nb_tp / (nb_tp + nb_fp + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Recall(object):\n",
        "    def __init__(self, des=\"Recall\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Reca\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return tp/(tp+fn)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            mperforms = nb_tp / (nb_tp + nb_fn + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                performs[int(ch)] = nb_tp / (nb_tp + nb_fn + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class F1Score(object):\n",
        "    def __init__(self, des=\"F1Score\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"F1Sc\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return 2*precision*recall/(precision+recall)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            _precision = nb_tp / (nb_tp + nb_fp + esp)\n",
        "            _recall = nb_tp / (nb_tp + nb_fn + esp)\n",
        "            mperforms = 2 * _precision * _recall / (_precision + _recall + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                _precision = nb_tp / (nb_tp + nb_fp + esp)\n",
        "                _recall = nb_tp / (nb_tp + nb_fn + esp)\n",
        "                performs[int(ch)] = 2 * _precision * \\\n",
        "                    _recall / (_precision + _recall + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Kappa(object):\n",
        "    def __init__(self, des=\"Kappa\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Kapp\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return (Po-Pe)/(1-Pe)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            nb_tn = _get_tn(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            nb_total = nb_tp + nb_fp + nb_tn + nb_fn\n",
        "            Po = (nb_tp + nb_tn) / nb_total\n",
        "            Pe = ((nb_tp + nb_fp) * (nb_tp + nb_fn) +\n",
        "                  (nb_fn + nb_tn) * (nb_fp + nb_tn)) / (nb_total**2)\n",
        "            mperforms = (Po - Pe) / (1 - Pe + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                nb_tn = _get_tn(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                nb_total = nb_tp + nb_fp + nb_tn + nb_fn\n",
        "                Po = (nb_tp + nb_tn) / nb_total\n",
        "                Pe = ((nb_tp + nb_fp) * (nb_tp + nb_fn)\n",
        "                      + (nb_fn + nb_tn) * (nb_fp + nb_tn)) / (nb_total**2)\n",
        "                performs[int(ch)] = (Po - Pe) / (1 - Pe + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Jaccard(object):\n",
        "    def __init__(self, des=\"Jaccard\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Jacc\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return intersection / (sum-intersection)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            _intersec = torch.sum(y_true * y_pred).float()\n",
        "            _sum = torch.sum(y_true + y_pred).float()\n",
        "            mperforms = _intersec / (_sum - _intersec + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                _intersec = torch.sum(y_true_ch * y_pred_ch).float()\n",
        "                _sum = torch.sum(y_true_ch + y_pred_ch).float()\n",
        "                performs[int(ch)] = _intersec / (_sum - _intersec + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class MSE(object):\n",
        "    def __init__(self, des=\"Mean Square Error\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MSE\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, dim=1, threshold=None):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return mean_squared_error, smaller the better\n",
        "        \"\"\"\n",
        "        if threshold:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "        return torch.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "\n",
        "class PSNR(object):\n",
        "    def __init__(self, des=\"Peak Signal to Noise Ratio\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"PSNR\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, dim=1, threshold=None):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return PSNR, larger the better\n",
        "        \"\"\"\n",
        "        if threshold:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "        mse = torch.mean((y_pred - y_true) ** 2)\n",
        "        return 10 * torch.log10(1 / mse)\n",
        "\n",
        "\n",
        "class SSIM(object):\n",
        "    '''\n",
        "    modified from https://github.com/jorge-pessoa/pytorch-msssim\n",
        "    '''\n",
        "    def __init__(self, des=\"structural similarity index\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SSIM\"\n",
        "\n",
        "    def gaussian(self, w_size, sigma):\n",
        "        gauss = torch.Tensor([math.exp(-(x - w_size//2)**2/float(2*sigma**2)) for x in range(w_size)])\n",
        "        return gauss/gauss.sum()\n",
        "\n",
        "    def create_window(self, w_size, channel=1):\n",
        "        _1D_window = self.gaussian(w_size, 1.5).unsqueeze(1)\n",
        "        _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "        window = _2D_window.expand(channel, 1, w_size, w_size).contiguous()\n",
        "        return window\n",
        "\n",
        "    def __call__(self, y_pred, y_true, w_size=11, size_average=True, full=False):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            w_size : int, default 11\n",
        "            size_average : boolean, default True\n",
        "            full : boolean, default False\n",
        "        return ssim, larger the better\n",
        "        \"\"\"\n",
        "        # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).\n",
        "        if torch.max(y_pred) > 128:\n",
        "            max_val = 255\n",
        "        else:\n",
        "            max_val = 1\n",
        "\n",
        "        if torch.min(y_pred) < -0.5:\n",
        "            min_val = -1\n",
        "        else:\n",
        "            min_val = 0\n",
        "        L = max_val - min_val\n",
        "\n",
        "        padd = 0\n",
        "        (_, channel, height, width) = y_pred.size()\n",
        "        window = self.create_window(w_size, channel=channel).to(y_pred.device)\n",
        "\n",
        "        mu1 = F.conv2d(y_pred, window, padding=padd, groups=channel)\n",
        "        mu2 = F.conv2d(y_true, window, padding=padd, groups=channel)\n",
        "\n",
        "        mu1_sq = mu1.pow(2)\n",
        "        mu2_sq = mu2.pow(2)\n",
        "        mu1_mu2 = mu1 * mu2\n",
        "\n",
        "        sigma1_sq = F.conv2d(y_pred * y_pred, window, padding=padd, groups=channel) - mu1_sq\n",
        "        sigma2_sq = F.conv2d(y_true * y_true, window, padding=padd, groups=channel) - mu2_sq\n",
        "        sigma12 = F.conv2d(y_pred * y_true, window, padding=padd, groups=channel) - mu1_mu2\n",
        "\n",
        "        C1 = (0.01 * L) ** 2\n",
        "        C2 = (0.03 * L) ** 2\n",
        "\n",
        "        v1 = 2.0 * sigma12 + C2\n",
        "        v2 = sigma1_sq + sigma2_sq + C2\n",
        "        cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
        "\n",
        "        ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
        "\n",
        "        if size_average:\n",
        "            ret = ssim_map.mean()\n",
        "        else:\n",
        "            ret = ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "        if full:\n",
        "            return ret, cs\n",
        "        return ret\n",
        "\n",
        "\n",
        "class LPIPS(object):\n",
        "    '''\n",
        "    borrowed from https://github.com/richzhang/PerceptualSimilarity\n",
        "    '''\n",
        "    def __init__(self, cuda, des=\"Learned Perceptual Image Patch Similarity\", version=\"0.1\"):\n",
        "        self.des = des\n",
        "        self.version = version\n",
        "        self.model = lpips.PerceptualLoss(model='net-lin',net='alex',use_gpu=cuda)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"LPIPS\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, normalized=True):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            normalized : change [0,1] => [-1,1] (default by LPIPS)\n",
        "        return LPIPS, smaller the better\n",
        "        \"\"\"\n",
        "        if normalized:\n",
        "            y_pred = y_pred * 2.0 - 1.0\n",
        "            y_true = y_true * 2.0 - 1.0\n",
        "        return self.model.forward(y_pred, y_true)\n",
        "\n",
        "\n",
        "class AE(object):\n",
        "    \"\"\"\n",
        "    Modified from matlab : colorangle.m, MATLAB V2019b\n",
        "    angle = acos(RGB1' * RGB2 / (norm(RGB1) * norm(RGB2)));\n",
        "    angle = 180 / pi * angle;\n",
        "    \"\"\"\n",
        "    def __init__(self, des='average Angular Error'):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AE\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "        return average AE, smaller the better\n",
        "        \"\"\"\n",
        "        dotP = torch.sum(y_pred * y_true, dim=1)\n",
        "        Norm_pred = torch.sqrt(torch.sum(y_pred * y_pred, dim=1))\n",
        "        Norm_true = torch.sqrt(torch.sum(y_true * y_true, dim=1))\n",
        "        ae = 180 / math.pi * torch.acos(dotP / (Norm_pred * Norm_true + eps))\n",
        "        return ae.mean(1).mean(1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for ch in [3, 1]:\n",
        "        batch_size, img_row, img_col = 1, 224, 224\n",
        "        y_true = torch.rand(batch_size, ch, img_row, img_col)\n",
        "        noise = torch.zeros(y_true.size()).data.normal_(0, std=0.1)\n",
        "        y_pred = y_true + noise\n",
        "        for cuda in [False, True]:\n",
        "            if cuda:\n",
        "                y_pred = y_pred.cuda()\n",
        "                y_true = y_true.cuda()\n",
        "\n",
        "            print('#'*20, 'Cuda : {} ; size : {}'.format(cuda, y_true.size()))\n",
        "            ########### similarity metrics\n",
        "            metric = MSE()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = PSNR()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = SSIM()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = LPIPS(cuda)\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = AE()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            ########### accuracy metrics\n",
        "            metric = OAAcc()\n",
        "            maccu, accu = metric(y_pred, y_true)\n",
        "            print('mAccu:', maccu, 'Accu', accu)\n",
        "\n",
        "            metric = Precision()\n",
        "            mprec, prec = metric(y_pred, y_true)\n",
        "            print('mPrec:', mprec, 'Prec', prec)\n",
        "\n",
        "            metric = Recall()\n",
        "            mreca, reca = metric(y_pred, y_true)\n",
        "            print('mReca:', mreca, 'Reca', reca)\n",
        "\n",
        "            metric = F1Score()\n",
        "            mf1sc, f1sc = metric(y_pred, y_true)\n",
        "            print('mF1sc:', mf1sc, 'F1sc', f1sc)\n",
        "\n",
        "            metric = Kappa()\n",
        "            mkapp, kapp = metric(y_pred, y_true)\n",
        "            print('mKapp:', mkapp, 'Kapp', kapp)\n",
        "\n",
        "            metric = Jaccard()\n",
        "            mjacc, jacc = metric(y_pred, y_true)\n",
        "            print('mJacc:', mjacc, 'Jacc', jacc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqpDwSEWAf0a",
        "cellView": "form"
      },
      "source": [
        "#@title loss.py\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class TVLoss(pl.LightningModule):\n",
        "    def __init__(self,TVLoss_weight=1):\n",
        "        super(TVLoss,self).__init__()\n",
        "        self.TVLoss_weight = TVLoss_weight\n",
        "\n",
        "    def forward(self,x):\n",
        "        batch_size = x.size()[0]\n",
        "        h_x = x.size()[2]\n",
        "        w_x = x.size()[3]\n",
        "        count_h = self._tensor_size(x[:,:,1:,:])\n",
        "        count_w = self._tensor_size(x[:,:,:,1:])\n",
        "        h_tv = torch.pow((x[:,:,1:,:]-x[:,:,:h_x-1,:]),2).sum()\n",
        "        w_tv = torch.pow((x[:,:,:,1:]-x[:,:,:,:w_x-1]),2).sum()\n",
        "        return self.TVLoss_weight*2*(h_tv/count_h+w_tv/count_w)/batch_size\n",
        "\n",
        "    def _tensor_size(self,t):\n",
        "        return t.size()[1]*t.size()[2]*t.size()[3]\n",
        "\n",
        "\n",
        "class hinge_loss(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(hinge_loss, self).__init__()\n",
        "\n",
        "    def forward(self, dis_fake, dis_real):\n",
        "        loss_real = torch.mean(F.relu(1. - dis_real))\n",
        "        loss_fake = torch.mean(F.relu(1. + dis_fake))\n",
        "        return loss_real + loss_fake\n",
        "\n",
        "\n",
        "class hinge_loss_G(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(hinge_loss_G, self).__init__()\n",
        "\n",
        "    def forward(self, dis_fake):\n",
        "        loss_fake = -torch.mean(dis_fake)\n",
        "        return loss_fake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZRGxo99trki"
      },
      "source": [
        "# Additional discriminators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDMnBN_8tpxx"
      },
      "source": [
        "# EfficientNet\n",
        "!pip install efficientnet_pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Gn26RbXEtywb"
      },
      "source": [
        "#@title ResNeSt.py\n",
        "#https://github.com/zhanghang1989/ResNeSt/blob/11eb547225c6b98bdf6cab774fb58dffc53362b1/resnest/torch/splat.py\n",
        "\"\"\"Split-Attention\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Conv2d, Module, Linear, BatchNorm2d, ReLU\n",
        "from torch.nn.modules.utils import _pair\n",
        "\n",
        "__all__ = ['SplAtConv2d']\n",
        "\n",
        "class SplAtConv2d(Module):\n",
        "    \"\"\"Split-Attention Conv2d\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, channels, kernel_size, stride=(1, 1), padding=(0, 0),\n",
        "                 dilation=(1, 1), groups=1, bias=True,\n",
        "                 radix=2, reduction_factor=4,\n",
        "                 rectify=False, rectify_avg=False, norm_layer=None,\n",
        "                 dropblock_prob=0.0, **kwargs):\n",
        "        super(SplAtConv2d, self).__init__()\n",
        "        padding = _pair(padding)\n",
        "        self.rectify = rectify and (padding[0] > 0 or padding[1] > 0)\n",
        "        self.rectify_avg = rectify_avg\n",
        "        inter_channels = max(in_channels*radix//reduction_factor, 32)\n",
        "        self.radix = radix\n",
        "        self.cardinality = groups\n",
        "        self.channels = channels\n",
        "        self.dropblock_prob = dropblock_prob\n",
        "        if self.rectify:\n",
        "            from rfconv import RFConv2d\n",
        "            self.conv = RFConv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n",
        "                                 groups=groups*radix, bias=bias, average_mode=rectify_avg, **kwargs)\n",
        "        else:\n",
        "            self.conv = Conv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n",
        "                               groups=groups*radix, bias=bias, **kwargs)\n",
        "        self.use_bn = norm_layer is not None\n",
        "        if self.use_bn:\n",
        "            self.bn0 = norm_layer(channels*radix)\n",
        "        self.relu = ReLU(inplace=True)\n",
        "        self.fc1 = Conv2d(channels, inter_channels, 1, groups=self.cardinality)\n",
        "        if self.use_bn:\n",
        "            self.bn1 = norm_layer(inter_channels)\n",
        "        self.fc2 = Conv2d(inter_channels, channels*radix, 1, groups=self.cardinality)\n",
        "        if dropblock_prob > 0.0:\n",
        "            self.dropblock = DropBlock2D(dropblock_prob, 3)\n",
        "        self.rsoftmax = rSoftMax(radix, groups)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.use_bn:\n",
        "            x = self.bn0(x)\n",
        "        if self.dropblock_prob > 0.0:\n",
        "            x = self.dropblock(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        batch, rchannel = x.shape[:2]\n",
        "        if self.radix > 1:\n",
        "            if torch.__version__ < '1.5':\n",
        "                splited = torch.split(x, int(rchannel//self.radix), dim=1)\n",
        "            else:\n",
        "                splited = torch.split(x, rchannel//self.radix, dim=1)\n",
        "            gap = sum(splited) \n",
        "        else:\n",
        "            gap = x\n",
        "        gap = F.adaptive_avg_pool2d(gap, 1)\n",
        "        gap = self.fc1(gap)\n",
        "\n",
        "        if self.use_bn:\n",
        "            gap = self.bn1(gap)\n",
        "        gap = self.relu(gap)\n",
        "\n",
        "        atten = self.fc2(gap)\n",
        "        atten = self.rsoftmax(atten).view(batch, -1, 1, 1)\n",
        "\n",
        "        if self.radix > 1:\n",
        "            if torch.__version__ < '1.5':\n",
        "                attens = torch.split(atten, int(rchannel//self.radix), dim=1)\n",
        "            else:\n",
        "                attens = torch.split(atten, rchannel//self.radix, dim=1)\n",
        "            out = sum([att*split for (att, split) in zip(attens, splited)])\n",
        "        else:\n",
        "            out = atten * x\n",
        "        return out.contiguous()\n",
        "\n",
        "class rSoftMax(nn.Module):\n",
        "    def __init__(self, radix, cardinality):\n",
        "        super().__init__()\n",
        "        self.radix = radix\n",
        "        self.cardinality = cardinality\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch = x.size(0)\n",
        "        if self.radix > 1:\n",
        "            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n",
        "            x = F.softmax(x, dim=1)\n",
        "            x = x.reshape(batch, -1)\n",
        "        else:\n",
        "            x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#https://github.com/zhanghang1989/ResNeSt/blob/11eb547225c6b98bdf6cab774fb58dffc53362b1/resnest/torch/resnet.py\n",
        "##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "## Created by: Hang Zhang\n",
        "## Email: zhanghang0704@gmail.com\n",
        "## Copyright (c) 2020\n",
        "##\n",
        "## LICENSE file in the root directory of this source tree \n",
        "##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\"\"\"ResNet variants\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#from .splat import SplAtConv2d\n",
        "\n",
        "__all__ = ['ResNet', 'Bottleneck']\n",
        "\n",
        "class DropBlock2D(object):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class GlobalAvgPool2d(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n",
        "        super(GlobalAvgPool2d, self).__init__()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return nn.functional.adaptive_avg_pool2d(inputs, 1).view(inputs.size(0), -1)\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    \"\"\"ResNet Bottleneck\n",
        "    \"\"\"\n",
        "    # pylint: disable=unused-argument\n",
        "    expansion = 4\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None,\n",
        "                 radix=1, cardinality=1, bottleneck_width=64,\n",
        "                 avd=False, avd_first=False, dilation=1, is_first=False,\n",
        "                 rectified_conv=False, rectify_avg=False,\n",
        "                 norm_layer=None, dropblock_prob=0.0, last_gamma=False):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        group_width = int(planes * (bottleneck_width / 64.)) * cardinality\n",
        "        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n",
        "        self.bn1 = norm_layer(group_width)\n",
        "        self.dropblock_prob = dropblock_prob\n",
        "        self.radix = radix\n",
        "        self.avd = avd and (stride > 1 or is_first)\n",
        "        self.avd_first = avd_first\n",
        "\n",
        "        if self.avd:\n",
        "            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)\n",
        "            stride = 1\n",
        "\n",
        "        if dropblock_prob > 0.0:\n",
        "            self.dropblock1 = DropBlock2D(dropblock_prob, 3)\n",
        "            if radix == 1:\n",
        "                self.dropblock2 = DropBlock2D(dropblock_prob, 3)\n",
        "            self.dropblock3 = DropBlock2D(dropblock_prob, 3)\n",
        "\n",
        "        if radix >= 1:\n",
        "            self.conv2 = SplAtConv2d(\n",
        "                group_width, group_width, kernel_size=3,\n",
        "                stride=stride, padding=dilation,\n",
        "                dilation=dilation, groups=cardinality, bias=False,\n",
        "                radix=radix, rectify=rectified_conv,\n",
        "                rectify_avg=rectify_avg,\n",
        "                norm_layer=norm_layer,\n",
        "                dropblock_prob=dropblock_prob)\n",
        "        elif rectified_conv:\n",
        "            from rfconv import RFConv2d\n",
        "            self.conv2 = RFConv2d(\n",
        "                group_width, group_width, kernel_size=3, stride=stride,\n",
        "                padding=dilation, dilation=dilation,\n",
        "                groups=cardinality, bias=False,\n",
        "                average_mode=rectify_avg)\n",
        "            self.bn2 = norm_layer(group_width)\n",
        "        else:\n",
        "            self.conv2 = nn.Conv2d(\n",
        "                group_width, group_width, kernel_size=3, stride=stride,\n",
        "                padding=dilation, dilation=dilation,\n",
        "                groups=cardinality, bias=False)\n",
        "            self.bn2 = norm_layer(group_width)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            group_width, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = norm_layer(planes*4)\n",
        "\n",
        "        if last_gamma:\n",
        "            from torch.nn.init import zeros_\n",
        "            zeros_(self.bn3.weight)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.dilation = dilation\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        if self.dropblock_prob > 0.0:\n",
        "            out = self.dropblock1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        if self.avd and self.avd_first:\n",
        "            out = self.avd_layer(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        if self.radix == 0:\n",
        "            out = self.bn2(out)\n",
        "            if self.dropblock_prob > 0.0:\n",
        "                out = self.dropblock2(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "        if self.avd and not self.avd_first:\n",
        "            out = self.avd_layer(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        if self.dropblock_prob > 0.0:\n",
        "            out = self.dropblock3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"ResNet Variants\n",
        "    Parameters\n",
        "    ----------\n",
        "    block : Block\n",
        "        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n",
        "    layers : list of int\n",
        "        Numbers of layers in each block\n",
        "    classes : int, default 1000\n",
        "        Number of classification classes.\n",
        "    dilated : bool, default False\n",
        "        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n",
        "        typically used in Semantic Segmentation.\n",
        "    norm_layer : object\n",
        "        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n",
        "        for Synchronized Cross-GPU BachNormalization).\n",
        "    Reference:\n",
        "        - He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n",
        "        - Yu, Fisher, and Vladlen Koltun. \"Multi-scale context aggregation by dilated convolutions.\"\n",
        "    \"\"\"\n",
        "    # pylint: disable=unused-variable\n",
        "    def __init__(self, block, layers, radix=1, groups=1, bottleneck_width=64,\n",
        "                 num_classes=1000, dilated=False, dilation=1,\n",
        "                 deep_stem=False, stem_width=64, avg_down=False,\n",
        "                 rectified_conv=False, rectify_avg=False,\n",
        "                 avd=False, avd_first=False,\n",
        "                 final_drop=0.0, dropblock_prob=0,\n",
        "                 last_gamma=False, norm_layer=nn.BatchNorm2d):\n",
        "        self.cardinality = groups\n",
        "        self.bottleneck_width = bottleneck_width\n",
        "        # ResNet-D params\n",
        "        self.inplanes = stem_width*2 if deep_stem else 64\n",
        "        self.avg_down = avg_down\n",
        "        self.last_gamma = last_gamma\n",
        "        # ResNeSt params\n",
        "        self.radix = radix\n",
        "        self.avd = avd\n",
        "        self.avd_first = avd_first\n",
        "\n",
        "        super(ResNet, self).__init__()\n",
        "        self.rectified_conv = rectified_conv\n",
        "        self.rectify_avg = rectify_avg\n",
        "        if rectified_conv:\n",
        "            from rfconv import RFConv2d\n",
        "            conv_layer = RFConv2d\n",
        "        else:\n",
        "            conv_layer = nn.Conv2d\n",
        "        conv_kwargs = {'average_mode': rectify_avg} if rectified_conv else {}\n",
        "        if deep_stem:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                conv_layer(3, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **conv_kwargs),\n",
        "                norm_layer(stem_width),\n",
        "                nn.ReLU(inplace=True),\n",
        "                conv_layer(stem_width, stem_width, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n",
        "                norm_layer(stem_width),\n",
        "                nn.ReLU(inplace=True),\n",
        "                conv_layer(stem_width, stem_width*2, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = conv_layer(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                                   bias=False, **conv_kwargs)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n",
        "        if dilated or dilation == 4:\n",
        "            self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n",
        "                                           dilation=2, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n",
        "                                           dilation=4, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "        elif dilation==2:\n",
        "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                           dilation=1, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n",
        "                                           dilation=2, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "        else:\n",
        "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                           norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                           norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "        self.avgpool = GlobalAvgPool2d()\n",
        "        self.drop = nn.Dropout(final_drop) if final_drop > 0.0 else None\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, norm_layer):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None,\n",
        "                    dropblock_prob=0.0, is_first=True):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            down_layers = []\n",
        "            if self.avg_down:\n",
        "                if dilation == 1:\n",
        "                    down_layers.append(nn.AvgPool2d(kernel_size=stride, stride=stride,\n",
        "                                                    ceil_mode=True, count_include_pad=False))\n",
        "                else:\n",
        "                    down_layers.append(nn.AvgPool2d(kernel_size=1, stride=1,\n",
        "                                                    ceil_mode=True, count_include_pad=False))\n",
        "                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                                             kernel_size=1, stride=1, bias=False))\n",
        "            else:\n",
        "                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                                             kernel_size=1, stride=stride, bias=False))\n",
        "            down_layers.append(norm_layer(planes * block.expansion))\n",
        "            downsample = nn.Sequential(*down_layers)\n",
        "\n",
        "        layers = []\n",
        "        if dilation == 1 or dilation == 2:\n",
        "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
        "                                radix=self.radix, cardinality=self.cardinality,\n",
        "                                bottleneck_width=self.bottleneck_width,\n",
        "                                avd=self.avd, avd_first=self.avd_first,\n",
        "                                dilation=1, is_first=is_first, rectified_conv=self.rectified_conv,\n",
        "                                rectify_avg=self.rectify_avg,\n",
        "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
        "                                last_gamma=self.last_gamma))\n",
        "        elif dilation == 4:\n",
        "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
        "                                radix=self.radix, cardinality=self.cardinality,\n",
        "                                bottleneck_width=self.bottleneck_width,\n",
        "                                avd=self.avd, avd_first=self.avd_first,\n",
        "                                dilation=2, is_first=is_first, rectified_conv=self.rectified_conv,\n",
        "                                rectify_avg=self.rectify_avg,\n",
        "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
        "                                last_gamma=self.last_gamma))\n",
        "        else:\n",
        "            raise RuntimeError(\"=> unknown dilation size: {}\".format(dilation))\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes,\n",
        "                                radix=self.radix, cardinality=self.cardinality,\n",
        "                                bottleneck_width=self.bottleneck_width,\n",
        "                                avd=self.avd, avd_first=self.avd_first,\n",
        "                                dilation=dilation, rectified_conv=self.rectified_conv,\n",
        "                                rectify_avg=self.rectify_avg,\n",
        "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
        "                                last_gamma=self.last_gamma))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        #x = x.view(x.size(0), -1)\n",
        "        x = torch.flatten(x, 1)\n",
        "        if self.drop:\n",
        "            x = self.drop(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "#https://github.com/zhanghang1989/ResNeSt/blob/master/resnest/torch/resnest.py\n",
        "import torch\n",
        "#from .resnet import ResNet, Bottleneck\n",
        "\n",
        "__all__ = ['resnest50', 'resnest101', 'resnest200', 'resnest269']\n",
        "\n",
        "_url_format = 'https://s3.us-west-1.wasabisys.com/resnest/torch/{}-{}.pth'\n",
        "\n",
        "_model_sha256 = {name: checksum for checksum, name in [\n",
        "    ('528c19ca', 'resnest50'),\n",
        "    ('22405ba7', 'resnest101'),\n",
        "    ('75117900', 'resnest200'),\n",
        "    ('0cc87c48', 'resnest269'),\n",
        "    ]}\n",
        "\n",
        "def short_hash(name):\n",
        "    if name not in _model_sha256:\n",
        "        raise ValueError('Pretrained model for {name} is not available.'.format(name=name))\n",
        "    return _model_sha256[name][:8]\n",
        "\n",
        "resnest_model_urls = {name: _url_format.format(name, short_hash(name)) for\n",
        "    name in _model_sha256.keys()\n",
        "}\n",
        "\n",
        "def resnest50(pretrained=False, root='~/.encoding/models', **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3],\n",
        "                   radix=2, groups=1, bottleneck_width=64,\n",
        "                   deep_stem=True, stem_width=32, avg_down=True,\n",
        "                   avd=True, avd_first=False, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(torch.hub.load_state_dict_from_url(\n",
        "            resnest_model_urls['resnest50'], progress=True, check_hash=True))\n",
        "    return model\n",
        "\n",
        "def resnest101(pretrained=False, root='~/.encoding/models', **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3],\n",
        "                   radix=2, groups=1, bottleneck_width=64,\n",
        "                   deep_stem=True, stem_width=64, avg_down=True,\n",
        "                   avd=True, avd_first=False, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(torch.hub.load_state_dict_from_url(\n",
        "            resnest_model_urls['resnest101'], progress=True, check_hash=True))\n",
        "    return model\n",
        "\n",
        "def resnest200(pretrained=False, root='~/.encoding/models', **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 24, 36, 3],\n",
        "                   radix=2, groups=1, bottleneck_width=64,\n",
        "                   deep_stem=True, stem_width=64, avg_down=True,\n",
        "                   avd=True, avd_first=False, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(torch.hub.load_state_dict_from_url(\n",
        "            resnest_model_urls['resnest200'], progress=True, check_hash=True))\n",
        "    return model\n",
        "\n",
        "def resnest269(pretrained=False, root='~/.encoding/models', **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 30, 48, 8],\n",
        "                   radix=2, groups=1, bottleneck_width=64,\n",
        "                   deep_stem=True, stem_width=64, avg_down=True,\n",
        "                   avd=True, avd_first=False, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(torch.hub.load_state_dict_from_url(\n",
        "            resnest_model_urls['resnest269'], progress=True, check_hash=True))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dURET3riP8W7"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "pLvrz8BPuphd"
      },
      "source": [
        "#@title block.py\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from models.modules.architectures.convolutions.partialconv2d import PartialConv2d #TODO\n",
        "#from models.modules.architectures.convolutions.deformconv2d import DeformConv2d\n",
        "#from models.networks import weights_init_normal, weights_init_xavier, weights_init_kaiming, weights_init_orthogonal\n",
        "\n",
        "\n",
        "####################\n",
        "# Basic blocks\n",
        "####################\n",
        "\n",
        "# Swish activation funtion\n",
        "def swish_func(x, beta=1.0):\n",
        "    \"\"\"\n",
        "    \"Swish: a Self-Gated Activation Function\"\n",
        "    Searching for Activation Functions (https://arxiv.org/abs/1710.05941)\n",
        "    \n",
        "    If beta=1 applies the Sigmoid Linear Unit (SiLU) function element-wise\n",
        "    If beta=0, Swish becomes the scaled linear function (identity \n",
        "      activation) f(x) = x/2\n",
        "    As beta -> , the sigmoid component converges to approach a 0-1 function\n",
        "      (unit step), and multiplying that by x gives us f(x)=2max(0,x), which \n",
        "      is the ReLU multiplied by a constant factor of 2, so Swish becomes like \n",
        "      the ReLU function.\n",
        "    \n",
        "    Including beta, Swish can be loosely viewed as a smooth function that \n",
        "      nonlinearly interpolate between identity (linear) and ReLU function.\n",
        "      The degree of interpolation can be controlled by the model if beta is \n",
        "      set as a trainable parameter.\n",
        "    \n",
        "    Alt: 1.78718727865 * (x * sigmoid(x) - 0.20662096414)\n",
        "    \"\"\"\n",
        "    \n",
        "    # In-place implementation, may consume less GPU memory: \n",
        "    \"\"\" \n",
        "    result = x.clone()\n",
        "    torch.sigmoid_(beta*x)\n",
        "    x *= result\n",
        "    return x\n",
        "    #\"\"\"\n",
        "    \n",
        "    # Normal out-of-place implementation:\n",
        "    #\"\"\"\n",
        "    return x * torch.sigmoid(beta * x)\n",
        "    #\"\"\"\n",
        "    \n",
        "# Swish module\n",
        "class Swish(nn.Module):\n",
        "    \n",
        "    __constants__ = ['beta', 'slope', 'inplace']\n",
        "    \n",
        "    def __init__(self, beta=1.0, slope=1.67653251702, inplace=False):\n",
        "        \"\"\"\n",
        "        Shape:\n",
        "        - Input: (N, *) where * means, any number of additional\n",
        "          dimensions\n",
        "        - Output: (N, *), same shape as the input\n",
        "        \"\"\"\n",
        "        super(Swish).__init__()\n",
        "        self.inplace = inplace\n",
        "        # self.beta = beta # user-defined beta parameter, non-trainable\n",
        "        # self.beta = beta * torch.nn.Parameter(torch.ones(1)) # learnable beta parameter, create a tensor out of beta\n",
        "        self.beta = torch.nn.Parameter(torch.tensor(beta)) # learnable beta parameter, create a tensor out of beta\n",
        "        self.beta.requiresGrad = True # set requiresGrad to true to make it trainable\n",
        "\n",
        "        self.slope = slope / 2 # user-defined \"slope\", non-trainable\n",
        "        # self.slope = slope * torch.nn.Parameter(torch.ones(1)) # learnable slope parameter, create a tensor out of slope\n",
        "        # self.slope = torch.nn.Parameter(torch.tensor(slope)) # learnable slope parameter, create a tensor out of slope\n",
        "        # self.slope.requiresGrad = True # set requiresGrad to true to true to make it trainable\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        # Disabled, using inplace causes:\n",
        "        # \"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\"\n",
        "        if self.inplace:\n",
        "            input.mul_(torch.sigmoid(self.beta*input))\n",
        "            return 2 * self.slope * input\n",
        "        else:\n",
        "            return 2 * self.slope * swish_func(input, self.beta)\n",
        "        \"\"\"\n",
        "        return 2 * self.slope * swish_func(input, self.beta)\n",
        "\n",
        "\n",
        "def act(act_type, inplace=True, neg_slope=0.2, n_prelu=1, beta=1.0):\n",
        "    # helper selecting activation\n",
        "    # neg_slope: for leakyrelu and init of prelu\n",
        "    # n_prelu: for p_relu num_parameters\n",
        "    # beta: for swish\n",
        "    act_type = act_type.lower()\n",
        "    if act_type == 'relu':\n",
        "        layer = nn.ReLU(inplace)\n",
        "    elif act_type == 'leakyrelu' or act_type == 'lrelu':\n",
        "        layer = nn.LeakyReLU(neg_slope, inplace)\n",
        "    elif act_type == 'prelu':\n",
        "        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n",
        "    elif act_type == 'Tanh' or act_type == 'tanh':  # [-1, 1] range output\n",
        "        layer = nn.Tanh()\n",
        "    elif act_type == 'sigmoid':  # [0, 1] range output\n",
        "        layer = nn.Sigmoid()\n",
        "    elif act_type == 'swish':\n",
        "        layer = Swish(beta=beta, inplace=inplace)\n",
        "    else:\n",
        "        raise NotImplementedError('activation layer [{:s}] is not found'.format(act_type))\n",
        "    return layer\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self, *kwargs):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x, *kwargs):\n",
        "        return x\n",
        "\n",
        "\n",
        "def norm(norm_type, nc):\n",
        "    \"\"\"Return a normalization layer\n",
        "    Parameters:\n",
        "        norm_type (str) -- the name of the normalization layer: batch | instance | none\n",
        "    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n",
        "    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n",
        "    \"\"\"\n",
        "    norm_type = norm_type.lower()\n",
        "    if norm_type == 'batch':\n",
        "        layer = nn.BatchNorm2d(nc, affine=True)\n",
        "        # norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
        "    elif norm_type == 'instance':\n",
        "        layer = nn.InstanceNorm2d(nc, affine=False)\n",
        "        # norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
        "    # elif norm_type == 'layer':\n",
        "    #     return lambda num_features: nn.GroupNorm(1, num_features)\n",
        "    elif norm_type == 'none':\n",
        "        def norm_layer(x): return Identity()\n",
        "    else:\n",
        "        raise NotImplementedError('normalization layer [{:s}] is not found'.format(norm_type))\n",
        "    return layer\n",
        "\n",
        "\n",
        "def add_spectral_norm(module, use_spectral_norm=False):\n",
        "    \"\"\" Add spectral norm to any module passed if use_spectral_norm = True,\n",
        "    else, returns the original module without change\n",
        "    \"\"\"\n",
        "    if use_spectral_norm:\n",
        "        return nn.utils.spectral_norm(module)\n",
        "    return module\n",
        "\n",
        "\n",
        "def pad(pad_type, padding):\n",
        "    \"\"\"\n",
        "    helper selecting padding layer\n",
        "    if padding is 'zero', can be done with conv layers\n",
        "    \"\"\"\n",
        "    pad_type = pad_type.lower()\n",
        "    if padding == 0:\n",
        "        return None\n",
        "    if pad_type == 'reflect':\n",
        "        layer = nn.ReflectionPad2d(padding)\n",
        "    elif pad_type == 'replicate':\n",
        "        layer = nn.ReplicationPad2d(padding)\n",
        "    elif pad_type == 'zero':\n",
        "        layer = nn.ZeroPad2d(padding)\n",
        "    else:\n",
        "        raise NotImplementedError('padding layer [{:s}] is not implemented'.format(pad_type))\n",
        "    return layer\n",
        "\n",
        "\n",
        "def get_valid_padding(kernel_size, dilation):\n",
        "    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
        "    padding = (kernel_size - 1) // 2\n",
        "    return padding\n",
        "\n",
        "\n",
        "class ConcatBlock(nn.Module):\n",
        "    # Concat the output of a submodule to its input\n",
        "    def __init__(self, submodule):\n",
        "        super(ConcatBlock, self).__init__()\n",
        "        self.sub = submodule\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = torch.cat((x, self.sub(x)), dim=1)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'Identity .. \\n|' + self.sub.__repr__().replace('\\n', '\\n|')\n",
        "\n",
        "\n",
        "class ShortcutBlock(nn.Module):\n",
        "    # Elementwise sum the output of a submodule to its input\n",
        "    def __init__(self, submodule):\n",
        "        super(ShortcutBlock, self).__init__()\n",
        "        self.sub = submodule\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x + self.sub(x)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'Identity + \\n|' + self.sub.__repr__().replace('\\n', '\\n|')\n",
        "\n",
        "\n",
        "def sequential(*args):\n",
        "    # Flatten Sequential. It unwraps nn.Sequential.\n",
        "    if len(args) == 1:\n",
        "        if isinstance(args[0], OrderedDict):\n",
        "            raise NotImplementedError('sequential does not support OrderedDict input.')\n",
        "        return args[0]  # No sequential is needed.\n",
        "    modules = []\n",
        "    for module in args:\n",
        "        if isinstance(module, nn.Sequential):\n",
        "            for submodule in module.children():\n",
        "                modules.append(submodule)\n",
        "        elif isinstance(module, nn.Module):\n",
        "            modules.append(module)\n",
        "    return nn.Sequential(*modules)\n",
        "\n",
        "\n",
        "def conv_block(in_nc, out_nc, kernel_size, stride=1, dilation=1, groups=1, bias=True, \\\n",
        "               pad_type='zero', norm_type=None, act_type='relu', mode='CNA', convtype='Conv2D', \\\n",
        "               spectral_norm=False):\n",
        "    \"\"\"\n",
        "    Conv layer with padding, normalization, activation\n",
        "    mode: CNA --> Conv -> Norm -> Act\n",
        "        NAC --> Norm -> Act --> Conv (Identity Mappings in Deep Residual Networks, ECCV16)\n",
        "    \"\"\"\n",
        "    assert mode in ['CNA', 'NAC', 'CNAC'], 'Wrong conv mode [{:s}]'.format(mode)\n",
        "    padding = get_valid_padding(kernel_size, dilation)\n",
        "    p = pad(pad_type, padding) if pad_type and pad_type != 'zero' else None\n",
        "    padding = padding if pad_type == 'zero' else 0\n",
        "    \n",
        "    if convtype=='PartialConv2D':\n",
        "        c = PartialConv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, \\\n",
        "               dilation=dilation, bias=bias, groups=groups)\n",
        "    elif convtype=='DeformConv2D':\n",
        "        c = DeformConv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, \\\n",
        "               dilation=dilation, bias=bias, groups=groups)\n",
        "    elif convtype=='Conv3D':\n",
        "        c = nn.Conv3d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, \\\n",
        "                dilation=dilation, bias=bias, groups=groups)\n",
        "    else: #default case is standard 'Conv2D':\n",
        "        c = nn.Conv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, \\\n",
        "                dilation=dilation, bias=bias, groups=groups) #normal conv2d\n",
        "            \n",
        "    if spectral_norm:\n",
        "        c = nn.utils.spectral_norm(c)\n",
        "    \n",
        "    a = act(act_type) if act_type else None\n",
        "    if 'CNA' in mode:\n",
        "        n = norm(norm_type, out_nc) if norm_type else None\n",
        "        return sequential(p, c, n, a)\n",
        "    elif mode == 'NAC':\n",
        "        if norm_type is None and act_type is not None:\n",
        "            a = act(act_type, inplace=False)\n",
        "            # Important!\n",
        "            # input----ReLU(inplace)----Conv--+----output\n",
        "            #        |________________________|\n",
        "            # inplace ReLU will modify the input, therefore wrong output\n",
        "        n = norm(norm_type, in_nc) if norm_type else None\n",
        "        return sequential(n, a, p, c)\n",
        "\n",
        "\n",
        "def make_layer(basic_block, num_basic_block, **kwarg):\n",
        "    \"\"\"Make layers by stacking the same blocks.\n",
        "    Args:\n",
        "        basic_block (nn.module): nn.module class for basic block. (block)\n",
        "        num_basic_block (int): number of blocks. (n_layers)\n",
        "    Returns:\n",
        "        nn.Sequential: Stacked blocks in nn.Sequential.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    for _ in range(num_basic_block):\n",
        "        layers.append(basic_block(**kwarg))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class Mean(nn.Module):\n",
        "  def __init__(self, dim: list, keepdim=False):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "    self.keepdim = keepdim\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.mean(x, self.dim, self.keepdim)\n",
        "\n",
        "\n",
        "####################\n",
        "# initialize modules\n",
        "####################\n",
        "\n",
        "@torch.no_grad()\n",
        "def default_init_weights(module_list, init_type='kaiming', scale=1, bias_fill=0, **kwargs):\n",
        "    \"\"\"Initialize network weights.\n",
        "    Args:\n",
        "        module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n",
        "        init_type (str): the type of initialization in: 'normal', 'kaiming' \n",
        "            or 'orthogonal'\n",
        "        scale (float): Scale initialized weights, especially for residual\n",
        "            blocks. Default: 1. (for 'kaiming')\n",
        "        bias_fill (float): The value to fill bias. Default: 0\n",
        "        kwargs (dict): Other arguments for initialization function:\n",
        "            mean and/or std for 'normal'.\n",
        "            a and/or mode for 'kaiming'\n",
        "            gain for 'orthogonal' and xavier\n",
        "    \"\"\"\n",
        "    \n",
        "    # TODO\n",
        "    # logger.info('Initialization method [{:s}]'.format(init_type))\n",
        "    if not isinstance(module_list, list):\n",
        "        module_list = [module_list]\n",
        "    for module in module_list:\n",
        "        for m in module.modules():\n",
        "            if init_type == 'normal':\n",
        "                weights_init_normal(m, bias_fill=bias_fill, **kwargs)\n",
        "            if init_type == 'xavier':\n",
        "                weights_init_xavier(m, scale=scale, bias_fill=bias_fill, **kwargs)    \n",
        "            elif init_type == 'kaiming':\n",
        "                weights_init_kaiming(m, scale=scale, bias_fill=bias_fill, **kwargs)\n",
        "            elif init_type == 'orthogonal':\n",
        "                weights_init_orthogonal(m, bias_fill=bias_fill)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [{:s}] not implemented'.format(init_type))\n",
        "\n",
        "\n",
        "\n",
        "####################\n",
        "# Upsampler\n",
        "####################\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    r\"\"\"Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.\n",
        "\n",
        "    The input data is assumed to be of the form\n",
        "    `minibatch x channels x [optional depth] x [optional height] x width`.\n",
        "\n",
        "    Args:\n",
        "        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], optional):\n",
        "            output spatial sizes\n",
        "        scale_factor (float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float], optional):\n",
        "            multiplier for spatial size. Has to match input size if it is a tuple.\n",
        "        mode (str, optional): the upsampling algorithm: one of ``'nearest'``,\n",
        "            ``'linear'``, ``'bilinear'``, ``'bicubic'`` and ``'trilinear'``.\n",
        "            Default: ``'nearest'``\n",
        "        align_corners (bool, optional): if ``True``, the corner pixels of the input\n",
        "            and output tensors are aligned, and thus preserving the values at\n",
        "            those pixels. This only has effect when :attr:`mode` is\n",
        "            ``'linear'``, ``'bilinear'``, or ``'trilinear'``. Default: ``False``\n",
        "    \"\"\"\n",
        "    # To prevent warning: nn.Upsample is deprecated\n",
        "    # https://discuss.pytorch.org/t/which-function-is-better-for-upsampling-upsampling-or-interpolate/21811/8\n",
        "    # From: https://pytorch.org/docs/stable/_modules/torch/nn/modules/upsampling.html#Upsample\n",
        "    # Alternative: https://discuss.pytorch.org/t/using-nn-function-interpolate-inside-nn-sequential/23588/2?u=ptrblck\n",
        "    \n",
        "    def __init__(self, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
        "        super(Upsample, self).__init__()\n",
        "        if isinstance(scale_factor, tuple):\n",
        "            self.scale_factor = tuple(float(factor) for factor in scale_factor)\n",
        "        else:\n",
        "            self.scale_factor = float(scale_factor) if scale_factor else None\n",
        "        self.mode = mode\n",
        "        self.size = size\n",
        "        self.align_corners = align_corners\n",
        "        # self.interp = nn.functional.interpolate\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return nn.functional.interpolate(x, size=self.size, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n",
        "        # return self.interp(x, size=self.size, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n",
        "    \n",
        "    def extra_repr(self):\n",
        "        if self.scale_factor is not None:\n",
        "            info = 'scale_factor=' + str(self.scale_factor)\n",
        "        else:\n",
        "            info = 'size=' + str(self.size)\n",
        "        info += ', mode=' + self.mode\n",
        "        return info\n",
        "\n",
        "def pixelshuffle_block(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1, bias=True, \\\n",
        "                        pad_type='zero', norm_type=None, act_type='relu', convtype='Conv2D'):\n",
        "    \"\"\"\n",
        "    Pixel shuffle layer\n",
        "    (Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional\n",
        "    Neural Network, CVPR17)\n",
        "    \"\"\"\n",
        "    conv = conv_block(in_nc, out_nc * (upscale_factor ** 2), kernel_size, stride, bias=bias, \\\n",
        "                        pad_type=pad_type, norm_type=None, act_type=None, convtype=convtype)\n",
        "    pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
        "\n",
        "    n = norm(norm_type, out_nc) if norm_type else None\n",
        "    a = act(act_type) if act_type else None\n",
        "    return sequential(conv, pixel_shuffle, n, a)\n",
        "\n",
        "def upconv_block(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1, bias=True, \\\n",
        "                pad_type='zero', norm_type=None, act_type='relu', mode='nearest', convtype='Conv2D'):\n",
        "    \"\"\"\n",
        "    Upconv layer described in https://distill.pub/2016/deconv-checkerboard/\n",
        "    Example to replace deconvolutions: \n",
        "        - from: nn.ConvTranspose2d(in_nc, out_nc, kernel_size=4, stride=2, padding=1)\n",
        "        - to: upconv_block(in_nc, out_nc,kernel_size=3, stride=1, act_type=None)\n",
        "    \"\"\"\n",
        "    # upsample = nn.Upsample(scale_factor=upscale_factor, mode=mode)\n",
        "    upscale_factor = (1, upscale_factor, upscale_factor) if convtype == 'Conv3D' else upscale_factor\n",
        "    upsample = Upsample(scale_factor=upscale_factor, mode=mode) #Updated to prevent the \"nn.Upsample is deprecated\" Warning\n",
        "    conv = conv_block(in_nc, out_nc, kernel_size, stride, bias=bias, \\\n",
        "                        pad_type=pad_type, norm_type=norm_type, act_type=act_type, convtype=convtype)\n",
        "    return sequential(upsample, conv)\n",
        "\n",
        "# PPON\n",
        "def conv_layer(in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1):\n",
        "    padding = int((kernel_size - 1) / 2) * dilation\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding, bias=True, dilation=dilation, groups=groups)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################\n",
        "# ESRGANplus\n",
        "####################\n",
        "\n",
        "class GaussianNoise(nn.Module):\n",
        "    def __init__(self, sigma=0.1, is_relative_detach=False):\n",
        "        super().__init__()\n",
        "        self.sigma = sigma\n",
        "        self.is_relative_detach = is_relative_detach\n",
        "        self.noise = torch.tensor(0, dtype=torch.float).to(torch.device('cuda'))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training and self.sigma != 0:\n",
        "            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n",
        "            sampled_noise = self.noise.repeat(*x.size()).normal_() * scale\n",
        "            x = x + sampled_noise\n",
        "        return x \n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "# TODO: Not used:\n",
        "# https://github.com/github-pengge/PyTorch-progressive_growing_of_gans/blob/master/models/base_model.py\n",
        "class minibatch_std_concat_layer(nn.Module):\n",
        "    def __init__(self, averaging='all'):\n",
        "        super(minibatch_std_concat_layer, self).__init__()\n",
        "        self.averaging = averaging.lower()\n",
        "        if 'group' in self.averaging:\n",
        "            self.n = int(self.averaging[5:])\n",
        "        else:\n",
        "            assert self.averaging in ['all', 'flat', 'spatial', 'none', 'gpool'], 'Invalid averaging mode'%self.averaging\n",
        "        self.adjusted_std = lambda x, **kwargs: torch.sqrt(torch.mean((x - torch.mean(x, **kwargs)) ** 2, **kwargs) + 1e-8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        shape = list(x.size())\n",
        "        target_shape = copy.deepcopy(shape)\n",
        "        vals = self.adjusted_std(x, dim=0, keepdim=True)\n",
        "        if self.averaging == 'all':\n",
        "            target_shape[1] = 1\n",
        "            vals = torch.mean(vals, dim=1, keepdim=True)\n",
        "        elif self.averaging == 'spatial':\n",
        "            if len(shape) == 4:\n",
        "                vals = mean(vals, axis=[2,3], keepdim=True)             # torch.mean(torch.mean(vals, 2, keepdim=True), 3, keepdim=True)\n",
        "        elif self.averaging == 'none':\n",
        "            target_shape = [target_shape[0]] + [s for s in target_shape[1:]]\n",
        "        elif self.averaging == 'gpool':\n",
        "            if len(shape) == 4:\n",
        "                vals = mean(x, [0,2,3], keepdim=True)                   # torch.mean(torch.mean(torch.mean(x, 2, keepdim=True), 3, keepdim=True), 0, keepdim=True)\n",
        "        elif self.averaging == 'flat':\n",
        "            target_shape[1] = 1\n",
        "            vals = torch.FloatTensor([self.adjusted_std(x)])\n",
        "        else:                                                           # self.averaging == 'group'\n",
        "            target_shape[1] = self.n\n",
        "            vals = vals.view(self.n, self.shape[1]/self.n, self.shape[2], self.shape[3])\n",
        "            vals = mean(vals, axis=0, keepdim=True).view(1, self.n, 1, 1)\n",
        "        vals = vals.expand(*target_shape)\n",
        "        return torch.cat([x, vals], 1)\n",
        "\n",
        "\n",
        "####################\n",
        "# Useful blocks\n",
        "####################\n",
        "\n",
        "class SelfAttentionBlock(nn.Module):\n",
        "    \"\"\" \n",
        "        Implementation of Self attention Block according to paper \n",
        "        'Self-Attention Generative Adversarial Networks' (https://arxiv.org/abs/1805.08318)\n",
        "        Flexible Self Attention (FSA) layer according to paper\n",
        "        Efficient Super Resolution For Large-Scale Images Using Attentional GAN (https://arxiv.org/pdf/1812.04821.pdf)\n",
        "          The FSA layer borrows the self attention layer from SAGAN, \n",
        "          and wraps it with a max-pooling layer to reduce the size \n",
        "          of the feature maps and enable large-size images to fit in memory.\n",
        "        Used in Generator and Discriminator Networks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, max_pool=False, poolsize = 4, spectral_norm=False, ret_attention=False): #in_dim = in_feature_maps\n",
        "        super(SelfAttentionBlock,self).__init__()\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.max_pool = max_pool\n",
        "        self.poolsize = poolsize\n",
        "        self.ret_attention = ret_attention\n",
        "        \n",
        "        if self.max_pool:\n",
        "            self.pooled = nn.MaxPool2d(kernel_size=self.poolsize, stride=self.poolsize) #kernel_size=4, stride=4\n",
        "            # Note: can test using strided convolutions instead of MaxPool2d! :\n",
        "            #upsample_block_num = int(math.log(scale_factor, 2))\n",
        "            #self.pooled = nn.Conv2d .... strided conv\n",
        "            # upsample_o = [UpconvBlock(in_channels=in_dim, out_channels=in_dim, upscale_factor=2, mode='bilinear', act_type='leakyrelu') for _ in range(upsample_block_num)]\n",
        "            ## upsample_o.append(nn.Conv2d(nf, in_nc, kernel_size=9, stride=1, padding=4))\n",
        "            ## self.upsample_o = nn.Sequential(*upsample_o)\n",
        "\n",
        "            # self.upsample_o = B.Upsample(scale_factor=self.poolsize, mode='bilinear', align_corners=False) \n",
        "            \n",
        "        self.conv_f = add_spectral_norm(\n",
        "            nn.Conv1d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1, padding = 0), \n",
        "            use_spectral_norm=spectral_norm) #query_conv \n",
        "        self.conv_g = add_spectral_norm(\n",
        "            nn.Conv1d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1, padding = 0), \n",
        "            use_spectral_norm=spectral_norm) #key_conv \n",
        "        self.conv_h = add_spectral_norm(\n",
        "            nn.Conv1d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1, padding = 0), \n",
        "            use_spectral_norm=spectral_norm) #value_conv \n",
        "\n",
        "        self.gamma = nn.Parameter(torch.zeros(1)) # Trainable interpolation parameter\n",
        "        self.softmax  = nn.Softmax(dim = -1)\n",
        "        \n",
        "    def forward(self,input):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                input : input feature maps( B X C X W X H)\n",
        "            returns :\n",
        "                out : self attention value + input feature \n",
        "                attention: B X N X N (N is Width*Height)\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.max_pool: #Downscale with Max Pool\n",
        "            x = self.pooled(input)\n",
        "        else:\n",
        "            x = input\n",
        "            \n",
        "        batch_size, C, width, height = x.size()\n",
        "        \n",
        "        N = width * height\n",
        "        x = x.view(batch_size, -1, N)\n",
        "        f = self.conv_f(x) #proj_query  # B X CX(N)\n",
        "        g = self.conv_g(x) #proj_key    # B X C x (*W*H)\n",
        "        h = self.conv_h(x) #proj_value  # B X C X N\n",
        "\n",
        "        s = torch.bmm(f.permute(0, 2, 1), g) # energy, transpose check\n",
        "        # get probabilities\n",
        "        attention = self.softmax(s) #beta #attention # BX (N) X (N) \n",
        "        \n",
        "        out = torch.bmm(h, attention.permute(0,2,1))\n",
        "        out = out.view(batch_size, C, width, height) \n",
        "        \n",
        "        if self.max_pool: #Upscale to original size\n",
        "            # out = self.upsample_o(out)\n",
        "            out = Upsample(size=(input.shape[2],input.shape[3]), mode='bicubic', align_corners=False)(out) #bicubic (PyTorch > 1.0) | bilinear others.\n",
        "        \n",
        "        out = self.gamma*out + input #Add original input\n",
        "        \n",
        "        if self.ret_attention:\n",
        "            return out, attention\n",
        "        else:\n",
        "            return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "fjpb2P8wumFA"
      },
      "source": [
        "#@title spectral_norm.py\n",
        "\"\"\"\n",
        "spectral_norm.py (12-2-20)\n",
        "https://github.com/victorca25/BasicSR/blob/master/codes/models/modules/spectral_norm.py\n",
        "\"\"\"\n",
        "'''\n",
        "Copy from pytorch github repo\n",
        "Spectral Normalization from https://arxiv.org/abs/1802.05957\n",
        "'''\n",
        "import torch\n",
        "from torch.nn.functional import normalize\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "class SpectralNorm(object):\n",
        "    def __init__(self, name='weight', n_power_iterations=1, dim=0, eps=1e-12):\n",
        "        self.name = name\n",
        "        self.dim = dim\n",
        "        if n_power_iterations <= 0:\n",
        "            raise ValueError('Expected n_power_iterations to be positive, but '\n",
        "                             'got n_power_iterations={}'.format(n_power_iterations))\n",
        "        self.n_power_iterations = n_power_iterations\n",
        "        self.eps = eps\n",
        "\n",
        "    def compute_weight(self, module):\n",
        "        weight = getattr(module, self.name + '_orig')\n",
        "        u = getattr(module, self.name + '_u')\n",
        "        weight_mat = weight\n",
        "        if self.dim != 0:\n",
        "            # permute dim to front\n",
        "            weight_mat = weight_mat.permute(self.dim,\n",
        "                                            *[d for d in range(weight_mat.dim()) if d != self.dim])\n",
        "        height = weight_mat.size(0)\n",
        "        weight_mat = weight_mat.reshape(height, -1)\n",
        "        with torch.no_grad():\n",
        "            for _ in range(self.n_power_iterations):\n",
        "                # Spectral norm of weight equals to `u^T W v`, where `u` and `v`\n",
        "                # are the first left and right singular vectors.\n",
        "                # This power iteration produces approximations of `u` and `v`.\n",
        "                v = normalize(torch.matmul(weight_mat.t(), u), dim=0, eps=self.eps)\n",
        "                u = normalize(torch.matmul(weight_mat, v), dim=0, eps=self.eps)\n",
        "\n",
        "        sigma = torch.dot(u, torch.matmul(weight_mat, v))\n",
        "        weight = weight / sigma\n",
        "        return weight, u\n",
        "\n",
        "    def remove(self, module):\n",
        "        weight = getattr(module, self.name)\n",
        "        delattr(module, self.name)\n",
        "        delattr(module, self.name + '_u')\n",
        "        delattr(module, self.name + '_orig')\n",
        "        module.register_parameter(self.name, torch.nn.Parameter(weight))\n",
        "\n",
        "    def __call__(self, module, inputs):\n",
        "        if module.training:\n",
        "            weight, u = self.compute_weight(module)\n",
        "            setattr(module, self.name, weight)\n",
        "            setattr(module, self.name + '_u', u)\n",
        "        else:\n",
        "            r_g = getattr(module, self.name + '_orig').requires_grad\n",
        "            getattr(module, self.name).detach_().requires_grad_(r_g)\n",
        "\n",
        "    @staticmethod\n",
        "    def apply(module, name, n_power_iterations, dim, eps):\n",
        "        fn = SpectralNorm(name, n_power_iterations, dim, eps)\n",
        "        weight = module._parameters[name]\n",
        "        height = weight.size(dim)\n",
        "\n",
        "        u = normalize(weight.new_empty(height).normal_(0, 1), dim=0, eps=fn.eps)\n",
        "        delattr(module, fn.name)\n",
        "        module.register_parameter(fn.name + \"_orig\", weight)\n",
        "        # We still need to assign weight back as fn.name because all sorts of\n",
        "        # things may assume that it exists, e.g., when initializing weights.\n",
        "        # However, we can't directly assign as it could be an nn.Parameter and\n",
        "        # gets added as a parameter. Instead, we register weight.data as a\n",
        "        # buffer, which will cause weight to be included in the state dict\n",
        "        # and also supports nn.init due to shared storage.\n",
        "        module.register_buffer(fn.name, weight.data)\n",
        "        module.register_buffer(fn.name + \"_u\", u)\n",
        "\n",
        "        module.register_forward_pre_hook(fn)\n",
        "        return fn\n",
        "\n",
        "\n",
        "def spectral_norm(module, name='weight', n_power_iterations=1, eps=1e-12, dim=None):\n",
        "    r\"\"\"Applies spectral normalization to a parameter in the given module.\n",
        "\n",
        "    .. math::\n",
        "         \\mathbf{W} &= \\dfrac{\\mathbf{W}}{\\sigma(\\mathbf{W})} \\\\\n",
        "         \\sigma(\\mathbf{W}) &= \\max_{\\mathbf{h}: \\mathbf{h} \\ne 0} \\dfrac{\\|\\mathbf{W} \\mathbf{h}\\|_2}{\\|\\mathbf{h}\\|_2}\n",
        "\n",
        "    Spectral normalization stabilizes the training of discriminators (critics)\n",
        "    in Generaive Adversarial Networks (GANs) by rescaling the weight tensor\n",
        "    with spectral norm :math:`\\sigma` of the weight matrix calculated using\n",
        "    power iteration method. If the dimension of the weight tensor is greater\n",
        "    than 2, it is reshaped to 2D in power iteration method to get spectral\n",
        "    norm. This is implemented via a hook that calculates spectral norm and\n",
        "    rescales weight before every :meth:`~Module.forward` call.\n",
        "\n",
        "    See `Spectral Normalization for Generative Adversarial Networks`_ .\n",
        "\n",
        "    .. _`Spectral Normalization for Generative Adversarial Networks`: https://arxiv.org/abs/1802.05957\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): containing module\n",
        "        name (str, optional): name of weight parameter\n",
        "        n_power_iterations (int, optional): number of power iterations to\n",
        "            calculate spectal norm\n",
        "        eps (float, optional): epsilon for numerical stability in\n",
        "            calculating norms\n",
        "        dim (int, optional): dimension corresponding to number of outputs,\n",
        "            the default is 0, except for modules that are instances of\n",
        "            ConvTranspose1/2/3d, when it is 1\n",
        "\n",
        "    Returns:\n",
        "        The original module with the spectal norm hook\n",
        "\n",
        "    Example::\n",
        "\n",
        "        >>> m = spectral_norm(nn.Linear(20, 40))\n",
        "        Linear (20 -> 40)\n",
        "        >>> m.weight_u.size()\n",
        "        torch.Size([20])\n",
        "\n",
        "    \"\"\"\n",
        "    if dim is None:\n",
        "        if isinstance(\n",
        "                module,\n",
        "            (torch.nn.ConvTranspose1d, torch.nn.ConvTranspose2d, torch.nn.ConvTranspose3d)):\n",
        "            dim = 1\n",
        "        else:\n",
        "            dim = 0\n",
        "    SpectralNorm.apply(module, name, n_power_iterations, dim, eps)\n",
        "    return module\n",
        "\n",
        "\n",
        "def remove_spectral_norm(module, name='weight'):\n",
        "    r\"\"\"Removes the spectral normalization reparameterization from a module.\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): containing module\n",
        "        name (str, optional): name of weight parameter\n",
        "\n",
        "    Example:\n",
        "        >>> m = spectral_norm(nn.Linear(40, 10))\n",
        "        >>> remove_spectral_norm(m)\n",
        "    \"\"\"\n",
        "    for k, hook in module._forward_pre_hooks.items():\n",
        "        if isinstance(hook, SpectralNorm) and hook.name == name:\n",
        "            hook.remove(module)\n",
        "            del module._forward_pre_hooks[k]\n",
        "            return module\n",
        "\n",
        "    raise ValueError(\"spectral_norm of '{}' not found in {}\".format(name, module))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "EEqSzXtTPWKM"
      },
      "source": [
        "#@title discriminator.py\n",
        "\"\"\"\n",
        "discriminators.py (12-2-20)\n",
        "https://github.com/victorca25/BasicSR/blob/master/codes/models/modules/architectures/discriminators.py\n",
        "\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "#from . import block as B\n",
        "from torch.nn.utils import spectral_norm as SN\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "\n",
        "####################\n",
        "# Discriminator\n",
        "####################\n",
        "\n",
        "\n",
        "# VGG style Discriminator\n",
        "class Discriminator_VGG(pl.LightningModule):\n",
        "    def __init__(self, size, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG, self).__init__()\n",
        "\n",
        "        conv_blocks = []\n",
        "        conv_blocks.append(conv_block(  in_nc, base_nf, kernel_size=3, stride=1, norm_type=None, \\\n",
        "            act_type=act_type, mode=mode))\n",
        "        conv_blocks.append(conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode))\n",
        "\n",
        "        cur_size = size // 2\n",
        "        cur_nc = base_nf\n",
        "        while cur_size > 4:\n",
        "            out_nc = cur_nc * 2 if cur_nc < 512 else cur_nc\n",
        "            conv_blocks.append(conv_block(cur_nc, out_nc, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "                act_type=act_type, mode=mode))\n",
        "            conv_blocks.append(conv_block(out_nc, out_nc, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "                act_type=act_type, mode=mode))\n",
        "            cur_nc = out_nc\n",
        "            cur_size //= 2\n",
        "\n",
        "        self.features = sequential(*conv_blocks)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(cur_nc * cur_size * cur_size, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(cur_nc * cur_size * cur_size, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# VGG style Discriminator with input size 96*96\n",
        "class Discriminator_VGG_96(pl.LightningModule):\n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG_96, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 96, 64\n",
        "        conv0 = conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode)\n",
        "        conv1 = conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 48, 64\n",
        "        conv2 = conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv3 = conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 24, 128\n",
        "        conv4 = conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv5 = conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 12, 256\n",
        "        conv6 = conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv7 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 6, 512\n",
        "        conv8 = conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv9 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 3, 512\n",
        "        self.features = sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            conv9)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# VGG style Discriminator with input size 128*128, Spectral Normalization\n",
        "class Discriminator_VGG_128_SN(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(Discriminator_VGG_128_SN, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 128, 64\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "        self.conv0 = spectral_norm(nn.Conv2d(3, 64, 3, 1, 1))\n",
        "        self.conv1 = spectral_norm(nn.Conv2d(64, 64, 4, 2, 1))\n",
        "        # 64, 64\n",
        "        self.conv2 = spectral_norm(nn.Conv2d(64, 128, 3, 1, 1))\n",
        "        self.conv3 = spectral_norm(nn.Conv2d(128, 128, 4, 2, 1))\n",
        "        # 32, 128\n",
        "        self.conv4 = spectral_norm(nn.Conv2d(128, 256, 3, 1, 1))\n",
        "        self.conv5 = spectral_norm(nn.Conv2d(256, 256, 4, 2, 1))\n",
        "        # 16, 256\n",
        "        self.conv6 = spectral_norm(nn.Conv2d(256, 512, 3, 1, 1))\n",
        "        self.conv7 = spectral_norm(nn.Conv2d(512, 512, 4, 2, 1))\n",
        "        # 8, 512\n",
        "        self.conv8 = spectral_norm(nn.Conv2d(512, 512, 3, 1, 1))\n",
        "        self.conv9 = spectral_norm(nn.Conv2d(512, 512, 4, 2, 1))\n",
        "        # 4, 512\n",
        "\n",
        "        # classifier\n",
        "        self.linear0 = spectral_norm(nn.Linear(512 * 4 * 4, 100))\n",
        "        self.linear1 = spectral_norm(nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lrelu(self.conv0(x))\n",
        "        x = self.lrelu(self.conv1(x))\n",
        "        x = self.lrelu(self.conv2(x))\n",
        "        x = self.lrelu(self.conv3(x))\n",
        "        x = self.lrelu(self.conv4(x))\n",
        "        x = self.lrelu(self.conv5(x))\n",
        "        x = self.lrelu(self.conv6(x))\n",
        "        x = self.lrelu(self.conv7(x))\n",
        "        x = self.lrelu(self.conv8(x))\n",
        "        x = self.lrelu(self.conv9(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.lrelu(self.linear0(x))\n",
        "        x = self.linear1(x)\n",
        "        return x\n",
        "\n",
        "# VGG style Discriminator with input size 128*128\n",
        "class Discriminator_VGG_128(pl.LightningModule):\n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG_128, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 128, 64\n",
        "        conv0 = conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode)\n",
        "        conv1 = conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 64, 64\n",
        "        conv2 = conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv3 = conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 32, 128\n",
        "        conv4 = conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv5 = conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 16, 256\n",
        "        conv6 = conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv7 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 8, 512\n",
        "        conv8 = conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv9 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 4, 512\n",
        "        self.features = sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            conv9)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# VGG style Discriminator with input size 192*192\n",
        "class Discriminator_VGG_192(pl.LightningModule): #vic in PPON is called Discriminator_192 \n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG_192, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 192, 64\n",
        "        conv0 = conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode) # 3-->64\n",
        "        conv1 = conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 64-->64, 96*96\n",
        "        # 96, 64\n",
        "        conv2 = conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 64-->128\n",
        "        conv3 = conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 128-->128, 48*48\n",
        "        # 48, 128\n",
        "        conv4 = conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 128-->256\n",
        "        conv5 = conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 256-->256, 24*24\n",
        "        # 24, 256\n",
        "        conv6 = conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 256-->512\n",
        "        conv7 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 512-->512 12*12\n",
        "        # 12, 512\n",
        "        conv8 = conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 512-->512\n",
        "        conv9 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 512-->512 6*6\n",
        "        # 6, 512\n",
        "        conv10 = conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv11 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 3*3\n",
        "        # 3, 512\n",
        "        self.features = sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            conv9, conv10, conv11)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1)) #vic PPON uses 128 and 128 instead of 100\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# VGG style Discriminator with input size 256*256\n",
        "class Discriminator_VGG_256(pl.LightningModule):\n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN'):\n",
        "        super(Discriminator_VGG_256, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 256, 64\n",
        "        conv0 = conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode)\n",
        "        conv1 = conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 128, 64\n",
        "        conv2 = conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv3 = conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 64, 128\n",
        "        conv4 = conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv5 = conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 32, 256\n",
        "        conv6 = conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv7 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 16, 512\n",
        "        conv8 = conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv9 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        # 8, 512\n",
        "        conv10 = conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode)\n",
        "        conv11 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode) # 3*3\n",
        "        # 4, 512\n",
        "        self.features = sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            conv9, conv10, conv11)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "####################\n",
        "# Perceptual Network\n",
        "####################\n",
        "\n",
        "\n",
        "# Assume input range is [0, 1]\n",
        "class VGGFeatureExtractor(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 feature_layer=34,\n",
        "                 use_bn=False,\n",
        "                 use_input_norm=True,\n",
        "                 device=torch.device('cpu'),\n",
        "                 z_norm=False): #Note: PPON uses cuda instead of CPU\n",
        "        super(VGGFeatureExtractor, self).__init__()\n",
        "        if use_bn:\n",
        "            model = torchvision.models.vgg19_bn(pretrained=True)\n",
        "        else:\n",
        "            model = torchvision.models.vgg19(pretrained=True)\n",
        "        self.use_input_norm = use_input_norm\n",
        "        if self.use_input_norm:\n",
        "            if z_norm: # if input in range [-1,1]\n",
        "                mean = torch.Tensor([0.485-1, 0.456-1, 0.406-1]).view(1, 3, 1, 1).to(device) \n",
        "                std = torch.Tensor([0.229*2, 0.224*2, 0.225*2]).view(1, 3, 1, 1).to(device)\n",
        "            else: # input in range [0,1]\n",
        "                mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)                 \n",
        "                std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "            self.register_buffer('mean', mean)\n",
        "            self.register_buffer('std', std)\n",
        "        self.features = nn.Sequential(*list(model.features.children())[:(feature_layer + 1)])\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean) / self.std\n",
        "        output = self.features(x)\n",
        "        return output\n",
        "\n",
        "# Assume input range is [0, 1]\n",
        "class ResNet101FeatureExtractor(pl.LightningModule):\n",
        "    def __init__(self, use_input_norm=True, device=torch.device('cpu'), z_norm=False):\n",
        "        super(ResNet101FeatureExtractor, self).__init__()\n",
        "        model = torchvision.models.resnet101(pretrained=True)\n",
        "        self.use_input_norm = use_input_norm\n",
        "        if self.use_input_norm:\n",
        "            if z_norm: # if input in range [-1,1]\n",
        "                mean = torch.Tensor([0.485-1, 0.456-1, 0.406-1]).view(1, 3, 1, 1).to(device)\n",
        "                std = torch.Tensor([0.229*2, 0.224*2, 0.225*2]).view(1, 3, 1, 1).to(device)\n",
        "            else: # input in range [0,1]\n",
        "                mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
        "                std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "            self.register_buffer('mean', mean)\n",
        "            self.register_buffer('std', std)\n",
        "        self.features = nn.Sequential(*list(model.children())[:8])\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean) / self.std\n",
        "        output = self.features(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "class MINCNet(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(MINCNet, self).__init__()\n",
        "        self.ReLU = nn.ReLU(True)\n",
        "        self.conv11 = nn.Conv2d(3, 64, 3, 1, 1)\n",
        "        self.conv12 = nn.Conv2d(64, 64, 3, 1, 1)\n",
        "        self.maxpool1 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv21 = nn.Conv2d(64, 128, 3, 1, 1)\n",
        "        self.conv22 = nn.Conv2d(128, 128, 3, 1, 1)\n",
        "        self.maxpool2 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv31 = nn.Conv2d(128, 256, 3, 1, 1)\n",
        "        self.conv32 = nn.Conv2d(256, 256, 3, 1, 1)\n",
        "        self.conv33 = nn.Conv2d(256, 256, 3, 1, 1)\n",
        "        self.maxpool3 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv41 = nn.Conv2d(256, 512, 3, 1, 1)\n",
        "        self.conv42 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.conv43 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.maxpool4 = nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv51 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.conv52 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        self.conv53 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.ReLU(self.conv11(x))\n",
        "        out = self.ReLU(self.conv12(out))\n",
        "        out = self.maxpool1(out)\n",
        "        out = self.ReLU(self.conv21(out))\n",
        "        out = self.ReLU(self.conv22(out))\n",
        "        out = self.maxpool2(out)\n",
        "        out = self.ReLU(self.conv31(out))\n",
        "        out = self.ReLU(self.conv32(out))\n",
        "        out = self.ReLU(self.conv33(out))\n",
        "        out = self.maxpool3(out)\n",
        "        out = self.ReLU(self.conv41(out))\n",
        "        out = self.ReLU(self.conv42(out))\n",
        "        out = self.ReLU(self.conv43(out))\n",
        "        out = self.maxpool4(out)\n",
        "        out = self.ReLU(self.conv51(out))\n",
        "        out = self.ReLU(self.conv52(out))\n",
        "        out = self.conv53(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Assume input range is [0, 1]\n",
        "class MINCFeatureExtractor(pl.LightningModule):\n",
        "    def __init__(self, feature_layer=34, use_bn=False, use_input_norm=True, \\\n",
        "                device=torch.device('cpu')):\n",
        "        super(MINCFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.features = MINCNet()\n",
        "        self.features.load_state_dict(\n",
        "            torch.load('../experiments/pretrained_models/VGG16minc_53.pth'), strict=True)\n",
        "        self.features.eval()\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.features(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "#TODO\n",
        "# moved from models.modules.architectures.ASRResNet_arch, did not bring the self-attention layer\n",
        "# VGG style Discriminator with input size 128*128, with feature_maps extraction and self-attention\n",
        "class Discriminator_VGG_128_fea(pl.LightningModule):\n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', \\\n",
        "         arch='ESRGAN', spectral_norm=False, self_attention = False, max_pool=False, poolsize = 4):\n",
        "        super(Discriminator_VGG_128_fea, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 128, 64\n",
        "        \n",
        "        # Self-Attention configuration\n",
        "        '''#TODO\n",
        "        self.self_attention = self_attention\n",
        "        self.max_pool = max_pool\n",
        "        self.poolsize = poolsize\n",
        "        '''\n",
        "        \n",
        "        # Remove BatchNorm2d if using spectral_norm\n",
        "        if spectral_norm:\n",
        "            norm_type = None\n",
        "        \n",
        "        self.conv0 = conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \\\n",
        "            mode=mode)\n",
        "        self.conv1 = conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 64, 64\n",
        "        self.conv2 = conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        self.conv3 = conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 32, 128\n",
        "        self.conv4 = conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        self.conv5 = conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 16, 256\n",
        "        \n",
        "        '''#TODO\n",
        "        if self.self_attention:\n",
        "            self.FSA = SelfAttentionBlock(in_dim = base_nf*4, max_pool=self.max_pool, poolsize = self.poolsize, spectral_norm=spectral_norm)\n",
        "        '''\n",
        "\n",
        "        self.conv6 = conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        self.conv7 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 8, 512\n",
        "        self.conv8 = conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        self.conv9 = conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm)\n",
        "        # 4, 512\n",
        "        # self.features = sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\\\n",
        "            # conv9)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 4 * 4, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    #TODO: modify to a listening dictionary like VGG_Model(), can select what maps to use\n",
        "    def forward(self, x, return_maps=False):\n",
        "        feature_maps = []\n",
        "        # x = self.features(x)\n",
        "        x = self.conv0(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv1(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv2(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv3(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv4(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv5(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv6(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv7(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv8(x)\n",
        "        feature_maps.append(x)\n",
        "        x = self.conv9(x)\n",
        "        feature_maps.append(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        if return_maps:\n",
        "            return [x, feature_maps]\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator_VGG_fea(pl.LightningModule):\n",
        "    def __init__(self, size, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', \\\n",
        "         arch='ESRGAN', spectral_norm=False, self_attention = False, max_pool=False, poolsize = 4):\n",
        "        super(Discriminator_VGG_fea, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "        # 128, 64\n",
        "        \n",
        "        # Self-Attention configuration\n",
        "        '''#TODO\n",
        "        self.self_attention = self_attention\n",
        "        self.max_pool = max_pool\n",
        "        self.poolsize = poolsize\n",
        "        '''\n",
        "        \n",
        "        # Remove BatchNorm2d if using spectral_norm\n",
        "        if spectral_norm:\n",
        "            norm_type = None\n",
        "\n",
        "        self.conv_blocks = []\n",
        "        self.conv_blocks.append(conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm))\n",
        "        self.conv_blocks.append(conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "            act_type=act_type, mode=mode, spectral_norm=spectral_norm))\n",
        "\n",
        "        cur_size = size // 2\n",
        "        cur_nc = base_nf\n",
        "        while cur_size > 4:\n",
        "            out_nc = cur_nc * 2 if cur_nc < 512 else cur_nc\n",
        "            self.conv_blocks.append(conv_block(cur_nc, out_nc, kernel_size=3, stride=1, norm_type=norm_type, \\\n",
        "                act_type=act_type, mode=mode, spectral_norm=spectral_norm))\n",
        "            self.conv_blocks.append(conv_block(out_nc, out_nc, kernel_size=4, stride=2, norm_type=norm_type, \\\n",
        "                act_type=act_type, mode=mode, spectral_norm=spectral_norm))\n",
        "            cur_nc = out_nc\n",
        "            cur_size //= 2\n",
        "        \n",
        "        '''#TODO\n",
        "        if self.self_attention:\n",
        "            self.FSA = SelfAttentionBlock(in_dim = base_nf*4, max_pool=self.max_pool, poolsize = self.poolsize, spectral_norm=spectral_norm)\n",
        "        '''\n",
        "\n",
        "        # self.features = sequential(*conv_blocks)\n",
        "\n",
        "        # classifier\n",
        "        if arch=='PPON':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(cur_nc * cur_size * cur_size, 128), nn.LeakyReLU(0.2, True), nn.Linear(128, 1))\n",
        "        else: #arch='ESRGAN':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(cur_nc * cur_size * cur_size, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n",
        "\n",
        "    #TODO: modify to a listening dictionary like VGG_Model(), can select what maps to use\n",
        "    def forward(self, x, return_maps=False):\n",
        "        feature_maps = []\n",
        "        # x = self.features(x)\n",
        "        for conv in self.conv_blocks:\n",
        "            # Fixes incorrect device error\n",
        "            device = x.device\n",
        "            conv = conv.to(device)\n",
        "            x = conv(x)\n",
        "            feature_maps.append(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        if return_maps:\n",
        "            return [x, feature_maps]\n",
        "        return x\n",
        "\n",
        "\n",
        "class NLayerDiscriminator(pl.LightningModule):\n",
        "    r\"\"\"\n",
        "    PatchGAN discriminator\n",
        "    https://arxiv.org/pdf/1611.07004v3.pdf\n",
        "    https://arxiv.org/pdf/1803.07422.pdf\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
        "        use_sigmoid=False, getIntermFeat=False, patch=True, use_spectral_norm=False):\n",
        "        \"\"\"Construct a PatchGAN discriminator\n",
        "        Parameters:\n",
        "            input_nc (int): the number of channels in input images\n",
        "            ndf (int): the number of filters in the last conv layer\n",
        "            n_layers (int): the number of conv layers in the discriminator\n",
        "            norm_layer (nn.Module): normalization layer (if not using Spectral Norm)\n",
        "            patch (bool): Select between an patch or a linear output\n",
        "            use_spectral_norm (bool): Select if Spectral Norm will be used\n",
        "        \"\"\"\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        '''\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        '''\n",
        "\n",
        "        if use_spectral_norm:\n",
        "            # disable Instance or Batch Norm if using Spectral Norm\n",
        "            norm_layer = Identity\n",
        "\n",
        "        #self.getIntermFeat = getIntermFeat # not used for now\n",
        "        #use_sigmoid not used for now\n",
        "        #TODO: test if there are benefits by incorporating the use of intermediate features from pix2pixHD\n",
        "\n",
        "        use_bias = False\n",
        "        kw = 4\n",
        "        padw = 1 # int(np.ceil((kw-1.0)/2))\n",
        "\n",
        "        sequence = [add_spectral_norm(\n",
        "                        nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), \n",
        "                        use_spectral_norm), \n",
        "                    nn.LeakyReLU(0.2, True)]\n",
        "        nf_mult = 1\n",
        "        nf_mult_prev = 1\n",
        "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2 ** n, 8)\n",
        "            sequence += [\n",
        "                add_spectral_norm(\n",
        "                    nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias), \n",
        "                    use_spectral_norm),\n",
        "                norm_layer(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2 ** n_layers, 8)\n",
        "        sequence += [\n",
        "            add_spectral_norm(\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias), \n",
        "                use_spectral_norm),\n",
        "            norm_layer(ndf * nf_mult),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        if patch:\n",
        "            # output patches as results\n",
        "            sequence += [add_spectral_norm(\n",
        "                nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw), \n",
        "                use_spectral_norm)]  # output 1 channel prediction map\n",
        "        else:\n",
        "            # linear vector classification output\n",
        "            sequence += [Mean([1, 2]), nn.Linear(ndf * nf_mult, 1)]\n",
        "        \n",
        "        if use_sigmoid:\n",
        "            sequence += [nn.Sigmoid()]\n",
        "        \n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class MultiscaleDiscriminator(pl.LightningModule):\n",
        "    r\"\"\"\n",
        "    Multiscale PatchGAN discriminator\n",
        "    https://arxiv.org/pdf/1711.11585.pdf\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
        "                 use_sigmoid=False, num_D=3, getIntermFeat=False):\n",
        "        \"\"\"Construct a pyramid of PatchGAN discriminators\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            n_layers (int)  -- the number of conv layers in the discriminator\n",
        "            norm_layer      -- normalization layer\n",
        "            use_sigmoid     -- boolean to use sigmoid in patchGAN discriminators\n",
        "            num_D (int)     -- number of discriminators/downscales in the pyramid\n",
        "            getIntermFeat   -- boolean to get intermediate features (unused for now)\n",
        "        \"\"\"\n",
        "        super(MultiscaleDiscriminator, self).__init__()\n",
        "        self.num_D = num_D\n",
        "        self.n_layers = n_layers\n",
        "        self.getIntermFeat = getIntermFeat\n",
        "     \n",
        "        for i in range(num_D):\n",
        "            netD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat)\n",
        "            if getIntermFeat:                                \n",
        "                for j in range(n_layers+2):\n",
        "                    setattr(self, 'scale'+str(i)+'_layer'+str(j), getattr(netD, 'model'+str(j)))                                   \n",
        "            else:\n",
        "                setattr(self, 'layer'+str(i), netD.model)\n",
        "\n",
        "        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n",
        "\n",
        "    def singleD_forward(self, model, input):\n",
        "        if self.getIntermFeat:\n",
        "            result = [input]\n",
        "            for i in range(len(model)):\n",
        "                result.append(model[i](result[-1]))\n",
        "            return result[1:]\n",
        "        else:\n",
        "            return [model(input)]\n",
        "\n",
        "    def forward(self, input):        \n",
        "        num_D = self.num_D\n",
        "        result = []\n",
        "        input_downsampled = input\n",
        "        for i in range(num_D):\n",
        "            if self.getIntermFeat:\n",
        "                model = [getattr(self, 'scale'+str(num_D-1-i)+'_layer'+str(j)) for j in range(self.n_layers+2)]\n",
        "            else:\n",
        "                model = getattr(self, 'layer'+str(num_D-1-i))\n",
        "            result.append(self.singleD_forward(model, input_downsampled))\n",
        "            if i != (num_D-1):\n",
        "                input_downsampled = self.downsample(input_downsampled)\n",
        "        return result\n",
        "\n",
        "\n",
        "class PixelDiscriminator(pl.LightningModule):\n",
        "    \"\"\"Defines a 1x1 PatchGAN discriminator (pixelGAN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d):\n",
        "        \"\"\"Construct a 1x1 PatchGAN discriminator\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            norm_layer      -- normalization layer\n",
        "        \"\"\"\n",
        "        super(PixelDiscriminator, self).__init__()\n",
        "        '''\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        '''\n",
        "        use_bias = False\n",
        "\n",
        "        self.net = [\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n",
        "            norm_layer(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n",
        "\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.net(input)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "models.py (21-12-20)\n",
        "https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/context_encoder/models.py\n",
        "\"\"\"\n",
        "\n",
        "class context_encoder(pl.LightningModule):\n",
        "    def __init__(self, channels=3):\n",
        "        super(context_encoder, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, stride, normalize):\n",
        "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        layers = []\n",
        "        in_filters = channels\n",
        "        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n",
        "            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n",
        "            in_filters = out_filters\n",
        "\n",
        "        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, img):\n",
        "        return self.model(img)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "discriminators.py (12-2-20)\n",
        "https://github.com/JoeyBallentine/BasicSR/blob/resnet-discriminator/codes/models/modules/architectures/discriminators.py\n",
        "\"\"\"\n",
        "# Assume input range is [0, 1]\n",
        "class ResNet101FeatureExtractor(pl.LightningModule):\n",
        "    def __init__(self, use_input_norm=True, device=torch.device('cpu'), z_norm=False):\n",
        "        super(ResNet101FeatureExtractor, self).__init__()\n",
        "        model = torchvision.models.resnet101(pretrained=True)\n",
        "        self.use_input_norm = use_input_norm\n",
        "        if self.use_input_norm:\n",
        "            if z_norm: # if input in range [-1,1]\n",
        "                mean = torch.Tensor([0.485-1, 0.456-1, 0.406-1]).view(1, 3, 1, 1).to(device)\n",
        "                std = torch.Tensor([0.229*2, 0.224*2, 0.225*2]).view(1, 3, 1, 1).to(device)\n",
        "            else: # input in range [0,1]\n",
        "                mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
        "                std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "            self.register_buffer('mean', mean)\n",
        "            self.register_buffer('std', std)\n",
        "        self.features = nn.Sequential(*list(model.children())[:8])\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean) / self.std\n",
        "        output = self.features(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "# ResNet50 style Discriminator with input size 128*128\n",
        "class Discriminator_ResNet_128(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Structure based off of the ResNet50 configuration from this repository:\n",
        "    https://github.com/bentrevett/pytorch-image-classification\n",
        "    \"\"\"\n",
        "    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA'):\n",
        "        super(Discriminator_ResNet_128, self).__init__()\n",
        "        # features\n",
        "        # hxw, c\n",
        "\n",
        "        self.in_channels = base_nf\n",
        "        \n",
        "        # 128, 3\n",
        "        conv0 = conv_block(in_nc, self.in_channels, kernel_size=7, norm_type=norm_type, act_type=act_type, \\\n",
        "            mode=mode, stride=2, bias=False)\n",
        "        pool0 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        # 32, 64\n",
        "\n",
        "        layer1 = self.get_resnet_layer(Bottleneck, 3, base_nf) # 32, 64\n",
        "        layer2 = self.get_resnet_layer(Bottleneck, 4, base_nf*2, stride = 2) # 16, 128\n",
        "        layer3 = self.get_resnet_layer(Bottleneck, 6, base_nf*4, stride = 2) # 8, 256\n",
        "        layer4 = self.get_resnet_layer(Bottleneck, 3, base_nf*8, stride = 2) # 4, 512\n",
        "\n",
        "        avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "        self.features = sequential(conv0, pool0, layer1, layer2, layer3, layer4, avgpool)\n",
        "\n",
        "        self.classifier = nn.Linear(self.in_channels, 1)\n",
        "\n",
        "    def get_resnet_layer(self, block, n_blocks, channels, stride = 1):\n",
        "    \n",
        "        layers = []\n",
        "        \n",
        "        if self.in_channels != block.expansion * channels:\n",
        "            downsample = True\n",
        "        else:\n",
        "            downsample = False\n",
        "        \n",
        "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
        "        \n",
        "        for i in range(1, n_blocks):\n",
        "            layers.append(block(block.expansion * channels, channels))\n",
        "\n",
        "        self.in_channels = block.expansion * channels\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Bottleneck(pl.LightningModule):\n",
        "    \n",
        "    expansion = 4\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1, \n",
        "                               stride = 1, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, \n",
        "                               stride = stride, padding = 1, bias = False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(out_channels, self.expansion * out_channels, kernel_size = 1,\n",
        "                               stride = 1, bias = False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion * out_channels)\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        \n",
        "        if downsample:\n",
        "            conv = nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size = 1, \n",
        "                             stride = stride, bias = False)\n",
        "            bn = nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            downsample = None\n",
        "            \n",
        "        self.downsample = downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        i = x\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "                \n",
        "        if self.downsample is not None:\n",
        "            i = self.downsample(i)\n",
        "            \n",
        "        x += i\n",
        "        x = self.relu(x)\n",
        "    \n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x9wN88gDykW"
      },
      "source": [
        "Edit ``vgg path`` and ``dictionary_path`` inside ``model``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0en1qQQM-4XU",
        "cellView": "form"
      },
      "source": [
        "#@title model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import functools\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter as P\n",
        "#from util import util\n",
        "from torchvision import models\n",
        "#import scipy.io as sio\n",
        "import numpy as np\n",
        "#import scipy.ndimage\n",
        "import torch.nn.utils.spectral_norm as SpectralNorm\n",
        "\n",
        "from torch.autograd import Function\n",
        "from math import sqrt\n",
        "import random\n",
        "import os\n",
        "import math\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "class VGGFeat(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Input: (B, C, H, W), RGB, [-1, 1]\n",
        "    \"\"\"\n",
        "    def __init__(self, weight_path='/content/DFDNet/weights/vgg19.pth'):\n",
        "        super().__init__()\n",
        "        self.model = models.vgg19(pretrained=False)\n",
        "        self.build_vgg_layers()\n",
        "        \n",
        "        self.model.load_state_dict(torch.load(weight_path))\n",
        "\n",
        "        self.register_parameter(\"RGB_mean\", nn.Parameter(torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)))\n",
        "        self.register_parameter(\"RGB_std\", nn.Parameter(torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)))\n",
        "        \n",
        "        # self.model.eval()\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def build_vgg_layers(self):\n",
        "        vgg_pretrained_features = self.model.features\n",
        "        self.features = []\n",
        "        # feature_layers = [0, 3, 8, 17, 26, 35]\n",
        "        feature_layers = [0, 8, 17, 26, 35]\n",
        "        for i in range(len(feature_layers)-1): \n",
        "            module_layers = torch.nn.Sequential() \n",
        "            for j in range(feature_layers[i], feature_layers[i+1]):\n",
        "                module_layers.add_module(str(j), vgg_pretrained_features[j])\n",
        "            self.features.append(module_layers)\n",
        "        self.features = torch.nn.ModuleList(self.features)\n",
        "\n",
        "    def preprocess(self, x):\n",
        "        x = (x + 1) / 2\n",
        "        x = (x - self.RGB_mean) / self.RGB_std\n",
        "        if x.shape[3] < 224:\n",
        "            x = torch.nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.preprocess(x)\n",
        "        features = []\n",
        "        for m in self.features:\n",
        "            # print(m)\n",
        "            x = m(x)\n",
        "            features.append(x)\n",
        "        return features \n",
        "\n",
        "\n",
        "class StyledUpBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1,upsample=False):\n",
        "        super().__init__()\n",
        "        if upsample:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                Blur(out_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                Blur(in_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding)\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        self.convup = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                # EqualConv2d(out_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(out_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                # Blur(out_channel),\n",
        "            )\n",
        "        # self.noise1 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain1 = AdaptiveInstanceNorm(out_channel)\n",
        "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        # self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\n",
        "        # self.noise2 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain2 = AdaptiveInstanceNorm(out_channel)\n",
        "        # self.lrelu2 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.ScaleModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1))\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "        self.ShiftModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1)),\n",
        "            nn.Sigmoid(),\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "       \n",
        "    def forward(self, input, style):\n",
        "        out = self.conv1(input)\n",
        "#         out = self.noise1(out, noise)\n",
        "        out = self.lrelu1(out)\n",
        "\n",
        "        Shift1 = self.ShiftModel1(style)\n",
        "        Scale1 = self.ScaleModel1(style)\n",
        "        out = out * Scale1 + Shift1\n",
        "        # out = self.adain1(out, style)\n",
        "        outup = self.convup(out)\n",
        "\n",
        "        return outup\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class StyledUpBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1,upsample=False):\n",
        "        super().__init__()\n",
        "        if upsample:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                Blur(out_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                Blur(in_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding)\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        self.convup = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                # EqualConv2d(out_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(out_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                # Blur(out_channel),\n",
        "            )\n",
        "        # self.noise1 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain1 = AdaptiveInstanceNorm(out_channel)\n",
        "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        # self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\n",
        "        # self.noise2 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain2 = AdaptiveInstanceNorm(out_channel)\n",
        "        # self.lrelu2 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.ScaleModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1))\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "        self.ShiftModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1)),\n",
        "            nn.Sigmoid(),\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "       \n",
        "    def forward(self, input, style):\n",
        "        out = self.conv1(input)\n",
        "#         out = self.noise1(out, noise)\n",
        "        out = self.lrelu1(out)\n",
        "\n",
        "        Shift1 = self.ShiftModel1(style)\n",
        "        Scale1 = self.ScaleModel1(style)\n",
        "        out = out * Scale1 + Shift1\n",
        "        # out = self.adain1(out, style)\n",
        "        outup = self.convup(out)\n",
        "\n",
        "        return outup\n",
        "\n",
        "class UNetDictFace(pl.LightningModule):\n",
        "    def __init__(self, ngf=64, dictionary_path='/content/DFDNet/DictionaryCenter512'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.part_sizes = np.array([80,80,50,110]) # size for 512\n",
        "        self.feature_sizes = np.array([256,128,64,32])\n",
        "        self.channel_sizes = np.array([128,256,512,512])\n",
        "        Parts = ['left_eye','right_eye','nose','mouth']\n",
        "        self.Dict_256 = {}\n",
        "        self.Dict_128 = {}\n",
        "        self.Dict_64 = {}\n",
        "        self.Dict_32 = {}\n",
        "        for j,i in enumerate(Parts):\n",
        "            f_256 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_256_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_256_reshape = f_256.reshape(f_256.size(0),self.channel_sizes[0],self.part_sizes[j]//2,self.part_sizes[j]//2)\n",
        "            max_256 = torch.max(torch.sqrt(compute_sum(torch.pow(f_256_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_256[i] = f_256_reshape #/ max_256\n",
        "            \n",
        "            f_128 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_128_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_128_reshape = f_128.reshape(f_128.size(0),self.channel_sizes[1],self.part_sizes[j]//4,self.part_sizes[j]//4)\n",
        "            max_128 = torch.max(torch.sqrt(compute_sum(torch.pow(f_128_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_128[i] = f_128_reshape #/ max_128\n",
        "\n",
        "            f_64 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_64_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_64_reshape = f_64.reshape(f_64.size(0),self.channel_sizes[2],self.part_sizes[j]//8,self.part_sizes[j]//8)\n",
        "            max_64 = torch.max(torch.sqrt(compute_sum(torch.pow(f_64_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_64[i] = f_64_reshape #/ max_64\n",
        "\n",
        "            f_32 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_32_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_32_reshape = f_32.reshape(f_32.size(0),self.channel_sizes[3],self.part_sizes[j]//16,self.part_sizes[j]//16)\n",
        "            max_32 = torch.max(torch.sqrt(compute_sum(torch.pow(f_32_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_32[i] = f_32_reshape #/ max_32\n",
        "\n",
        "        self.le_256 = AttentionBlock(128)\n",
        "        self.le_128 = AttentionBlock(256)\n",
        "        self.le_64 = AttentionBlock(512)\n",
        "        self.le_32 = AttentionBlock(512)\n",
        "\n",
        "        self.re_256 = AttentionBlock(128)\n",
        "        self.re_128 = AttentionBlock(256)\n",
        "        self.re_64 = AttentionBlock(512)\n",
        "        self.re_32 = AttentionBlock(512)\n",
        "\n",
        "        self.no_256 = AttentionBlock(128)\n",
        "        self.no_128 = AttentionBlock(256)\n",
        "        self.no_64 = AttentionBlock(512)\n",
        "        self.no_32 = AttentionBlock(512)\n",
        "\n",
        "        self.mo_256 = AttentionBlock(128)\n",
        "        self.mo_128 = AttentionBlock(256)\n",
        "        self.mo_64 = AttentionBlock(512)\n",
        "        self.mo_32 = AttentionBlock(512)\n",
        "\n",
        "        #norm\n",
        "        self.VggExtract = VGGFeat()\n",
        "        \n",
        "        ######################\n",
        "        self.MSDilate = MSDilateBlock(ngf*8, dilation = [4,3,2,1])  #\n",
        "\n",
        "        self.up0 = StyledUpBlock(ngf*8,ngf*8)\n",
        "        self.up1 = StyledUpBlock(ngf*8, ngf*4) #\n",
        "        self.up2 = StyledUpBlock(ngf*4, ngf*2) #\n",
        "        self.up3 = StyledUpBlock(ngf*2, ngf) #\n",
        "        self.up4 = nn.Sequential( # 128\n",
        "            # nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            SpectralNorm(nn.Conv2d(ngf, ngf, 3, 1, 1)),\n",
        "            # nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            UpResBlock(ngf),\n",
        "            UpResBlock(ngf),\n",
        "            # SpectralNorm(nn.Conv2d(ngf, 3, kernel_size=3, stride=1, padding=1)),\n",
        "            nn.Conv2d(ngf, 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.to_rgb0 = ToRGB(ngf*8)\n",
        "        self.to_rgb1 = ToRGB(ngf*4)\n",
        "        self.to_rgb2 = ToRGB(ngf*2)\n",
        "        self.to_rgb3 = ToRGB(ngf*1)\n",
        "\n",
        "        # for param in self.BlurInputConv.parameters():\n",
        "        #     param.requires_grad = False\n",
        "    \n",
        "    def forward(self, input, part_locations):\n",
        "        VggFeatures = self.VggExtract(input)\n",
        "        # for b in range(input.size(0)):\n",
        "        b = 0\n",
        "        UpdateVggFeatures = []\n",
        "        for i, f_size in enumerate(self.feature_sizes):\n",
        "            cur_feature = VggFeatures[i]\n",
        "\n",
        "            update_feature = cur_feature.clone() #* 0\n",
        "            cur_part_sizes = self.part_sizes // (512/f_size)\n",
        "            dicts_feature = getattr(self, 'Dict_'+str(f_size))\n",
        "            \n",
        "            LE_Dict_feature = dicts_feature['left_eye'].to(input)\n",
        "            RE_Dict_feature = dicts_feature['right_eye'].to(input)\n",
        "            NO_Dict_feature = dicts_feature['nose'].to(input)\n",
        "            MO_Dict_feature = dicts_feature['mouth'].to(input)\n",
        "\n",
        "            le_location = (part_locations[0][b] // (512/f_size)).int()\n",
        "            re_location = (part_locations[1][b] // (512/f_size)).int()\n",
        "            no_location = (part_locations[2][b] // (512/f_size)).int()\n",
        "            mo_location = (part_locations[3][b] // (512/f_size)).int()\n",
        "\n",
        "            LE_feature = cur_feature[:,:,le_location[1]:le_location[3],le_location[0]:le_location[2]].clone()\n",
        "            RE_feature = cur_feature[:,:,re_location[1]:re_location[3],re_location[0]:re_location[2]].clone()\n",
        "            NO_feature = cur_feature[:,:,no_location[1]:no_location[3],no_location[0]:no_location[2]].clone()\n",
        "            MO_feature = cur_feature[:,:,mo_location[1]:mo_location[3],mo_location[0]:mo_location[2]].clone()\n",
        "            \n",
        "            #resize\n",
        "            LE_feature_resize = F.interpolate(LE_feature,(LE_Dict_feature.size(2),LE_Dict_feature.size(3)),mode='bilinear',align_corners=False)\n",
        "            RE_feature_resize = F.interpolate(RE_feature,(RE_Dict_feature.size(2),RE_Dict_feature.size(3)),mode='bilinear',align_corners=False)\n",
        "            NO_feature_resize = F.interpolate(NO_feature,(NO_Dict_feature.size(2),NO_Dict_feature.size(3)),mode='bilinear',align_corners=False)\n",
        "            MO_feature_resize = F.interpolate(MO_feature,(MO_Dict_feature.size(2),MO_Dict_feature.size(3)),mode='bilinear',align_corners=False)\n",
        "            \n",
        "            LE_Dict_feature_norm = adaptive_instance_normalization_4D(LE_Dict_feature, LE_feature_resize)\n",
        "            RE_Dict_feature_norm = adaptive_instance_normalization_4D(RE_Dict_feature, RE_feature_resize)\n",
        "            NO_Dict_feature_norm = adaptive_instance_normalization_4D(NO_Dict_feature, NO_feature_resize)\n",
        "            MO_Dict_feature_norm = adaptive_instance_normalization_4D(MO_Dict_feature, MO_feature_resize)\n",
        "            \n",
        "            LE_score = F.conv2d(LE_feature_resize, LE_Dict_feature_norm)\n",
        "\n",
        "            LE_score = F.softmax(LE_score.view(-1),dim=0)\n",
        "            LE_index = torch.argmax(LE_score)\n",
        "            LE_Swap_feature = F.interpolate(LE_Dict_feature_norm[LE_index:LE_index+1], (LE_feature.size(2), LE_feature.size(3)))\n",
        "\n",
        "            LE_Attention = getattr(self, 'le_'+str(f_size))(LE_Swap_feature-LE_feature)\n",
        "            LE_Att_feature = LE_Attention * LE_Swap_feature\n",
        "            \n",
        "\n",
        "            RE_score = F.conv2d(RE_feature_resize, RE_Dict_feature_norm)\n",
        "            RE_score = F.softmax(RE_score.view(-1),dim=0)\n",
        "            RE_index = torch.argmax(RE_score)\n",
        "            RE_Swap_feature = F.interpolate(RE_Dict_feature_norm[RE_index:RE_index+1], (RE_feature.size(2), RE_feature.size(3)))\n",
        "            \n",
        "            RE_Attention = getattr(self, 're_'+str(f_size))(RE_Swap_feature-RE_feature)\n",
        "            RE_Att_feature = RE_Attention * RE_Swap_feature\n",
        "\n",
        "            NO_score = F.conv2d(NO_feature_resize, NO_Dict_feature_norm)\n",
        "            NO_score = F.softmax(NO_score.view(-1),dim=0)\n",
        "            NO_index = torch.argmax(NO_score)\n",
        "            NO_Swap_feature = F.interpolate(NO_Dict_feature_norm[NO_index:NO_index+1], (NO_feature.size(2), NO_feature.size(3)))\n",
        "            \n",
        "            NO_Attention = getattr(self, 'no_'+str(f_size))(NO_Swap_feature-NO_feature)\n",
        "            NO_Att_feature = NO_Attention * NO_Swap_feature\n",
        "\n",
        "            \n",
        "            MO_score = F.conv2d(MO_feature_resize, MO_Dict_feature_norm)\n",
        "            MO_score = F.softmax(MO_score.view(-1),dim=0)\n",
        "            MO_index = torch.argmax(MO_score)\n",
        "            MO_Swap_feature = F.interpolate(MO_Dict_feature_norm[MO_index:MO_index+1], (MO_feature.size(2), MO_feature.size(3)))\n",
        "            \n",
        "            MO_Attention = getattr(self, 'mo_'+str(f_size))(MO_Swap_feature-MO_feature)\n",
        "            MO_Att_feature = MO_Attention * MO_Swap_feature\n",
        "\n",
        "            update_feature[:,:,le_location[1]:le_location[3],le_location[0]:le_location[2]] = LE_Att_feature + LE_feature\n",
        "            update_feature[:,:,re_location[1]:re_location[3],re_location[0]:re_location[2]] = RE_Att_feature + RE_feature\n",
        "            update_feature[:,:,no_location[1]:no_location[3],no_location[0]:no_location[2]] = NO_Att_feature + NO_feature\n",
        "            update_feature[:,:,mo_location[1]:mo_location[3],mo_location[0]:mo_location[2]] = MO_Att_feature + MO_feature\n",
        "\n",
        "            UpdateVggFeatures.append(update_feature) \n",
        "        \n",
        "        fea_vgg = self.MSDilate(VggFeatures[3])\n",
        "        #new version\n",
        "        fea_up0 = self.up0(fea_vgg, UpdateVggFeatures[3])\n",
        "        # out1 = F.interpolate(fea_up0,(512,512))\n",
        "        # out1 = self.to_rgb0(out1)\n",
        "\n",
        "        fea_up1 = self.up1( fea_up0, UpdateVggFeatures[2]) #\n",
        "        # out2 = F.interpolate(fea_up1,(512,512))\n",
        "        # out2 = self.to_rgb1(out2)\n",
        "\n",
        "        fea_up2 = self.up2(fea_up1, UpdateVggFeatures[1]) #\n",
        "        # out3 = F.interpolate(fea_up2,(512,512))\n",
        "        # out3 = self.to_rgb2(out3)\n",
        "\n",
        "        fea_up3 = self.up3(fea_up2, UpdateVggFeatures[0]) #\n",
        "        # out4 = F.interpolate(fea_up3,(512,512))\n",
        "        # out4 = self.to_rgb3(out4)\n",
        "\n",
        "        output = self.up4(fea_up3) #\n",
        "        \n",
        "    \n",
        "        return output  #+ out4 + out3 + out2 + out1\n",
        "        #0 128 * 256 * 256\n",
        "        #1 256 * 128 * 128\n",
        "        #2 512 * 64 * 64\n",
        "        #3 512 * 32 * 32\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# smaller blocks\n",
        "def AttentionBlock(in_channel):\n",
        "    return nn.Sequential(\n",
        "        SpectralNorm(nn.Conv2d(in_channel, in_channel, 3, 1, 1)),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        SpectralNorm(nn.Conv2d(in_channel, in_channel, 3, 1, 1))\n",
        "    )\n",
        "\n",
        "\n",
        "class MSDilateBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channels,conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, kernel_size=3, dilation=[1,1,1,1], bias=True):\n",
        "        super(MSDilateBlock, self).__init__()\n",
        "        self.conv1 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[0], bias=bias)\n",
        "        self.conv2 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[1], bias=bias)\n",
        "        self.conv3 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[2], bias=bias)\n",
        "        self.conv4 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[3], bias=bias)\n",
        "        self.convi =  SpectralNorm(conv_layer(in_channels*4, in_channels, kernel_size=kernel_size, stride=1, padding=(kernel_size-1)//2, bias=bias))\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv1(x)\n",
        "        conv2 = self.conv2(x)\n",
        "        conv3 = self.conv3(x)\n",
        "        conv4 = self.conv4(x)\n",
        "        cat  = torch.cat([conv1, conv2, conv3, conv4], 1)\n",
        "        out = self.convi(cat) + x\n",
        "        return out\n",
        "\n",
        "\n",
        "class VggClassNet(pl.LightningModule):\n",
        "    def __init__(self, select_layer = ['0','5','10','19']):\n",
        "        super(VggClassNet, self).__init__()\n",
        "        self.select = select_layer\n",
        "        self.vgg = models.vgg19(pretrained=True).features\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for name, layer in self.vgg._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in self.select:\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class UpResBlock(pl.LightningModule):\n",
        "    def __init__(self, dim, conv_layer = nn.Conv2d, norm_layer = nn.BatchNorm2d):\n",
        "        super(UpResBlock, self).__init__()\n",
        "        self.Model = nn.Sequential(\n",
        "            # SpectralNorm(conv_layer(dim, dim, 3, 1, 1)),\n",
        "            conv_layer(dim, dim, 3, 1, 1),\n",
        "            # norm_layer(dim),\n",
        "            nn.LeakyReLU(0.2,True),\n",
        "            # SpectralNorm(conv_layer(dim, dim, 3, 1, 1)),\n",
        "            conv_layer(dim, dim, 3, 1, 1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        out = x + self.Model(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "L-8r5VTPWBpK"
      },
      "source": [
        "#@title functions.py\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "def compute_sum(x, axis=None, keepdim=False):\n",
        "    if not axis:\n",
        "        axis = range(len(x.shape))\n",
        "    for i in sorted(axis, reverse=True):\n",
        "        x = torch.sum(x, dim=i, keepdim=keepdim)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def convU(in_channels, out_channels,conv_layer, norm_layer, kernel_size=3, stride=1,dilation=1, bias=True):\n",
        "    return nn.Sequential(\n",
        "        SpectralNorm(conv_layer(in_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias)),\n",
        "#         conv_layer(in_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias),\n",
        "#         nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        SpectralNorm(conv_layer(out_channels, out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, padding=((kernel_size-1)//2)*dilation, bias=bias)),\n",
        "    )\n",
        "\n",
        "\n",
        "class AdaptiveInstanceNorm(pl.LightningModule):\n",
        "    def __init__(self, in_channel):\n",
        "        super().__init__()\n",
        "        self.norm = nn.InstanceNorm2d(in_channel)\n",
        "\n",
        "    def forward(self, input, style):\n",
        "        style_mean, style_std = calc_mean_std_4D(style)\n",
        "        out = self.norm(input)\n",
        "        size = input.size()\n",
        "        out = style_std.expand(size) * out + style_mean.expand(size)\n",
        "        return out\n",
        "\n",
        "class BlurFunctionBackward(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, grad_output, kernel, kernel_flip):\n",
        "        ctx.save_for_backward(kernel, kernel_flip)\n",
        "\n",
        "        grad_input = F.conv2d(\n",
        "            grad_output, kernel_flip, padding=1, groups=grad_output.shape[1]\n",
        "        )\n",
        "        return grad_input\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, gradgrad_output):\n",
        "        kernel, kernel_flip = ctx.saved_tensors\n",
        "\n",
        "        grad_input = F.conv2d(\n",
        "            gradgrad_output, kernel, padding=1, groups=gradgrad_output.shape[1]\n",
        "        )\n",
        "        return grad_input, None, None\n",
        "\n",
        "\n",
        "class BlurFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, kernel, kernel_flip):\n",
        "        ctx.save_for_backward(kernel, kernel_flip)\n",
        "\n",
        "        output = F.conv2d(input, kernel, padding=1, groups=input.shape[1])\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        kernel, kernel_flip = ctx.saved_tensors\n",
        "\n",
        "        grad_input = BlurFunctionBackward.apply(grad_output, kernel, kernel_flip)\n",
        "\n",
        "        return grad_input, None, None\n",
        "\n",
        "blur = BlurFunction.apply\n",
        "\n",
        "\n",
        "class Blur(pl.LightningModule):\n",
        "    def __init__(self, channel):\n",
        "        super().__init__()\n",
        "\n",
        "        weight = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=torch.float32)\n",
        "        weight = weight.view(1, 1, 3, 3)\n",
        "        weight = weight / weight.sum()\n",
        "        weight_flip = torch.flip(weight, [2, 3])\n",
        "\n",
        "        self.register_buffer('weight', weight.repeat(channel, 1, 1, 1))\n",
        "        self.register_buffer('weight_flip', weight_flip.repeat(channel, 1, 1, 1))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return blur(input, self.weight, self.weight_flip)\n",
        "\n",
        "class EqualLR:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def compute_weight(self, module):\n",
        "        weight = getattr(module, self.name + '_orig')\n",
        "        fan_in = weight.data.size(1) * weight.data[0][0].numel()\n",
        "        return weight * sqrt(2 / fan_in)\n",
        "    @staticmethod\n",
        "    def apply(module, name):\n",
        "        fn = EqualLR(name)\n",
        "\n",
        "        weight = getattr(module, name)\n",
        "        del module._parameters[name]\n",
        "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
        "        module.register_forward_pre_hook(fn)\n",
        "\n",
        "        return fn\n",
        "\n",
        "    def __call__(self, module, input):\n",
        "        weight = self.compute_weight(module)\n",
        "        setattr(module, self.name, weight)\n",
        "\n",
        "def equal_lr(module, name='weight'):\n",
        "    EqualLR.apply(module, name)\n",
        "    return module\n",
        "\n",
        "class EqualConv2d(pl.LightningModule):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        conv = nn.Conv2d(*args, **kwargs)\n",
        "        conv.weight.data.normal_()\n",
        "        conv.bias.data.zero_()\n",
        "        self.conv = equal_lr(conv)\n",
        "    def forward(self, input):\n",
        "        return self.conv(input)\n",
        "\n",
        "class NoiseInjection(pl.LightningModule):\n",
        "    def __init__(self, channel):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))\n",
        "    def forward(self, image, noise):\n",
        "        return image + self.weight * noise\n",
        "\n",
        "\n",
        "def ToRGB(in_channel):\n",
        "    return nn.Sequential(\n",
        "        SpectralNorm(nn.Conv2d(in_channel,in_channel,3, 1, 1)),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        SpectralNorm(nn.Conv2d(in_channel,3,3, 1, 1))\n",
        "    )\n",
        "\n",
        "def adaptive_instance_normalization_4D(content_feat, style_feat): # content_feat is ref feature, style is degradate feature\n",
        "    # assert (content_feat.size()[:2] == style_feat.size()[:2])\n",
        "    size = content_feat.size()\n",
        "    style_mean, style_std = calc_mean_std_4D(style_feat)\n",
        "    content_mean, content_std = calc_mean_std_4D(content_feat)\n",
        "    normalized_feat = (content_feat - content_mean.expand(\n",
        "        size)) / content_std.expand(size)\n",
        "    return normalized_feat * style_std + style_mean\n",
        "\n",
        "def calc_mean_std_4D(feat, eps=1e-5):\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
        "    size = feat.size()\n",
        "    assert (len(size) == 4)\n",
        "    N, C = size[:2]\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
        "    return feat_mean, feat_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdLqmPn9sw_",
        "cellView": "form"
      },
      "source": [
        "#@title CustomTrainClass.py\n",
        "from vic.loss import CharbonnierLoss, GANLoss, GradientPenaltyLoss, HFENLoss, TVLoss, GradientLoss, ElasticLoss, RelativeL1, L1CosineSim, ClipL1, MaskedL1Loss, MultiscalePixelLoss, FFTloss, OFLoss, L1_regularization, ColorLoss, AverageLoss, GPLoss, CPLoss, SPL_ComputeWithTrace, SPLoss, Contextual_Loss, StyleLoss\n",
        "from vic.perceptual_loss import PerceptualLoss\n",
        "from metrics import *\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "class CustomTrainClass(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # generator\n",
        "    self.netG = UNetDictFace(64)\n",
        "    weights_init(self.netG, 'kaiming')\n",
        "\n",
        "    # discriminator\n",
        "    self.netD = context_encoder()\n",
        "\n",
        "    # VGG\n",
        "    #self.netD = Discriminator_VGG(size=256, in_nc=3, base_nf=64, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D', arch='ESRGAN')\n",
        "    #self.netD = Discriminator_VGG_fea(size=256, in_nc=3, base_nf=64, norm_type='batch', act_type='leakyrelu', mode='CNA', convtype='Conv2D',\n",
        "    #     arch='ESRGAN', spectral_norm=False, self_attention = False, max_pool=False, poolsize = 4)\n",
        "    #self.netD = Discriminator_VGG_128_SN()\n",
        "    #self.netD = VGGFeatureExtractor(feature_layer=34,use_bn=False,use_input_norm=True,device=torch.device('cpu'),z_norm=False)\n",
        "\n",
        "    # PatchGAN\n",
        "    #self.netD = NLayerDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
        "    #    use_sigmoid=False, getIntermFeat=False, patch=True, use_spectral_norm=False)\n",
        "\n",
        "    # Multiscale\n",
        "    #self.netD = MultiscaleDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
        "    #             use_sigmoid=False, num_D=3, getIntermFeat=False)\n",
        "\n",
        "    # ResNet\n",
        "    #self.netD = Discriminator_ResNet_128(in_nc=3, base_nf=64, norm_type='batch', act_type='leakyrelu', mode='CNA')\n",
        "    #self.netD = ResNet101FeatureExtractor(use_input_norm=True, device=torch.device('cpu'), z_norm=False)\n",
        "    \n",
        "    # MINC\n",
        "    #self.netD = MINCNet()\n",
        "\n",
        "    # Pixel\n",
        "    #self.netD = PixelDiscriminator(input_nc=3, ndf=64, norm_layer=nn.BatchNorm2d)\n",
        "\n",
        "    # EfficientNet\n",
        "    #from efficientnet_pytorch import EfficientNet\n",
        "    #self.netD = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "\n",
        "    # ResNeSt\n",
        "    # [\"resnest50\", \"resnest101\", \"resnest200\", \"resnest269\"]\n",
        "    #self.netD = resnest50(pretrained=True)\n",
        "\n",
        "    weights_init(self.netD, 'kaiming')\n",
        "\n",
        "\n",
        "    # loss functions\n",
        "    self.l1 = nn.L1Loss()\n",
        "    l_hfen_type = L1CosineSim()\n",
        "    self.HFENLoss = HFENLoss(loss_f=l_hfen_type, kernel='log', kernel_size=15, sigma = 2.5, norm = False)\n",
        "    self.ElasticLoss = ElasticLoss(a=0.2, reduction='mean')\n",
        "    self.RelativeL1 = RelativeL1(eps=.01, reduction='mean')\n",
        "    self.L1CosineSim = L1CosineSim(loss_lambda=5, reduction='mean')\n",
        "    self.ClipL1 = ClipL1(clip_min=0.0, clip_max=10.0)\n",
        "    self.FFTloss = FFTloss(loss_f = torch.nn.L1Loss, reduction='mean')\n",
        "    self.OFLoss = OFLoss()\n",
        "    self.GPLoss = GPLoss(trace=False, spl_denorm=False)\n",
        "    self.CPLoss = CPLoss(rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False)\n",
        "    self.StyleLoss = StyleLoss()\n",
        "    self.TVLoss = TVLoss(tv_type='tv', p = 1)\n",
        "    self.PerceptualLoss = PerceptualLoss(model='net-lin', net='alex', colorspace='rgb', spatial=False, use_gpu=True, gpu_ids=[0], model_path=None)\n",
        "    layers_weights = {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "    self.Contextual_Loss = Contextual_Loss(layers_weights, crop_quarter=False, max_1d_size=100,\n",
        "        distance_type = 'cosine', b=1.0, band_width=0.5,\n",
        "        use_vgg = True, net = 'vgg19', calc_type = 'regular')\n",
        "\n",
        "    self.MSELoss = torch.nn.MSELoss()\n",
        "    self.L1Loss = nn.L1Loss()\n",
        "\n",
        "    # metrics\n",
        "    \"\"\"\n",
        "    self.psnr_metric = PSNR()\n",
        "    self.ssim_metric = SSIM()\n",
        "    self.ae_metric = AE()\n",
        "    self.mse_metric = MSE()\n",
        "    \"\"\"\n",
        "\n",
        "  def forward(self, ):\n",
        "    return self.netG()\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "\n",
        "      #return {'A': A, 'C': C, 'path': path, 'part_locations': part_locations}\n",
        "      # train_batch[0] = A # lr\n",
        "      # train_batch[1] = C # hr\n",
        "      # train_batch[3] = part_locations\n",
        "\n",
        "      data_part_locations = train_batch[2]\n",
        "\n",
        "      ####################################\n",
        "      # generator training\n",
        "      out = self.netG(train_batch[0], part_locations=train_batch[3])\n",
        "\n",
        "      ############################\n",
        "      # loss calculation\n",
        "      total_loss = 0\n",
        "      \"\"\"\n",
        "      HFENLoss_forward = self.HFENLoss(out, train_batch[1])\n",
        "      total_loss += HFENLoss_forward\n",
        "      ElasticLoss_forward = self.ElasticLoss(out, train_batch[1])\n",
        "      total_loss += ElasticLoss_forward\n",
        "      RelativeL1_forward = self.RelativeL1(out, train_batch[1])\n",
        "      total_loss += RelativeL1_forward\n",
        "      \"\"\"\n",
        "      L1CosineSim_forward = 5*self.L1CosineSim(out, train_batch[1])\n",
        "      total_loss += L1CosineSim_forward\n",
        "      self.log('loss/L1CosineSim', L1CosineSim_forward)\n",
        "\n",
        "      \"\"\"\n",
        "      ClipL1_forward = self.ClipL1(out, train_batch[1])\n",
        "      total_loss += ClipL1_forward\n",
        "      FFTloss_forward = self.FFTloss(out, train_batch[1])\n",
        "      total_loss += FFTloss_forward\n",
        "      OFLoss_forward = self.OFLoss(out)\n",
        "      total_loss += OFLoss_forward\n",
        "      GPLoss_forward = self.GPLoss(out, train_batch[1])\n",
        "      total_loss += GPLoss_forward\n",
        "      \n",
        "      CPLoss_forward = 0.1*self.CPLoss(out, train_batch[1])\n",
        "      total_loss += CPLoss_forward\n",
        "      \n",
        "\n",
        "      Contextual_Loss_forward = self.Contextual_Loss(out, train_batch[1])\n",
        "      total_loss += Contextual_Loss_forward\n",
        "      self.log('loss/contextual', Contextual_Loss_forward)\n",
        "      \"\"\"\n",
        "\n",
        "      #style_forward = 240*self.StyleLoss(out, train_batch[1])\n",
        "      #total_loss += style_forward\n",
        "      #self.log('loss/style', style_forward)\n",
        "\n",
        "      tv_forward = 0.0000005*self.TVLoss(out)\n",
        "      total_loss += tv_forward\n",
        "      self.log('loss/tv', tv_forward)\n",
        "\n",
        "      perceptual_forward = 2*self.PerceptualLoss(out, train_batch[1])\n",
        "      total_loss += perceptual_forward\n",
        "      self.log('loss/perceptual', perceptual_forward)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # train discriminator\n",
        "      Tensor = torch.cuda.FloatTensor #if cuda else torch.FloatTensor\n",
        "      valid = Variable(Tensor(out.shape).fill_(1.0), requires_grad=False)\n",
        "      fake = Variable(Tensor(out.shape).fill_(0.0), requires_grad=False)\n",
        "      dis_real_loss = self.MSELoss(train_batch[1], valid)\n",
        "      dis_fake_loss = self.MSELoss(out, fake)\n",
        "\n",
        "      d_loss = (dis_real_loss + dis_fake_loss) / 2\n",
        "      self.log('loss/d_loss', d_loss)\n",
        "\n",
        "      return total_loss+d_loss\n",
        "\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.netG.parameters(), lr=2e-4, betas=(0.5, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "    return optimizer\n",
        "\n",
        "  def validation_step(self, train_batch, train_idx):\n",
        "    # return {'A': A, 'path': path, 'part_locations': part_locations}\n",
        "    # train_batch[0] = lr\n",
        "    # train_batch[1] = path\n",
        "    # train_batch[2] = part_locations\n",
        "    out = self.netG(train_batch[0], part_locations=train_batch[2])\n",
        "\n",
        "    \"\"\"\n",
        "    # metrics\n",
        "    # currently not usable, but are correctly implemented, needs a modified dataloader\n",
        "    self.log('metrics/PSNR', self.psnr_metric(train_batch[2], out))\n",
        "    self.log('metrics/SSIM', self.ssim_metric(train_batch[2], out))\n",
        "    self.log('metrics/MSE', self.mse_metric(train_batch[2], out))\n",
        "    self.log('metrics/LPIPS', self.PerceptualLoss(out, train_batch[2]))\n",
        "    \"\"\"\n",
        "\n",
        "    validation_output = '/content/validation_output/'\n",
        "\n",
        "    # train_batch[3] can contain multiple files, depending on the batch_size\n",
        "    for f in train_batch[1]:\n",
        "      # data is processed as a batch, to save indididual files, a counter is used\n",
        "      counter = 0\n",
        "      if not os.path.exists(os.path.join(validation_output, os.path.splitext(os.path.basename(f))[0])):\n",
        "        os.makedirs(os.path.join(validation_output, os.path.splitext(os.path.basename(f))[0]))\n",
        "\n",
        "      filename_with_extention = os.path.basename(f)\n",
        "      filename = os.path.splitext(filename_with_extention)[0]\n",
        "      save_image(out[counter], os.path.join(validation_output, filename, str(self.trainer.global_step) + '.png'))\n",
        "\n",
        "\n",
        "\n",
        "  def test_step(self, train_batch, train_idx):\n",
        "    # train_batch[0] = masked\n",
        "    # train_batch[1] = mask\n",
        "    # train_batch[2] = path\n",
        "    print(\"testing\")\n",
        "    test_output = '/content/test_output/'\n",
        "    if not os.path.exists(test_output):\n",
        "      os.makedirs(test_output)\n",
        "\n",
        "    out = self(train_batch[0].unsqueeze(0),train_batch[1].unsqueeze(0))\n",
        "    out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    save_image(out, os.path.join(test_output, os.path.splitext(os.path.basename(train_batch[2]))[0] + '.png'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsLMzAYjP-hy"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVZEeiVEF8h-"
      },
      "source": [
        "Restart with ``runtime > restart runtime`` once if you see ``AttributeError: module 'PIL.TiffTags' has no attribute 'IFD'``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDRXNC2UgeHG"
      },
      "source": [
        "# delete logs if needed\n",
        "%cd /content/\n",
        "!sudo rm -rf /content/lightning_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u2PNOua0K5r"
      },
      "source": [
        "# Training\n",
        "%cd /content/\n",
        "dm = DFNetDataModule(training_path = '/content/DFDNet/ffhq/', train_partpath = '/content/DFDNet/landmarks', validation_path = '/content/validation/', val_partpath='/content/landmarks', batch_size=1)\n",
        "model = CustomTrainClass()\n",
        "#model = model.load_from_checkpoint('/content/Checkpoint_0_450.ckpt') # start training from checkpoint, warning: apperantly global_step will be reset to zero and overwriting validation images, you could manually make an offset\n",
        "\n",
        "# GPU\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=100, save_path='/content/')])\n",
        "# 2+ GPUS (locally, not inside Google Colab)\n",
        "# Recommended: Pytorch 1.8+. 1.7.1 seems to have dataloader issues and ddp seems to cause problems in general.\n",
        "#trainer = pl.Trainer(gpus=2, distributed_backend='dp', max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=100, save_path='/content/')])\n",
        "# GPU with AMP (amp_level='O1' = mixed precision)\n",
        "# currently not working\n",
        "#trainer = pl.Trainer(gpus=1, precision=16, amp_level='O1', max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "# TPU\n",
        "#trainer = pl.Trainer(tpu_cores=8, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "trainer.fit(model, dm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmpUGYIA0RS3"
      },
      "source": [
        "# Testing\n",
        "\n",
        "Not properly implemented/tested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yBefvtq0Qb7"
      },
      "source": [
        "# testing the model\n",
        "dm = DS_green_from_mask('/content/test')\n",
        "model = CustomTrainClass()\n",
        "# GPU\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "# GPU with AMP (amp_level='O1' = mixed precision)\n",
        "#trainer = pl.Trainer(gpus=1, precision=16, amp_level='O1', max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "# TPU\n",
        "#trainer = pl.Trainer(tpu_cores=8, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "trainer.test(model, dm, ckpt_path='/content/Checkpoint_2_2250.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urk69Nn-zv7B"
      },
      "source": [
        "----------------------------------\n",
        "\n",
        "Helpful code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe53B8ghd2h3"
      },
      "source": [
        "# Landmark generation\n",
        "\n",
        "Install that and restart runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5PWc4-3XfIH"
      },
      "source": [
        "!pip install face-alignment\n",
        "!pip install matplotlib --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Gf-mz6Ljd3-B"
      },
      "source": [
        "%cd /content/\n",
        "import face_alignment\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False)\n",
        "\n",
        "unchecked_input_path = '/content/validation' #@param {type:\"string\"}\n",
        "checked_output_path = '/content/validation' #@param {type:\"string\"}\n",
        "failed_output_path = '/content/validation' #@param {type:\"string\"}\n",
        "landmark_output_path = '/content/landmarks' #@param {type:\"string\"}\n",
        "\n",
        "if not os.path.exists(unchecked_input_path):\n",
        "    os.makedirs(unchecked_input_path)\n",
        "if not os.path.exists(checked_output_path):\n",
        "    os.makedirs(checked_output_path)\n",
        "if not os.path.exists(failed_output_path):\n",
        "    os.makedirs(failed_output_path)\n",
        "if not os.path.exists(landmark_output_path):\n",
        "    os.makedirs(landmark_output_path)\n",
        "\n",
        "files = glob.glob(unchecked_input_path + '/**/*.png', recursive=True)\n",
        "files_jpg = glob.glob(unchecked_input_path + '/**/*.jpg', recursive=True)\n",
        "files.extend(files_jpg)\n",
        "err_files=[]\n",
        "\n",
        "for f in tqdm(files):\n",
        "  input = io.imread(f)\n",
        "  preds = fa.get_landmarks(input)\n",
        "  #print(preds)\n",
        "  if preds is not None:\n",
        "    np.savetxt(os.path.join(landmark_output_path, os.path.basename(f)+\".txt\"), preds[0], delimiter=' ', fmt='%1.3f')   # X is an array\n",
        "    shutil.move(f, os.path.join(checked_output_path,os.path.basename(f)))\n",
        "  else:\n",
        "    shutil.move(f, os.path.join(failed_output_path,os.path.basename(f)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrVKiygYpTLx"
      },
      "source": [
        "# Check data within dataset\n",
        "\n",
        "Training can crash if the extracted feature has dimension 0 somewhere inside the shape. To avoid this problem, it is recommended to test all images with vgg extraction, to make sure that won't crash normal training. Test for one epoch. If a checkpoint gets generated, don't use it for real training. Broken filenames will get printed. If epoch is finished, the test is complete. Delete the printed files (and their landmarks). Data inside validation folders does not matter and will be skipped, but you probably should put at least one file there to avoid crashing. Ignore loss.\n",
        "\n",
        "Tip: Avoid pictures with multiple faces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpcQ30KJtf8s",
        "cellView": "form"
      },
      "source": [
        "#@title model.py (removing training code, adding error status code for file removal)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import functools\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter as P\n",
        "#from util import util\n",
        "from torchvision import models\n",
        "#import scipy.io as sio\n",
        "import numpy as np\n",
        "#import scipy.ndimage\n",
        "import torch.nn.utils.spectral_norm as SpectralNorm\n",
        "\n",
        "from torch.autograd import Function\n",
        "from math import sqrt\n",
        "import random\n",
        "import os\n",
        "import math\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "class VGGFeat(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Input: (B, C, H, W), RGB, [-1, 1]\n",
        "    \"\"\"\n",
        "    def __init__(self, weight_path='/content/DFDNet/weights/vgg19.pth'):\n",
        "        super().__init__()\n",
        "        self.model = models.vgg19(pretrained=False)\n",
        "        self.build_vgg_layers()\n",
        "        \n",
        "        self.model.load_state_dict(torch.load(weight_path))\n",
        "\n",
        "        self.register_parameter(\"RGB_mean\", nn.Parameter(torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)))\n",
        "        self.register_parameter(\"RGB_std\", nn.Parameter(torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)))\n",
        "        \n",
        "        # self.model.eval()\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def build_vgg_layers(self):\n",
        "        vgg_pretrained_features = self.model.features\n",
        "        self.features = []\n",
        "        # feature_layers = [0, 3, 8, 17, 26, 35]\n",
        "        feature_layers = [0, 8, 17, 26, 35]\n",
        "        for i in range(len(feature_layers)-1): \n",
        "            module_layers = torch.nn.Sequential() \n",
        "            for j in range(feature_layers[i], feature_layers[i+1]):\n",
        "                module_layers.add_module(str(j), vgg_pretrained_features[j])\n",
        "            self.features.append(module_layers)\n",
        "        self.features = torch.nn.ModuleList(self.features)\n",
        "\n",
        "    def preprocess(self, x):\n",
        "        x = (x + 1) / 2\n",
        "        x = (x - self.RGB_mean) / self.RGB_std\n",
        "        if x.shape[3] < 224:\n",
        "            x = torch.nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.preprocess(x)\n",
        "        features = []\n",
        "        for m in self.features:\n",
        "            # print(m)\n",
        "            x = m(x)\n",
        "            features.append(x)\n",
        "        return features \n",
        "\n",
        "\n",
        "class StyledUpBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1,upsample=False):\n",
        "        super().__init__()\n",
        "        if upsample:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                Blur(out_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                Blur(in_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding)\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        self.convup = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                # EqualConv2d(out_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(out_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                # Blur(out_channel),\n",
        "            )\n",
        "        # self.noise1 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain1 = AdaptiveInstanceNorm(out_channel)\n",
        "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        # self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\n",
        "        # self.noise2 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain2 = AdaptiveInstanceNorm(out_channel)\n",
        "        # self.lrelu2 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.ScaleModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1))\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "        self.ShiftModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1)),\n",
        "            nn.Sigmoid(),\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "       \n",
        "    def forward(self, input, style):\n",
        "        out = self.conv1(input)\n",
        "#         out = self.noise1(out, noise)\n",
        "        out = self.lrelu1(out)\n",
        "\n",
        "        Shift1 = self.ShiftModel1(style)\n",
        "        Scale1 = self.ScaleModel1(style)\n",
        "        out = out * Scale1 + Shift1\n",
        "        # out = self.adain1(out, style)\n",
        "        outup = self.convup(out)\n",
        "\n",
        "        return outup\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class StyledUpBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1,upsample=False):\n",
        "        super().__init__()\n",
        "        if upsample:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                Blur(out_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                Blur(in_channel),\n",
        "                # EqualConv2d(in_channel, out_channel, kernel_size, padding=padding)\n",
        "                SpectralNorm(nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        self.convup = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                # EqualConv2d(out_channel, out_channel, kernel_size, padding=padding),\n",
        "                SpectralNorm(nn.Conv2d(out_channel, out_channel, kernel_size, padding=padding)),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                # Blur(out_channel),\n",
        "            )\n",
        "        # self.noise1 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain1 = AdaptiveInstanceNorm(out_channel)\n",
        "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        # self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\n",
        "        # self.noise2 = equal_lr(NoiseInjection(out_channel))\n",
        "        # self.adain2 = AdaptiveInstanceNorm(out_channel)\n",
        "        # self.lrelu2 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.ScaleModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1))\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "        self.ShiftModel1 = nn.Sequential(\n",
        "            # Blur(in_channel),\n",
        "            SpectralNorm(nn.Conv2d(in_channel,out_channel,3, 1, 1)),\n",
        "            # nn.Conv2d(in_channel,out_channel,3, 1, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            SpectralNorm(nn.Conv2d(out_channel, out_channel, 3, 1, 1)),\n",
        "            nn.Sigmoid(),\n",
        "            # nn.Conv2d(out_channel, out_channel, 3, 1, 1)\n",
        "        )\n",
        "       \n",
        "    def forward(self, input, style):\n",
        "        out = self.conv1(input)\n",
        "#         out = self.noise1(out, noise)\n",
        "        out = self.lrelu1(out)\n",
        "\n",
        "        Shift1 = self.ShiftModel1(style)\n",
        "        Scale1 = self.ScaleModel1(style)\n",
        "        out = out * Scale1 + Shift1\n",
        "        # out = self.adain1(out, style)\n",
        "        outup = self.convup(out)\n",
        "\n",
        "        return outup\n",
        "\n",
        "class UNetDictFace(pl.LightningModule):\n",
        "    def __init__(self, ngf=64, dictionary_path='/content/DFDNet/DictionaryCenter512'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.part_sizes = np.array([80,80,50,110]) # size for 512\n",
        "        self.feature_sizes = np.array([256,128,64,32])\n",
        "        self.channel_sizes = np.array([128,256,512,512])\n",
        "        Parts = ['left_eye','right_eye','nose','mouth']\n",
        "        self.Dict_256 = {}\n",
        "        self.Dict_128 = {}\n",
        "        self.Dict_64 = {}\n",
        "        self.Dict_32 = {}\n",
        "        for j,i in enumerate(Parts):\n",
        "            f_256 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_256_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_256_reshape = f_256.reshape(f_256.size(0),self.channel_sizes[0],self.part_sizes[j]//2,self.part_sizes[j]//2)\n",
        "            max_256 = torch.max(torch.sqrt(compute_sum(torch.pow(f_256_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_256[i] = f_256_reshape #/ max_256\n",
        "            \n",
        "            f_128 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_128_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_128_reshape = f_128.reshape(f_128.size(0),self.channel_sizes[1],self.part_sizes[j]//4,self.part_sizes[j]//4)\n",
        "            max_128 = torch.max(torch.sqrt(compute_sum(torch.pow(f_128_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_128[i] = f_128_reshape #/ max_128\n",
        "\n",
        "            f_64 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_64_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_64_reshape = f_64.reshape(f_64.size(0),self.channel_sizes[2],self.part_sizes[j]//8,self.part_sizes[j]//8)\n",
        "            max_64 = torch.max(torch.sqrt(compute_sum(torch.pow(f_64_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_64[i] = f_64_reshape #/ max_64\n",
        "\n",
        "            f_32 = torch.from_numpy(np.load(os.path.join(dictionary_path, '{}_32_center.npy'.format(i)), allow_pickle=True))\n",
        "\n",
        "            f_32_reshape = f_32.reshape(f_32.size(0),self.channel_sizes[3],self.part_sizes[j]//16,self.part_sizes[j]//16)\n",
        "            max_32 = torch.max(torch.sqrt(compute_sum(torch.pow(f_32_reshape, 2), axis=[1, 2, 3], keepdim=True)),torch.FloatTensor([1e-4]))\n",
        "            self.Dict_32[i] = f_32_reshape #/ max_32\n",
        "\n",
        "        self.le_256 = AttentionBlock(128)\n",
        "        self.le_128 = AttentionBlock(256)\n",
        "        self.le_64 = AttentionBlock(512)\n",
        "        self.le_32 = AttentionBlock(512)\n",
        "\n",
        "        self.re_256 = AttentionBlock(128)\n",
        "        self.re_128 = AttentionBlock(256)\n",
        "        self.re_64 = AttentionBlock(512)\n",
        "        self.re_32 = AttentionBlock(512)\n",
        "\n",
        "        self.no_256 = AttentionBlock(128)\n",
        "        self.no_128 = AttentionBlock(256)\n",
        "        self.no_64 = AttentionBlock(512)\n",
        "        self.no_32 = AttentionBlock(512)\n",
        "\n",
        "        self.mo_256 = AttentionBlock(128)\n",
        "        self.mo_128 = AttentionBlock(256)\n",
        "        self.mo_64 = AttentionBlock(512)\n",
        "        self.mo_32 = AttentionBlock(512)\n",
        "\n",
        "        #norm\n",
        "        self.VggExtract = VGGFeat()\n",
        "        \n",
        "        ######################\n",
        "        self.MSDilate = MSDilateBlock(ngf*8, dilation = [4,3,2,1])  #\n",
        "\n",
        "        self.up0 = StyledUpBlock(ngf*8,ngf*8)\n",
        "        self.up1 = StyledUpBlock(ngf*8, ngf*4) #\n",
        "        self.up2 = StyledUpBlock(ngf*4, ngf*2) #\n",
        "        self.up3 = StyledUpBlock(ngf*2, ngf) #\n",
        "        self.up4 = nn.Sequential( # 128\n",
        "            # nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            SpectralNorm(nn.Conv2d(ngf, ngf, 3, 1, 1)),\n",
        "            # nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            UpResBlock(ngf),\n",
        "            UpResBlock(ngf),\n",
        "            # SpectralNorm(nn.Conv2d(ngf, 3, kernel_size=3, stride=1, padding=1)),\n",
        "            nn.Conv2d(ngf, 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.to_rgb0 = ToRGB(ngf*8)\n",
        "        self.to_rgb1 = ToRGB(ngf*4)\n",
        "        self.to_rgb2 = ToRGB(ngf*2)\n",
        "        self.to_rgb3 = ToRGB(ngf*1)\n",
        "\n",
        "        # for param in self.BlurInputConv.parameters():\n",
        "        #     param.requires_grad = False\n",
        "    \n",
        "    def forward(self, input, part_locations):\n",
        "        VggFeatures = self.VggExtract(input)\n",
        "        # for b in range(input.size(0)):\n",
        "        b = 0\n",
        "        delete_flag = 0\n",
        "        UpdateVggFeatures = []\n",
        "        for i, f_size in enumerate(self.feature_sizes):\n",
        "            cur_feature = VggFeatures[i]\n",
        "\n",
        "            update_feature = cur_feature.clone() #* 0\n",
        "            cur_part_sizes = self.part_sizes // (512/f_size)\n",
        "            dicts_feature = getattr(self, 'Dict_'+str(f_size))\n",
        "            \n",
        "            LE_Dict_feature = dicts_feature['left_eye'].to(input)\n",
        "            RE_Dict_feature = dicts_feature['right_eye'].to(input)\n",
        "            NO_Dict_feature = dicts_feature['nose'].to(input)\n",
        "            MO_Dict_feature = dicts_feature['mouth'].to(input)\n",
        "\n",
        "            le_location = (part_locations[0][b] // (512/f_size)).int()\n",
        "            re_location = (part_locations[1][b] // (512/f_size)).int()\n",
        "            no_location = (part_locations[2][b] // (512/f_size)).int()\n",
        "            mo_location = (part_locations[3][b] // (512/f_size)).int()\n",
        "\n",
        "            LE_feature = cur_feature[:,:,le_location[1]:le_location[3],le_location[0]:le_location[2]].clone()\n",
        "            RE_feature = cur_feature[:,:,re_location[1]:re_location[3],re_location[0]:re_location[2]].clone()\n",
        "            NO_feature = cur_feature[:,:,no_location[1]:no_location[3],no_location[0]:no_location[2]].clone()\n",
        "            MO_feature = cur_feature[:,:,mo_location[1]:mo_location[3],mo_location[0]:mo_location[2]].clone()\n",
        "            \n",
        "            #resize\n",
        "            # check\n",
        "            # good: torch.Size([1, 128, 45, 46])\n",
        "            # bad:  torch.Size([1, 128, 36, 0])\n",
        "            \n",
        "            if LE_feature.shape[2] == 0 or LE_feature.shape[3] == 0:\n",
        "              delete_flag = 1\n",
        "            if RE_feature.shape[2] == 0 or RE_feature.shape[3] == 0:\n",
        "              delete_flag = 1\n",
        "            if NO_feature.shape[2] == 0 or NO_feature.shape[3] == 0:\n",
        "              delete_flag = 1\n",
        "            if MO_feature.shape[2] == 0 or MO_feature.shape[3] == 0:\n",
        "              delete_flag = 1\n",
        "\n",
        "        return delete_flag\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# smaller blocks\n",
        "def AttentionBlock(in_channel):\n",
        "    return nn.Sequential(\n",
        "        SpectralNorm(nn.Conv2d(in_channel, in_channel, 3, 1, 1)),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        SpectralNorm(nn.Conv2d(in_channel, in_channel, 3, 1, 1))\n",
        "    )\n",
        "\n",
        "\n",
        "class MSDilateBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channels,conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, kernel_size=3, dilation=[1,1,1,1], bias=True):\n",
        "        super(MSDilateBlock, self).__init__()\n",
        "        self.conv1 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[0], bias=bias)\n",
        "        self.conv2 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[1], bias=bias)\n",
        "        self.conv3 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[2], bias=bias)\n",
        "        self.conv4 =  convU(in_channels, in_channels,conv_layer, norm_layer, kernel_size,dilation=dilation[3], bias=bias)\n",
        "        self.convi =  SpectralNorm(conv_layer(in_channels*4, in_channels, kernel_size=kernel_size, stride=1, padding=(kernel_size-1)//2, bias=bias))\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv1(x)\n",
        "        conv2 = self.conv2(x)\n",
        "        conv3 = self.conv3(x)\n",
        "        conv4 = self.conv4(x)\n",
        "        cat  = torch.cat([conv1, conv2, conv3, conv4], 1)\n",
        "        out = self.convi(cat) + x\n",
        "        return out\n",
        "\n",
        "\n",
        "class VggClassNet(pl.LightningModule):\n",
        "    def __init__(self, select_layer = ['0','5','10','19']):\n",
        "        super(VggClassNet, self).__init__()\n",
        "        self.select = select_layer\n",
        "        self.vgg = models.vgg19(pretrained=True).features\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for name, layer in self.vgg._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in self.select:\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class UpResBlock(pl.LightningModule):\n",
        "    def __init__(self, dim, conv_layer = nn.Conv2d, norm_layer = nn.BatchNorm2d):\n",
        "        super(UpResBlock, self).__init__()\n",
        "        self.Model = nn.Sequential(\n",
        "            # SpectralNorm(conv_layer(dim, dim, 3, 1, 1)),\n",
        "            conv_layer(dim, dim, 3, 1, 1),\n",
        "            # norm_layer(dim),\n",
        "            nn.LeakyReLU(0.2,True),\n",
        "            # SpectralNorm(conv_layer(dim, dim, 3, 1, 1)),\n",
        "            conv_layer(dim, dim, 3, 1, 1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        out = x + self.Model(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3P_fywUuaiF",
        "cellView": "form"
      },
      "source": [
        "#@title CustomTrainClass.py (deleting loss and discriminator, printing corrupt files)\n",
        "from vic.loss import CharbonnierLoss, GANLoss, GradientPenaltyLoss, HFENLoss, TVLoss, GradientLoss, ElasticLoss, RelativeL1, L1CosineSim, ClipL1, MaskedL1Loss, MultiscalePixelLoss, FFTloss, OFLoss, L1_regularization, ColorLoss, AverageLoss, GPLoss, CPLoss, SPL_ComputeWithTrace, SPLoss, Contextual_Loss, StyleLoss\n",
        "from vic.perceptual_loss import PerceptualLoss\n",
        "from metrics import *\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "class CustomTrainClass(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # generator\n",
        "    self.netG = UNetDictFace(64)\n",
        "    weights_init(self.netG, 'kaiming')\n",
        "\n",
        "    #self.MSELoss = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "  def forward(self, ):\n",
        "    return self.netG()\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "\n",
        "      #return {'A': A, 'C': C, 'path': path, 'part_locations': part_locations}\n",
        "      # train_batch[0] = A # lr\n",
        "      # train_batch[1] = C # hr\n",
        "      # train_batch[3] = part_locations\n",
        "\n",
        "      data_part_locations = train_batch[2]\n",
        "\n",
        "      ####################################\n",
        "      # generator training\n",
        "      delete_status = self.netG(train_batch[0], part_locations=train_batch[3])\n",
        "      if delete_status == 1:\n",
        "        print(train_batch[2])\n",
        "      #return 123\n",
        "\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.netG.parameters(), lr=2e-4, betas=(0.5, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "    return optimizer\n",
        "\n",
        "  def validation_step(self, train_batch, train_idx):\n",
        "    print(\"val_skip\")\n",
        "\n",
        "\n",
        "\n",
        "  def test_step(self, train_batch, train_idx):\n",
        "    # train_batch[0] = masked\n",
        "    # train_batch[1] = mask\n",
        "    # train_batch[2] = path\n",
        "    print(\"testing\")\n",
        "    test_output = '/content/test_output/'\n",
        "    if not os.path.exists(test_output):\n",
        "      os.makedirs(test_output)\n",
        "\n",
        "    out = self(train_batch[0].unsqueeze(0),train_batch[1].unsqueeze(0))\n",
        "    out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    save_image(out, os.path.join(test_output, os.path.splitext(os.path.basename(train_batch[2]))[0] + '.png'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_P8-rlkvBcj"
      },
      "source": [
        "# check data\n",
        "%cd /content/\n",
        "dm = DFNetDataModule(training_path = '/content/DFDNet/ffhq/', train_partpath = '/content/DFDNet/landmarks', validation_path = '/content/validation/', val_partpath='/content/landmarks', batch_size=1)\n",
        "model = CustomTrainClass()\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=1, progress_bar_refresh_rate=20, default_root_dir='/content/')\n",
        "trainer.fit(model, dm)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
